{
  "hash": "6c86289f15ceb64406f6bc0b0856ebc3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Experimental absenteeism and logistic regression - Part II\nauthor: Ladislas Nalborczyk\ndate: 2017-09-22\ncategories:\n  - R\n  - Bayesian\n  - brms\n  - Logistic regression\n  - Multilevel model\ndescription: This post continues our exploration of the logistic regression model by extending it to a multilevel logistic regression model, using the `brms` package.\n---\n\n::: {.cell}\n\n:::\n\n\nIn a [previous post](../2017-08-05-absenteeim1/2017-08-05-absenteeism1.qmd), we tried to determine whether we could predict the presence of students that registered for psychological experiments, based on their mode of enrolment and the sending of a reminder by e-mail. As these two factors were confounded, we will focus in the current post on evaluating the effect of the reminder only, based on a bigger dataset gathered by several researchers.\n\nLet's say that I have convinced ten of my colleagues to systematically send a reminder to one half of the participants of their next study, and no reminder to the other half.[^1] As my colleagues love high-powered studies, each of them aimed at recruiting approximately 200 participants per experiment, while this sample size could vary a little bit from one researcher to another (+/- 20%).\n\n## Step 1: Getting the data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n(data <-\n    read.csv(\n        \"http://raw.githubusercontent.com/lnalborczyk/old_blog/master/_posts/absenteeism/absence2.csv\",\n        stringsAsFactors = FALSE) )\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n   reminder researcher presence absence total\n1        -1          1       16      86   102\n2        -1          2       53      59   112\n3        -1          3       31      65    96\n4        -1          4       61      34    95\n5        -1          5       34      49    83\n6        -1          6       34      54    88\n7        -1          7        3      77    80\n8        -1          8       38      54    92\n9        -1          9       31      65    96\n10       -1         10       23      58    81\n11        1          1       96       6   102\n12        1          2      109       3   112\n13        1          3       87       9    96\n14        1          4       92       3    95\n15        1          5       65      18    83\n16        1          6       82       6    88\n17        1          7       64      16    80\n18        1          8       81      11    92\n19        1          9       89       7    96\n20        1         10       76       5    81\n```\n\n\n:::\n:::\n\n\nThe `reminder` column indicates whether a reminder e-mail was sent to the participant prior to the experiment (coded as 1), while the `researcher` column indicates which researcher ran the study, from 1 to 10. The last three columns represent the counts of present, absent, and total number of participants, for a grand total of 1850 participants.\n\n## Step 2: Introducing brms\n\nIn this first section, we will introduce the `brms` package (Bürkner, 2017), and fit a first simple model to try to predict the mean **log-odds** of a participant being present, through a linear combination of an intercept $\\alpha$ and a slope $\\beta$, the latter allowing to quantify the effect of the reminder.\n\n$$\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha + \\beta \\times \\text{reminder} \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\end{aligned}\n$$\n\nOne great advantage of `brms` is that it allows to specify models using an [lme4](https://cran.r-project.org/web/packages/lme4/lme4.pdf)-like syntax, where the left side of the formula describes the outcome to be predicted and the right side describes the predictors (both constant and varying effects). When trying to predict aggregated binomial data, the outcome that is modeled is the number of *successes* (in our case, *presence*) out of the total number of *trials*, which is expressed in `brms` as `sucesses|trials(total)`[^2].\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nAs previously, we can retrieve the grand mean probability of presence by transforming back the intercept, as we know that $p = \\exp(\\alpha) / (1 + \\exp(\\alpha) )$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\na <- fixef(mod1)[1] # extracting the intercept\nexp(a) / (1 + exp(a) ) # equivalent to plogis(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6994943\n```\n\n\n:::\n:::\n\n\nA summary of this model can be obtained using the `posterior_summary()` function, which provides the mean of the posterior distribution along with its standard error and credible intervals.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nposterior_summary(x = mod1, pars = \"^b_\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Estimate  Est.Error      Q2.5     Q97.5\nb_Intercept 0.8448907 0.06894024 0.7098691 0.9830591\nb_reminder  1.4635349 0.07011244 1.3280188 1.6055522\n```\n\n\n:::\n:::\n\n\nAlternatively, `brms` (in combination with `bayesplot`) offers a nice method to plot `brmsfit` objects. Below, we plot an histogram of samples from the posterior distribution for both the intercept $\\alpha$ and the slope $\\beta$, along with [traceplots](http://sbfnk.github.io/mfiidd/mcmc_diagnostics.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod1 %>%\n    plot(\n        combo = c(\"hist\", \"trace\"), widths = c(1, 1.5),\n        theme = theme_bw(base_size = 12) )\n```\n\n::: {.cell-output-display}\n![](2017-09-22-absenteeism2_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nFrom this first model we can also compute (as previously) the **odds ratio** (OR), between the no-reminder and the reminder conditions, by simply exponentiating the slope. In our case, the OR is approximately equal to 4.32 (95% HDI [3.79, 4.94]), meaning that it is 4.32 times more likely that participants will be present if a reminder is sent prior to the experiment.\n\nThere is one major issue though with this analysis, related to the nested structure of the data. Each researcher evaluated the effects of the reminder in his study, and we could expect each study to have its own baseline level of presence probability, perhaps related to the specificities of the study, or to the field of research, the university, etc. In other words, we could expect participants of the same *cluster* (i.e., the same study, ran by the same researcher) to be more similar to each others, than participants of different *clusters*. In other words, observations are **interdependant**. In this situation, we can not run a standard logistic regression analysis because this violates one of the most important assumptions in the linear model, namely the assumption of **independence of the residuals** (Sommet & Morselli, 2017). Multilevel models (MLMs) allow to disentangle the effects intrinsic to a specific cluster and the between-clusters effects, by allowing parameters to vary by *cluster*.\n\n## Step 3: Varying the intercept\n\nWith the following model, we will estimate the mean probability of presence, only specifying an intercept (this model is sometimes called the *unconditionnal mean model*), that we will allow to vary by `researcher`.\n\n$$\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha_{researcher_{[i]}} \\\\\n\\alpha_{researcher} &\\sim \\mathrm{Normal}(\\alpha, \\sigma) \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\sigma &\\sim \\mathrm{HalfCauchy}(0, 10) \\\\\n\\end{aligned}\n$$\n\nFrom this formulation we can see that the **log-odds** are now allowed to vary by *cluster* (i.e., by researcher), and that we are also estimating the parameters of the distribution from which these intercepts are issued (i.e., the *population* of intercepts, described in the third line of the model). This way, the model can learn information both at the level of the `researcher`, and at the level of the population of `researchers`, thus fighting the anterograde amnesia of the constant-effects models (see McElreath, 2016, page 355).\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nposterior_summary(x = mod2, pars = c(\"^b_\", \"^sd_\") )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                          Estimate Est.Error      Q2.5     Q97.5\nb_Intercept              0.5307313 0.1690843 0.2045174 0.8617104\nsd_researcher__Intercept 0.4932914 0.1581643 0.2696283 0.8798965\n```\n\n\n:::\n:::\n\n\nWe can interpret the variation of the intercept $\\alpha$ between researchers by considering the *intra-class correlation* (ICC)[^3], which goes from 0 if the grouping conveys no information to 1 if all levels of a cluster are identical (Gelman, 2006, p. 258). In other words, ICC = 0 indicates perfect independence of residuals: the observations do not depend on cluster membership. When the ICC is not different from zero or negligible, one could consider running traditional one-level regression analysis. However, ICC = 1 indicates perfect interdependence of residuals: the observations only vary between clusters (Sommet & Morselli, 2017).\n\nThe ICC is usually expressed as $\\frac{\\tau^{2}}{\\tau^{2} + \\sigma^{2}}$, where $\\tau^{2}$ denotes the variance of the distribution of the varying effects. However, in the context of logistic regression, we do not have residuals (i.e., the $\\sigma^{2}$) on the first level... A first approach to compute the ICC in multilevel logistic regression is known as the *latent variable approach*, as we assume that the true underlying variable is continuous but that we can only observe a binary response that indicates whether the underlying variable is greater or less than a given threshold. In the logistic regression model, the underlying continuous variable will come from a logistic distribution, with a variance of $\\frac{\\pi^2}{3}$, and hence we substitute this for the level 1 variance, resulting in the formula: $\\frac{\\tau^{2}}{\\tau^{2} + \\frac{\\pi^2}{3}}$ when using a logit link (Austin & Merlo, 2017; Browne, Subramanian, Jones, & Goldstein, 2005; Sommet & Morselli, 2017).\n\nNote though that this method of estimating the ICC can differ considerably from other methods (e.g., the simulation method, wait for the next post...) as we assume that the level 1 variance is fixed and independent of the predictor variables.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# extracting tau^2\ntau2 <- posterior_summary(x = mod2, pars = \"^sd_\")\ntau2 <- tau2[1]^2\n\n# computing the ICC\n(ICC <- tau2 / (tau2 + (pi^2 / 3) ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06887129\n```\n\n\n:::\n:::\n\n\nIn our case, the ICC is equal to 0.0689, indicating that 6.89% of the chances of being present is explained by between-study differences, and conversely, that 6.89% is explained by within-study differences.\n\nAnother way to visualise the variability of the varying intercepts is to plot them. In the following plot, we use the `ggjoy` package (Wilke, 2017) to represent the posterior distribution of the estimated mean probability of presence for each researcher along with raw data estimation (the black crosses), while the vertical dotted line represents the grand mean probability of presence.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2017-09-22-absenteeism2_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThis plot reveals the phenomenon of **shrinkage**, that is the phenomenon through which the model expresses its skepticism toward extreme values. The mean predicted probability of presence for each researcher (the varying intercept) will differ from the raw estimation in an amount that is dependant to the distance between this raw proportion and the grand mean probability (and the precision of the estimation for this particular cluster). The more the data seem weird, the more the model's estimation will be *shrunk* to the grand mean.\n\n## Step 4: Varying the slope\n\nIn the same manner that the mean probability of presence might be different from researcher to researcher, one might ask whether the effects of the reminder are identical between researchers. In the next model, we will then allow the slope to vary by researcher too.\n\n$$\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha_{researcher_{[i]}} + \\beta_{researcher_{[i]}} \\times \\text{reminder}_{i} \\\\\n\\begin{bmatrix}\n\\alpha_{\\text{researcher}} \\\\\n\\beta_{\\text{researcher}} \\\\\n\\end{bmatrix}\n&\\sim \\mathrm{MVNormal}\\bigg(\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\textbf{S}\\bigg) \\\\\n\\textbf{S} &=\n\\begin{pmatrix}\n\\sigma_{\\alpha} & 0 \\\\\n0 & \\sigma_{\\beta} \\\\\n\\end{pmatrix}\n\\textbf{R} \\begin{pmatrix}\n\\sigma_{\\alpha} & 0 \\\\\n0 & \\sigma_{\\beta} \\\\\n\\end{pmatrix} \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 10) \\\\\n(\\sigma_{\\alpha}, \\sigma_{\\beta}) &\\sim \\mathrm{HalfCauchy}(0, 10) \\\\\n\\textbf{R} &\\sim \\mathrm{LKJcorr}(2) \\\\\n\\end{aligned}\n$$\n\nWe chose to model the varying intercept and the varying slope as issued from the same **multivariate normal distribution** (on the third line), allowing to estimate the correlation between them (for more details see McElreath, 2016). One reason to do this might be that a low mean probability of presence for a particular study, could be associated with a stronger / weaker effect of the reminder for this particular study. For instance, we could imagine that some researchers paid their participants, and some others did not, and that the latter kind of study would have a lower mean probability of presence, but would exhibit a stronger effect of the `reminder` (i.e., we would expect a negative correlation between the intercept and the slope).\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nNote that one way to model both varying intercept and varying slope without modelling the correlation between these two terms is to specify a double pipe in the formula like `(1+reminder||researcher)`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nposterior_summary(x = mod3, pars = c(\"^b_\", \"cor\", \"^sd_\") )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                      Estimate Est.Error       Q2.5     Q97.5\nb_Intercept                          0.8554010 0.2922946  0.2672683 1.4299838\nb_reminder                           1.6058720 0.1740362  1.2763908 1.9635360\ncor_researcher__Intercept__reminder -0.1602937 0.3239736 -0.7238505 0.5141273\nsd_researcher__Intercept             0.8633531 0.2651655  0.4998002 1.5242362\nsd_researcher__reminder              0.4741958 0.1676596  0.2356156 0.8983865\n```\n\n\n:::\n:::\n\n\nThis output indeed indicates a negative correlation between the varying intercept and the varying slope but relatively weak and associated with a lot of uncertainty ($\\rho=-0.19$, 95% HDI [-0.75, 0.47]).\n\nOne way to compare the models we fitted would be to compare their predictive abilities. It can be done with the `LOO` function, which uses leave-one-out cross-validation (Gelfand, Dey, & Chang 1992; Vehtari, Gelman, & Gabry, 2015).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbrms::LOO(mod1, mod2, mod3, compare = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOutput of model 'mod1':\n\nComputed from 2000 by 20 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -125.9 30.0\np_loo        14.3  4.6\nlooic       251.9 59.9\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.3, 1.0]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     19    95.0%   51      \n   (0.7, 1]   (bad)       1     5.0%   <NA>    \n   (1, Inf)   (very bad)  0     0.0%   <NA>    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'mod2':\n\nComputed from 2000 by 20 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo   -598.4  53.3\np_loo       279.9  22.7\nlooic      1196.8 106.6\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.7, 1.0]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)      0      0.0%  <NA>    \n   (0.7, 1]   (bad)       0      0.0%  <NA>    \n   (1, Inf)   (very bad) 20    100.0%  <NA>    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'mod3':\n\nComputed from 2000 by 20 log-likelihood matrix.\n\n         Estimate  SE\nelpd_loo    -64.0 2.8\np_loo        14.9 1.8\nlooic       128.0 5.6\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.0]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)      4    20.0%   173     \n   (0.7, 1]   (bad)      14    70.0%   <NA>    \n   (1, Inf)   (very bad)  2    10.0%   <NA>    \nSee help('pareto-k-diagnostic') for details.\n```\n\n\n:::\n:::\n\n\nThis comparison reveals that the third model has the best predictive abilities, indicating that it was savvy to include a varying slope. Below we plot the posterior predictions of this last model, at the group-level as well at the population-level (in base plot, because we love base plot).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# extracting posterior samples from the last model\npost <- posterior_samples(mod3, \"b\")\n\n# plotting the raw data\nplot(data$prob ~ data$reminder, las = 1,\n    pch = 1, cex = 0.75,\n    xlab = \"reminder\", ylab = \"p(presence)\")\n\n# adding a grid\nabline(h = seq(0, 1, 0.2), v = seq(-1, 1, 0.2),\n    col = adjustcolor(\"gray\", alpha.f = 0.2), lty = 1)\n\n# extracting coefficients\ncoefs <- rbind(\n  data.frame(t(matrix(fixef(mod3)[, 1]) ) ) %>%\n    rename(intercept = X1, slope = X2),\n  data.frame(coef(mod3) %>% data.frame %>% select(contains(\"Estimate\") ) ) %>%\n    rename(intercept = 1, slope = 2)\n  )\n\n# plotting population-level predictions\nx_plot <- seq(-1, 1, length.out = 2000)\ny_plot <- plogis(coefs[1, 1] + coefs[1, 2] * x_plot)\n\nlines(x_plot, y_plot, lwd = 2, col = \"steelblue\")\n\n# plotting group-level predictions\nfor(i in 2:nrow(coefs) ){\n    \n    x_plot <- seq(-1, 1, length.out = 2000)\n    y_plot <- plogis(coefs[i, 1] + coefs[i, 2] * x_plot)\n    lines(x_plot, y_plot, lwd = 1,\n        col = adjustcolor(\"steelblue\", alpha.f = 0.25) )\n}\n```\n\n::: {.cell-output-display}\n![](2017-09-22-absenteeism2_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe uncertainty associated with the estimation of the intercept and the slope at the population-level can be appreciated by superimposing many samples from the posterior (see also [this blogpost](https://mvuorre.github.io/post/2016/2016-03-06-multilevel-predictions/) from Matti Vuorre for much nicer plots and how to plot credible intervals at the individual-level).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# plotting the raw data\nplot(data$prob ~ data$reminder, las = 1,\n    pch = 1, cex = 0.75,\n    xlab = \"reminder\", ylab = \"p(presence)\")\n\n# adding a grid\nabline(h = seq(0, 1, 0.2), v = seq(-1, 1, 0.2),\n    col = adjustcolor(\"gray\", alpha.f = 0.2), lty = 1)\n\n# plotting group-level predictions (surimposing 1 out of 10 posterior samples)\nfor(i in seq(1, nrow(post), 10) ){\n    \n    x_plot <- seq(-1, 1, length.out = 2000)\n    y_plot <- plogis(post[i,1] + post[i,2] * x_plot)\n    \n    lines(x_plot, y_plot, lwd = 0.75,\n        col = adjustcolor(\"steelblue\", alpha.f = 0.05) )\n        \n}\n```\n\n::: {.cell-output-display}\n![](2017-09-22-absenteeism2_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Conclusions\n\nTo sum up, the effect of the reminder, while non negligeable, appears as quite variable accross different studies ran by different researchers. The baseline probability of presence by researcher appears as even more variable. Thus, predictions of our models could be improved by incorporating more information in the model, for instance by taking into account the similarity between different teams (e.g., teams of the same lab, or studies realised in the same field of research), by refining the structure of the model and adding additionnal levels.\n\n## References\n\n<details>\n  <summary>Click to expand</summary>\n\n<div markdown=\"1\">\n\nBrowne, W. J., Subramanian, S. V., Jones, K., Goldstein, H. (2005). Variance partitioning in multilevel logistic models that exhibit\noverdispersion. *Journal of the Royal Statistical Society—Series A, 168*(3):599–613.\n\nBürkner, P.-C. (2017). brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software, 80(1), 1-28. doi:10.18637/jss.v080.i01\n\nClaus O. Wilke (2017). ggjoy: Joyplots in 'ggplot2'. R package version 0.2.0. https://CRAN.R-project.org/package=ggjoy\n\nEvans, M., Hastings, N., Peacock, B. (1993). Statistical Distributions. John Wiley and Sons: New York, NY.\n\nGelfand, A.E., Dey, D.K., Chang, H. (1992). Model Determination Using Predictive Distributions with Implementation via Sampling-Based Methods. Technical report, DTIC Document.\n\nMcElreath, R. (2016). Statistical Rethinking. Chapman; Hall/CRC.\n\nRobinson, D. (2017). broom: Convert Statistical Analysis Objects into Tidy Data Frames. R package version 0.4.2. https://CRAN.R-project.org/package=broom\n\nSommet, N., & Morselli, D. (2017). Keep Calm and Learn Multilevel Logistic Modeling: A Simplified Three-Step Procedure Using Stata, R, Mplus, and SPSS. International Review of Social Psychology, 30(1), 203–218, DOI: https://doi.org/10.5334/irsp.90\n\nVehtari, A., Gelman, A., Gabry, J. (2015). Efficient Implementation of Leave-One-Out Cross- Validation and WAIC for Evaluating Fitted Bayesian Models. Unpublished manuscript, pp. 1–22. URL http://www.stat.columbia.edu/~gelman/research/unpublished/loo_ stan.pdf.\n\n</div>\n\n</details>\n\n[^1]: Obviously I do not possess such impressive persuasion skills, thus we will work with simulated data.\n\n[^2]: There are several ways to specify priors in `brms`, that can be retrieved by typing `?prior` in the console.\n\n[^3]: The ICC is also known as the *Variance Partitioning Coefficient*.\n",
    "supporting": [
      "2017-09-22-absenteeism2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}