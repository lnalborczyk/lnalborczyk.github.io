{
  "hash": "80f062a1179d7b2b82da0854934ad7c6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Using R to make sense of the generalised linear model\nauthor: Ladislas Nalborczyk\ndate: 2018-01-15\ncategories:\n  - Tutorial\n  - R\n  - GLM\ndescription: What is the difference between the errors and the residuals? What does it mean for a model to *predict* something? What is a link function? In the current post, we use four `R` functions (the `predict`, `fitted`, `residuals` and `simulate` functions) to illustrate the mechanisms and assumptions of the generalised linear model.\n---\n\n\n\n\n## Statistical models\n\nModern science rests on several foundational pillars. Among these is the ability to construct sufficiently solid theoretical abstractions that are able to explain concrete observable aspects of the world. Once the applicability range of a theoretical model has been defined, it is usually compared to another model that is similar in its goal (i.e., another model that aims to explain similar aspects of the world). However, we rarely directly compare theoretical models. Instead, we are brought to compare statistical models that aim to represent theories.\n\nAccording to Rouder, Morey, & Wagenmakers (2016), \"Models are devices that connect theories to data. A model is an instantiation of a theory as a set of probabilistic statements\". One common and convenient example of such instantiation is the linear model, which --in its general form-- allows to predict parameter(s) of a distribution, which is supposed to reflect the distribution from which the observed data is issued (the data generation process).\n\nBut what does it mean for a model to *predict* something? In the current post, I focus on four R functions (the `predict`, `fitted`, `residuals` and `simulate` functions), exploring the similarities and differences between these functions, to illustrate the mechanisms and assumptions of the generalised linear model.\n\nThe usual linear model is of the following form.\n\n$$\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Normal}(\\mu_{i}, \\sigma) \\\\\n\\mu_{i} &= \\alpha + \\beta \\cdot \\text{x}_{i} \\\\\n\\end{aligned}\n$$\n\nWhere, in Bayesian terms, the first line of the model corresponds to the *likelihood* of the model, which is the assumption made about the data generation process. We make the assumption that the outcomes $y_{i}$ are normally distributed around a mean $\\mu_{i}$ with some error $\\sigma$. This is equivalent to say that the errors are normally distributed around $0$.\n\nOf course, the distributional assumption is not restricted to be Gaussian, and can be adapted to whatever distribution that makes sense in consideration of the data at hand. The linear aspect of the linear model actually refers to the second line of the above description, in which one tries to predict parameters of the distribution (e.g., $\\mu_{i}$ or $\\sigma$) by a *linear combination* of some predictor variable(s). Generalising to other distributions, the generalised linear model can be rewritten as:\n\n$$\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{D}(f(\\eta_{i}), \\theta) \\\\\n\\eta &= \\mathbf{X} \\beta \\\\\n\\end{aligned}\n$$\n\nWhere the response $y_{i}$ is **predicted** through the linear combination $\\eta$ of predictors transformed by the inverse link function $f$, assuming a certain distribution $D$ for $y$ (also called the *family*), and family-specific parameters $\\theta$ (BÃ¼rkner, 2017).\n\nBelow, we illustrate a simple Gaussian linear model using the `Howell1` dataset from the `rethinking` package (McElreath, 2016), which contains data about 544 individuals, including height (centimetres), weight (kilograms), age (years) and gender (0 indicating female and 1 indicating male).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rethinking)\nlibrary(tidyverse)\nlibrary(ggExtra)\n\ndata(Howell1)\nd <- Howell1 %>% filter(age >= 18)\n\np <- d %>%\n  ggplot(aes(x = weight, y = height) ) +\n  geom_point(pch = 21, color = \"white\", fill = \"black\", size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", colour = \"black\") +\n  theme_bw(base_size = 12)\n\nggMarginal(p, type = \"histogram\")\n```\n\n::: {.cell-output-display}\n![](2018-01-20-glm_files/figure-html/marginal-plot-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nA quick visual exploration of the dataset reveals a positive relationship between height and weight. The above plotted regression line corresponds to the following model, where we assume a normal likelihood:\n\n$$\n\\begin{aligned}\n\\text{height}_{i} &\\sim \\mathrm{Normal}(\\mu_{i}, \\sigma) \\\\\n\\mu_{i} &= \\alpha + \\beta \\cdot \\text{weight}_{i} \\\\\n\\end{aligned}\n$$\n\nThis model can be fitted easily in `R` with the following syntax.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(mod1 <- lm(height ~ weight, data = d) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = height ~ weight, data = d)\n\nCoefficients:\n(Intercept)       weight  \n    113.879        0.905  \n```\n\n\n:::\n:::\n\n\nThe intercept (113.879) represents the predicted height when weight is at 0 (which makes no much sense in our case), whereas the slope (0.905) represents the change in height when weight increases by one unit (i.e., one kilogram).\n\n## Prediction is key: `predict` and `fitted`\n\nThe main advantage of the previous model is that it allows to make predictions for any value of $\\text{weight}$. In `R`, this is done using the aptly named `predict` function. For instance, we can ask our model what is the expected height for an individual of weight 43, which is equal to $\\alpha + \\beta \\cdot 43$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwght <- 43\n\nd %>%\n  ggplot(aes(x = weight, y = height) ) +\n  geom_line(aes(y = predict(mod1) ), size = 1) +\n  geom_point(size = 2, alpha = 0.2) +\n  geom_segment(\n    x = wght, xend = wght,\n    y = 0, yend = predict(mod1, newdata = data.frame(weight = wght) ),\n    linetype = 2, lwd = 0.5\n    ) +\n  geom_segment(\n    x = 0, xend = wght,\n    y = predict(mod1, newdata = data.frame(weight = wght) ),\n    yend = predict(mod1, newdata = data.frame(weight = wght) ),\n    linetype = 2, lwd = 0.5\n    ) +\n  theme_bw(base_size = 12)\n```\n\n::: {.cell-output-display}\n![](2018-01-20-glm_files/figure-html/predict-example-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nImplementing the function `predict` by hand is quite easy and will allow us to better understand how it works. This function is actually simply retrieving parameters of the fitted model (in our case, the intercept and the slope) to make predictions about the outcome variable, given some values of the predictor(s). In other words, it corresponds to the second line of our model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- d %>%\n  mutate(\n    pred_mod1 = predict(mod1),\n    pred_mod1_2 = coef(mod1)[1] + coef(mod1)[2] * weight\n    )\n\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   height   weight age male pred_mod1 pred_mod1_2\n1 151.765 47.82561  63    1  157.1630    157.1630\n2 139.700 36.48581  63    0  146.9001    146.9001\n3 136.525 31.86484  65    0  142.7180    142.7180\n4 156.845 53.04191  41    1  161.8839    161.8839\n5 145.415 41.27687  51    0  151.2362    151.2362\n6 163.830 62.99259  35    1  170.8895    170.8895\n```\n\n\n:::\n:::\n\n\nWe could also be interested in predicting the height of individuals with other weights that the weights we observed (e.g., weights between 80 and 100 kgs). Below we simulate new data from our model (i.e., we simulate heights) and predictions for this new set of data (i.e., the $\\mu_{i}$).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# generating weights from 80 to 100 kgs\ndata.frame(weight = 80:100) %>%\n  # retrieving mod1 predictions\n  mutate(pred = predict(mod1, newdata = .) ) %>%\n  # simulating data from our model, taking into account sigma\n  mutate(sim =  pred + rnorm(length(weight), 0, sd(residuals(mod1) ) ) ) %>%\n  # or using sigma(mod1) instead of sd(residuals(mod1) ), as suggested\n  # by Tristan Mahr (https://www.tjmahr.com)\n  mutate(sim =  pred + rnorm(length(weight), 0, sigma(mod1) ) ) %>%\n  # plotting these predictions\n  ggplot(aes(x = weight, y = pred) ) +\n  geom_line(size = 1) +\n  geom_point(aes(x = weight, y = sim), size = 2) +\n  geom_segment(\n      aes(xend = weight, yend = sim),\n      size = 0.5, alpha = 0.5, lineend = \"round\"\n      ) +\n  theme_bw(base_size = 12)\n```\n\n::: {.cell-output-display}\n![](2018-01-20-glm_files/figure-html/pred-simulations-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWhere the vertical lines represent deviations from the predicted values. OK, so we've seen the the `predict` function simply uses the linear model to make predictions about the $\\mu_{i}$.\n\nYou might know that there exists a similar function, the `fitted` function, which allows to extract fitted values of a model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- d %>% mutate(fitted_mod1 = fitted(mod1) )\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   height   weight age male pred_mod1 pred_mod1_2 fitted_mod1\n1 151.765 47.82561  63    1  157.1630    157.1630    157.1630\n2 139.700 36.48581  63    0  146.9001    146.9001    146.9001\n3 136.525 31.86484  65    0  142.7180    142.7180    142.7180\n4 156.845 53.04191  41    1  161.8839    161.8839    161.8839\n5 145.415 41.27687  51    0  151.2362    151.2362    151.2362\n6 163.830 62.99259  35    1  170.8895    170.8895    170.8895\n```\n\n\n:::\n:::\n\n\nSurprisingly, the `predict` and `fitted` functions seem to do the exact same thing (at least their results are the same)... but do they? To answer this question, let's ask another one.\n\n## Link function, toward GLMs\n\nCan we predict gender by individual height? The usual way to answer this kind of question is through a *logistic regression* model (or logit model). Logistic regression is used to model binary outcome variables, using the linear regression framework. In the logit model, the *log-odds* of the outcome $p_{i}$ are modelled as a linear combination of the predictor variables:\n\n$$\nlogit(p_{i}) = log\\Big(\\frac{p_{i}}{1 - p_{i}}\\Big) = \\alpha + \\beta _{1} x_{1} + \\cdots + \\beta _{n} x_{n}\n$$\n\nThus, although the observed outcome variable is a dichotomic variable, the logistic regression estimates the **log-odds**, as a continuous variable, that the outcome variable is in a certain state (in our case, that the individual is a man)^[For more details on the logit model, please check this [previous post](../2017-08-05-absenteeism1/2017-08-05-absenteeism1.qmd).]. This model can be written as follows, where $p_{i}$ is the probability that an individual is a man.\n\n$$\n\\begin{aligned}\n\\text{gender}_{i} &\\sim \\mathrm{Binomial}(1, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha + \\beta \\cdot \\text{height}_{i} \\\\\n\\end{aligned}\n$$\n\nThis model is implemented easily in R using the `glm` function, where the `family` argument is used to specify the likelihood of the model, and the link function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(mod2 <- glm(male ~ height, data = d, family = binomial(link = \"logit\") ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glm(formula = male ~ height, family = binomial(link = \"logit\"), \n    data = d)\n\nCoefficients:\n(Intercept)       height  \n   -53.3653       0.3438  \n\nDegrees of Freedom: 351 Total (i.e. Null);  350 Residual\nNull Deviance:\t    486.6 \nResidual Deviance: 258.7 \tAIC: 262.7\n```\n\n\n:::\n:::\n\n\nBelow we print predictions of the model, using both the `predict` and `fitted` functions.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- d %>%\n  mutate(\n    pred_mod2 = predict(mod2),\n    fitted_mod2 = fitted(mod2)\n    )\n\nd %>%\n  select(height, weight, male, pred_mod2, fitted_mod2) %>%\n  head\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   height   weight male  pred_mod2 fitted_mod2\n1 151.765 47.82561    1 -1.1910139 0.233077651\n2 139.700 36.48581    0 -5.3387584 0.004778882\n3 136.525 31.86484    0 -6.4302701 0.001609421\n4 156.845 53.04191    1  0.5554049 0.635388646\n5 145.415 41.27687    0 -3.3740373 0.033116790\n6 163.830 62.99259    1  2.9567306 0.950580634\n```\n\n\n:::\n:::\n\n\nThis time the results of `predict` and `fitted` appear to be quite different... We can plot the predictions of `mod2` following the same strategy as previously using `fitted`. The `logit_dotplot` function displays the prediction of the logit model along with the marginal distribution of height by gender (detailed code can be found [here](https://github.com/lnalborczyk/lnalborczyk.github.io/blob/master/code/logit_dotplot.R) ).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsource(\"logit_dotplot.R\")\nlogit_dotplot(d$height, d$male, xlab = \"height\", ylab = \"p(male)\")\n```\n\n::: {.cell-output-display}\n![](2018-01-20-glm_files/figure-html/logit_dotplot-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe output of the `predict` and `fitted` functions are different when we use a GLM because the `predict` function returns predictions of the model on the scale of the linear predictor (here in the log-odds scale), whereas the `fitted` function returns predictions on the scale of the response. To obtain the *fitted* values, we thus have to apply the inverse of the link function to the predicted values obtained with `predict`. In our case, this translates to the logistic transformation:\n\n$$p = \\exp(\\alpha) / (1 + \\exp(\\alpha) )$$\n\nWhich, in R, gives:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexp(coef(mod2) ) / (1 + exp(coef(mod2) ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)       height \n6.664323e-24 5.851092e-01 \n```\n\n\n:::\n:::\n\n\nWhich is equivalent to using the `plogis` function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplogis(coef(mod2) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)       height \n6.664323e-24 5.851092e-01 \n```\n\n\n:::\n:::\n\n\nLet's compare our previous calls to the `predict` and `fitted` functions...\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- d %>%\n  mutate(pred_mod2_2 = plogis(predict(mod2) %>% as.numeric) )\n\nd %>%\n  select(height, weight, male, pred_mod2, fitted_mod2, pred_mod2_2) %>%\n  head\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   height   weight male  pred_mod2 fitted_mod2 pred_mod2_2\n1 151.765 47.82561    1 -1.1910139 0.233077651 0.233077651\n2 139.700 36.48581    0 -5.3387584 0.004778882 0.004778882\n3 136.525 31.86484    0 -6.4302701 0.001609421 0.001609421\n4 156.845 53.04191    1  0.5554049 0.635388646 0.635388646\n5 145.415 41.27687    0 -3.3740373 0.033116790 0.033116790\n6 163.830 62.99259    1  2.9567306 0.950580634 0.950580634\n```\n\n\n:::\n:::\n\n\nTo sum up, the `fitted` function automatically applies the inverse transformation to provide prediction on the scales of the outcome. A similar behaviour can be obtained by using the `predict` function, and by specifying the scale in which we want to obtain predictions (e.g., in the scale of the response variable).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- d %>%\n  mutate(pred_mod2_3 = predict(mod2, type = \"response\") )\n\nd %>%\n  select(height, weight, male, pred_mod2, fitted_mod2, pred_mod2_2, pred_mod2_3) %>%\n  head\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   height   weight male  pred_mod2 fitted_mod2 pred_mod2_2 pred_mod2_3\n1 151.765 47.82561    1 -1.1910139 0.233077651 0.233077651 0.233077651\n2 139.700 36.48581    0 -5.3387584 0.004778882 0.004778882 0.004778882\n3 136.525 31.86484    0 -6.4302701 0.001609421 0.001609421 0.001609421\n4 156.845 53.04191    1  0.5554049 0.635388646 0.635388646 0.635388646\n5 145.415 41.27687    0 -3.3740373 0.033116790 0.033116790 0.033116790\n6 163.830 62.99259    1  2.9567306 0.950580634 0.950580634 0.950580634\n```\n\n\n:::\n:::\n\n\nTo relate this to our understanding of the linear model equation, prediction pertains to the $\\mu_{i}$, meaning that we try to predict the mean observed outcome for a specific value of the predictor $x_{i}$.\n\nWe should go further and implement uncertainty in these predictions, but we should first take a break to examine the concepts of errors and residuals, and their relationship.\n\n## Errors and residuals: the `residuals` function\n\nA very common fallacy about the assumptions of the linear (Gaussian) model is that the *outcome variable* should be normally distributed. Instead, this assumption concerns the distribution of the outcome variable **around its predicted value** (i.e., the distribution of the errors).\n\nWhat we actually said above is that the errors $\\epsilon_{i}$ should be normally distributed around the predicted value. But the errors are the the non-observed (and non-observable) differences between the theoretical predicted value $\\mu$ and the observed outcomes. Consequently, we do not have access to it. Instead, what we can work with are the residuals $e_{i}$, which can be seen as an estimate (from the sample) of the errors $\\epsilon_{i}$, in a similar way as $b$ is an estimate of $\\beta$. To sum up, the residuals are the $e_{i} = y_{i} - \\bar{X}$ whereas the errors are the $\\epsilon_{i} = y_{i} - \\mu_{i}$.\n\nIn other words, errors pertain to the data generation process, whereas residuals are the difference between the model's estimation and the observed outcomes. Basically, the residuals are the difference between the observed value and the predicted value. We can obtain them easily using the `residuals` function (which is useful for more complex models), or by subtracting to each observed outcome $y_{i}$ the predicted $\\mu_{i}$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- d %>%\n  mutate(\n    res1 = residuals(mod1),\n    res2 = height - pred_mod1\n    )\n\nd %>%\n  select(height, weight, male, pred_mod1, res1, res2) %>%\n  head\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   height   weight male pred_mod1      res1      res2\n1 151.765 47.82561    1  157.1630 -5.397960 -5.397960\n2 139.700 36.48581    0  146.9001 -7.200111 -7.200111\n3 136.525 31.86484    0  142.7180 -6.193000 -6.193000\n4 156.845 53.04191    1  161.8839 -5.038870 -5.038870\n5 145.415 41.27687    0  151.2362 -5.821164 -5.821164\n6 163.830 62.99259    1  170.8895 -7.059520 -7.059520\n```\n\n\n:::\n:::\n\n\nBelow we plot these residuals, and make the alpha (i.e., the transparency) and the size of the points dependent on the distance to the predicted value (so that larger residuals appear as bigger and less transparent). This distance is also represented by the length of the vertical lines.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd %>%\n  sample_frac(0.5) %>% # selecting a (50%) subsample of the data\n  ggplot(aes(x = weight, y = height) ) +\n  geom_line(aes(y = pred_mod1), size = 1) +\n  geom_point(aes(alpha = abs(res1), size = abs(res1) ) ) +\n  guides(alpha = FALSE, size = FALSE) +\n  geom_segment(aes(xend = weight, yend = pred_mod1, alpha = abs(res1) ) ) +\n  theme_bw(base_size = 12)\n```\n\n::: {.cell-output-display}\n![](2018-01-20-glm_files/figure-html/abs-residuals-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nIf we take all the above verticals bars (i.e., the residuals) and plot their distribution, we can compare them to a normal distribution with mean 0, and standard deviation $\\sigma$ equal to the standard deviation of the residuals, to check our assumption about the distribution of the residuals (here a normality assumption).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd %>%\n  ggplot(aes(x = res1) ) +\n  geom_histogram(aes(y = ..density..), bins = 20, alpha = 0.6) +\n  geom_line(aes(y = dnorm(res1, mean = 0, sd = sd(res1) ) ), size = 1) +\n  guides(fill = FALSE) +\n  theme_bw(base_size = 12) +\n  labs(x = \"Residuals\", y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](2018-01-20-glm_files/figure-html/distribution-residuals-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nTo sum up, the distributional assumption of the linear model (Gaussian or whatever) concerns the distribution of the errors, that we do not know. Instead, we can evaluate the distribution of the residuals, which are the observed differences between the outcome variable and the predictions of the model.\n\n## Prediction uncertainty: the `simulate` function\n\nRecall that our model is:\n\n$$\n\\begin{aligned}\n\\text{height}_{i} &\\sim \\mathrm{Normal}(\\mu_{i}, \\sigma) \\\\\n\\mu_{i} &= \\alpha + \\beta \\cdot \\text{weight}_{i} \\\\\n\\end{aligned}\n$$\n\nSo far, we used the linear part of the linear model (i.e., the second line) to make predictions about the $\\mu_{i}$, from $\\alpha$ and $\\beta$. In other words, we made predictions about mean values of the Gaussian distribution for a specific value of `weight`.\n\nNow, if we want to generate actual data (i.e., the $y_{i}$), from our model, we still have to include $\\sigma$. This can be done \"by-hand\", by adding error (i.e., some random variation) to our predictions.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- d %>%\n  mutate(\n    sim1 = coef(mod1)[1] + coef(mod1)[2] * d$weight + rnorm(1, 0, sigma(mod1) )\n    )\n\nd %>%\n  select(height, weight, male, pred_mod1, sim1) %>%\n  head\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   height   weight male pred_mod1     sim1\n1 151.765 47.82561    1  157.1630 166.1058\n2 139.700 36.48581    0  146.9001 155.8430\n3 136.525 31.86484    0  142.7180 151.6609\n4 156.845 53.04191    1  161.8839 170.8268\n5 145.415 41.27687    0  151.2362 160.1790\n6 163.830 62.99259    1  170.8895 179.8324\n```\n\n\n:::\n:::\n\n\nThe `simulate` function allows to automate the previous step and to run it `nsims` times.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnsims <- 1e4\nsims <- simulate(mod1, nsim = nsims) %>% data.frame\n\nsims[1:6, 1:6]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     sim_1    sim_2    sim_3    sim_4    sim_5    sim_6\n1 153.7530 154.5991 152.1783 152.1764 166.4657 152.1244\n2 148.1678 150.9586 155.5804 144.2313 161.7476 149.9280\n3 144.0021 141.7363 139.5549 137.0940 140.7992 145.3508\n4 159.7125 160.7178 158.2824 158.8875 161.3088 163.5873\n5 149.8344 156.1012 150.1331 159.9842 160.2862 151.8925\n6 167.3291 177.1412 166.7218 178.6586 173.9297 169.8771\n```\n\n\n:::\n\n```{.r .cell-code}\nlower_ci_sim <- apply(sims, 1, function(x) quantile(x, probs = 0.025) )\nupper_ci_sim <- apply(sims, 1, function(x) quantile(x, probs = 0.975) )\n\nsims_summary <- data.frame(\n  lower = lower_ci_sim,\n  upper = upper_ci_sim\n  )\n\nd %>%\n  ggplot(aes(x = weight, y = height) ) +\n  geom_smooth(method = \"lm\", color = \"black\") +\n  geom_point(size = 2, alpha = 0.3) +\n  geom_ribbon(\n    data = sims_summary, inherit.aes = FALSE,\n    aes(x = d$weight, ymin = lower, ymax = upper), alpha = 0.1\n    ) +\n  theme_bw(base_size = 12)\n```\n\n::: {.cell-output-display}\n![](2018-01-20-glm_files/figure-html/simulate-ntimes-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe first shaded region around the regression line represents the 95% confidence interval around the regression line, whereas the second broader shaded area represents the 95% central quantiles of the Gaussian distribution at each value of weight. These are computed by simulating `nsims` height values at each weight value, and by taking the 95% mean values of this distribution.\n\n## Conclusions\n\nIn this post, we tried to shed light on some concepts of the generalised linear model by examining the similarities and differences between four common R functions. It should be acknowledged that these functions (`predict`, `fitted`, `residuals`, and `simulate`) work with many different types of models, including models fitted with `lme4` or `brms`. For instance, here is an overview of the available methods in my current session.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmethods(predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] predict.ar*                predict.Arima*            \n [3] predict.arima0*            predict.bam*              \n [5] predict.brmsfit*           predict.bs*               \n [7] predict.bSpline*           predict.emmGrid*          \n [9] predict.gam*               predict.glm               \n[11] predict.glmmPQL*           predict.gls*              \n[13] predict.gnls*              predict.HoltWinters*      \n[15] predict.jam*               predict.lda*              \n[17] predict.lm                 predict.lme*              \n[19] predict.lmList*            predict.lmList4*          \n[21] predict.loess*             predict.lqs*              \n[23] predict.mca*               predict.merMod*           \n[25] predict.mlm*               predict.nbSpline*         \n[27] predict.nlme*              predict.nls*              \n[29] predict.npolySpline*       predict.ns*               \n[31] predict.pbSpline*          predict.polr*             \n[33] predict.poly*              predict.polySpline*       \n[35] predict.ppolySpline*       predict.ppr*              \n[37] predict.prcomp*            predict.princomp*         \n[39] predict.qda*               predict.rlm*              \n[41] predict.smooth.spline*     predict.smooth.spline.fit*\n[43] predict.StructTS*         \nsee '?methods' for accessing help and source code\n```\n\n\n:::\n:::\n\n\n## References\n\n<details>\n  <summary>Click to expand</summary>\n\n<div markdown=\"1\">\n\nBÃ¼rkner, P.-C. (2017). brms: An R package for bayesian multilevel models using Stan. Journal of Statistical Software, 80 (1), 1â28. doi:10.18637/jss.v080.i01\n\nMcElreath, R. (2016). Statistical Rethinking (p. 469). Chapman; Hall/CRC.\n\nRouder, J. N., Morey, R. D., & Wagenmakers, E.-J. (2016). The Interplay between Subjectivity, Statistical Practice, and Psychological Science. Collabra, 2(1), 1â12. doi:10.1525/collabra.28\n\n</div>\n\n</details>\n",
    "supporting": [
      "2018-01-20-glm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}