{
  "hash": "30314ae9b927c1d3142e094195f88dee",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Experimental absenteeism and logistic regression - Part I\nauthor: Ladislas Nalborczyk\ndate: 2017-08-05\ncategories:\n  - R\n  - Bayesian\n  - Logistic regression\ndescription: This post aims to assess the average probability of participant presence in psychological experiments and, in the meantime, to introduce Bayesian logistic regression using `R` and the `rethinking` package.\naliases:\n  - /post/absenteeism/\n---\n\n::: {.cell}\n\n:::\n\n\nOne of the greatest aspect of being a PhD student in experimental psychology is to have the immeasurable pleasure to carry experiments with human subjects. Sure, it comes with a few disagreements, like having to teach undergraduate students how to use online schedule appointers (e.g., [doodle](https://doodle.com)), or spending a few hours per day answering clumsy e-mails asking for the location of the experimental room... and last but not least, being in a constant state of ignorance about the presence or the absence of the so-much expected student-guinea-pig.\n\nAfter many speculative and fruitless conversations about the rate of students' participation to psychological studies, I have finally decided to make use of the data harvested along my PhD studies to answer the ultimate question: can we predict the participation of students?\n\n## Which strategy?\n\nOn a practical ground, I have took advantage of the fact that a friend and I had to recruit participants for some studies of our PhD. We then simply started to systematically write down which participants did or did not come to their appointment, as well as information about their participation, like the mode of recruitment (online versus IRL registration), the fact that we sent a reminder e-mail (or not), and the day of the week. To sum up, the basic question was to know whether we can estimate the **probability** that a registered participant will come to its appointment, based on the information we collected.\n\n## What is a logistic regression?\n\nLogistic regression (also called a logit model) is used to model binary outcome variables (e.g., \"absent\" versus \"present\"), using the general regression framework.\n\nIn the logit model, the [log-odds](https://en.wikipedia.org/wiki/Odds) of the outcome $p_{i}$ are modelled as a linear combination of the predictor variables:\n\n$$logit(p_{i}) = log\\Big(\\frac{p_{i}}{1-p_{i}}\\Big)= \\alpha + \\beta _{1} x_{1} + \\cdots + \\beta _{n} x_{n}$$\n\nThus, although the observed dependent variable is a dichotomic variable, the logistic regression estimates the **log-odds**, as a continuous variable, that the dependent variable is in one state or the other.\n\nTo clarify the mechanism of the logit model, we will go through a first simple example, trying to estimate the overall probability of participants being present.\n\n## Step 1: Getting the data\n\nData can be retrieved directly from GitHub with:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(data <- read.csv(\"http://raw.githubusercontent.com/lnalborczyk/old_blog/master/_posts/absenteeism/absence.csv\", stringsAsFactors = FALSE) )\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n         day inscription reminder absence presence total\n1     Friday      doodle       no       7       11    18\n2     Friday      doodle      yes       0        2     2\n3     Friday       panel      yes       0       10    10\n4     Monday      doodle       no       5        4     9\n5     Monday      doodle      yes       2        6     8\n6     Monday       panel      yes       6       12    18\n7   Thursday      doodle       no       3       11    14\n8    Tuesday      doodle       no       4       10    14\n9    Tuesday      doodle      yes       1        7     8\n10   Tuesday       panel      yes       0        9     9\n11 Wednesday      doodle       no       6       11    17\n12 Wednesday      doodle      yes       0        4     4\n13 Wednesday       panel      yes       0       14    14\n```\n\n\n:::\n:::\n\n\nThe `inscription` column describes the mode of inscription of students (i.e., whether they registered for the experiment online, or IRL) while the `reminder` column indicates whether a reminder e-mail was sent to the participant prior to the experiment. The three last columns represent counts of present, absent, and total number of participants, respectively, for a grand total of 145 participants.\n\n## Step 2: Writing down a first model\n\nThe first step of every Bayesian analysis (oops, did I mention that this post will be Bayesian?) is to specify the full probability model. It starts with describing the data-generating process from which the data is issued (i.e., the *likelihood*). In our case, observations $y_{i}$ are distributed according to a Binomial distribution:\n\n$$y_{i} \\sim \\mathrm{Binomial}(n_{i}, p_{i})$$\n\nwhere $y_{i}$ is a count, $p_{i}$ is the probability of any particular \"trial\" and $n_{i}$ is the total number of observations. Our aim will be to predict the probability $p_{i}$ of each trial $i$.\n\nWith this first model, we want to estimate the mean probability of tha participant being present. We can model this situation by an intercept-only model, as following:\n\n$$\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i})\\\\\nlogit(p_{i}) &= \\alpha\\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10)\\\\\n\\end{aligned}\n$$\n\nIn this model, we state that the **log-odds** (or the **logit**) of $p_{i}$ are given by a single parameter $\\alpha$, the intercept, on which we assign a very vague prior (note that this prior is expressed in the log-odds space, and not directly in the space of the outcome).\n\n## Step 3: Fitting the model with rethinking\n\nThroughout this post, we will use the [rethinking package](https://github.com/rmcelreath/rethinking), as it allows to fit a model using almost the same syntax as the mathematical one. For instance, the model we described above would look like:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rethinking)\nlibrary(tidyverse)\nlibrary(BEST)\n\nmod1 <-\n    rethinking::map(\n        alist(\n            presence <- dbinom(total, p),\n            logit(p) <- a,\n            a ~ dnorm(0, 10) ),\n        data = data)        \n```\n:::\n\n\nThe two first lines state that we fit the model using the `map` function, which requires a model expressed as an `alist` object (i.e., a non-evaluated `list` object). The three next lines describe the model very similarly to the mathematical description.\n\nA brief summary of the model can be obtained using the `precis` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprecis(mod1, prob = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mean        sd      2.5%    97.5%\na 1.182719 0.1959511 0.7986619 1.566776\n```\n\n\n:::\n:::\n\n\nwhich gives the mode of the posterior distribution for the intercept $\\alpha$ along with the standard deviation of the distribution as well as the credible intervals. The intercept represents the estimated **log-odds** of being present, which can be translated back to the overall probability of being present, as we know that:\n\n$$p = \\exp(\\alpha) / (1 + \\exp(\\alpha) )$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexp(coef(mod1)) / (1 + exp(coef(mod1) ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        a \n0.7654363 \n```\n\n\n:::\n:::\n\n\nwhich is equivalent to using the `plogis` function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplogis(coef(mod1) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        a \n0.7654363 \n```\n\n\n:::\n:::\n\n\nSo we already know that there is an estimated 0.77 probability that a registered participant will come to his appointment...and if the intercept represents the **log-odds** of being present, a simple exponential transformation allows to get back to odds:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexp(coef(mod1) ) # odds of presence\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       a \n3.263235 \n```\n\n\n:::\n:::\n\n\nwhich can be interpreted directly by saying that the odds of a registered participant being present are of 3.26 (or 3.26:1)[^1].\n\n## Step 4: Adding predictors\n\nNice, but we said that the logistic regression allows to map a relationship between a binary outcome variable and a linear combination of predictor variables, so let's add predictors.\n\n### Effects of the reminder\n\nFor instance, we might be interested in knowing whether sending a reminder e-mail has an impact on the presence. We first start by dummy-coding our predictors.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata <-\n    data %>%\n    mutate(\n        reminder = ifelse(reminder == \"no\", 0, 1),\n        inscription = ifelse(inscription == \"panel\", 0, 1) )\n```\n:::\n\n\nThe model that map the outcome and the effects of the reminder can be expressed as follows:\n\n$$\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha + \\beta \\times \\text{reminder} \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\end{aligned}\n$$\n\nwhere the effects of the reminder on the log-odds of being present are realised through the slope $\\beta$, which is also assigned a very vague prior. This model is fitted with `rethinking` using a very similar syntax to previously:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod2 <-\n    rethinking::map(\n        alist(\n            presence <- dbinom(total, p),\n            logit(p) <- a + b * reminder,\n            a ~ dnorm(0, 10),\n            b ~ dnorm(0, 10) ),\n        data = data)\n```\n:::\n\n\nFirst, we might want to compare the *efficiency* of these two models. The `compare` function allows to compare models fit on the same data using the Wattanabe Akaike Information Criterion (WAIC; Watanabe, 2010), a generalisation of the AIC, previously discussed on this blog [here](http://www.barelysignificant.com/aicbic/). This criterion can be seen as an approximation of the out-of-sample deviance, and in simple words, of the predictive abilities of the model (McElreath, 2016).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompare(mod1, mod2) # yeah, model 2 wins\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         WAIC       SE    dWAIC      dSE    pWAIC     weight\nmod2 48.33272 8.926933 0.000000       NA 3.534778 0.97616377\nmod1 55.75756 7.308822 7.424847 9.667886 1.942421 0.02383623\n```\n\n\n:::\n:::\n\n\nThis comparison reveals that the second model is better in the sense of the WAIC (as for the deviance, the lower is the better), and that the parameter we added (i.e., the slope $\\beta$) improved the predictive abilities of the model.\n\nBy adding a slope in the model, the intercept now represents the **log-odds** for the no-reminder condition (as it was coded as 0), while the coefficient for `reminder` represents the log odds-ratio between the reminder and the no-reminder groups. We can then obtain the **odds-ratio** by simply exponentiating the slope:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexp(coef(mod2)[2]) # odds ratio between no-reminder and reminder\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       b \n3.774536 \n```\n\n\n:::\n:::\n\n\nwhich can be read as a propotionnal increase of 3.77 in the **odds** of being present when a reminder is sent. We also might be interested in the absolute change in probability, rather than the **odds ratio**. To answer this question, we first extract the samples from the posterior distribution estimated by this second model and store it in `post1`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npost1 <- extract.samples(mod2) # extracting posterior samples\n```\n:::\n\n\nThen, from these posterior samples, we can compute almost all statistics of interest. For instance we can compute the probability of being present according to the reminder status.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np.no <- plogis(post1$a) # mean probability of presence when no reminder\np.yes <- plogis(post1$a + post1$b) # mean probability of presence when reminder\nplotPost(p.yes - p.no, compVal = 0, showMode = TRUE, xlab = \"\")      \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThis histogram depicts the posterior distribution of the difference of probability of being present by comparing the no-reminder and the reminder groups. In other words, it represents an estimation of the effect of the reminder, while the mode of the distribution informs us about the most probable value of increase in probability due to the reminder.\n\n### Effects of the mode of inscription\n\nLikewise, we could be interested in the maybe more subtle effects of the mode of inscription on the probability of presence. This model is expressed very similarly as the previous one:\n\n$$\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha + \\beta \\times \\text{inscription} \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\end{aligned}\n$$\n\nwhere $\\beta$ now represents the effects of the mode of inscription (i.e., online versus IRL).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod3 <-\n    rethinking::map(\n        alist(\n            presence <- dbinom(total, p),\n            logit(p) <- a + b * inscription,\n            a ~ dnorm(0, 10),\n            b ~ dnorm(0, 10) ),\n        data = data)\n```\n:::\n\n\nand we compare it to the intercept-only model in the same way as before:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompare(mod1, mod3) # yeah, model 3 wins\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         WAIC       SE    dWAIC      dSE    pWAIC    weight\nmod3 54.26925 9.913801 0.000000       NA 4.898551 0.6515147\nmod1 55.52065 7.267908 1.251408 10.70296 1.794424 0.3484853\n```\n\n\n:::\n:::\n\n\nthis time again the model comparison tells us that the inclusion of this predictor in the model improves the predictions abilities of the model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npost2 <- extract.samples(mod2)\np.panel <- plogis(post2$a) # mean probability of presence for panel\np.doodle <- plogis(post2$a + post2$b) # mean probability of presence for doodle\nplotPost(p.doodle - p.panel, compVal = 0, showMode = TRUE, xlab = \"\")    \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe obtain an estimation of the effect similar to the effect of the reminder. In other words, the mode of inscription seems to be as much important as the reminder, to predict the presence of participants...\n\n## Building the full model: multi-what?\n\nWe are now going to buil a last model that includes both `reminder` and `incription` as predictors.\n\n$$\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha + \\beta_{r} \\times \\text{reminder} + \\beta_{i} \\times \\text{inscription} \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta_{r} &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta_{i} &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\end{aligned}\n$$\n\nAs previously, the `rethinking` model follows a similar syntax, with the only new thing here is the compact specification of the prior for the two slopes, using `c()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod4 <- rethinking::map(\n    alist(\n        presence <- dbinom(total, p),\n        logit(p) <- a + br * reminder + bi * inscription,\n        a ~ dnorm(0, 10),\n        c(br, bi) ~ dnorm(0, 10) ),\n    data = data)\n```\n:::\n\n\nAdding that the output of `precis` can be directly plotted.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(precis(mod4, prob = 0.95) )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThis figure shows the mean of the posterior distribution of each parameter of the model along with 95% credible intervals. Wait...why `inscription` does not seem to have an influence anymore? Let's compare the four models we built to better understand what's happening here.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompare(mod1, mod2, mod3, mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         WAIC       SE    dWAIC      dSE    pWAIC     weight\nmod2 48.21541 8.781045 0.000000       NA 3.443579 0.71913505\nmod4 50.60255 9.641261 2.387147 1.314572 4.630774 0.21799578\nmod3 53.80681 9.754853 5.591401 3.282625 4.667485 0.04391907\nmod1 55.48789 7.258086 7.272480 9.580430 1.773878 0.01895010\n```\n\n\n:::\n:::\n\n\nOk so model 4 performs worse than the others, and the best model seems to be the second model (effet of `reminder` only). Why `inscription` does not seem to have an influence anymore, in the full model? Oh yeah... I forgot to tell you that we were two experimenters running experiments and collecting data about absenteeism. I am used to recruit participants only through doodle and this time I was too ~~busy~~ lazy to send a reminder to each participant (and so sometimes I forgot), while my colleague recruited participants through an IRL panel and sent an e-mail reminder to each participant. In other words, it means that the `reminder` and the `inscription` variables are almost perfectly counfounded (i.e., \"correlated\").\n\nA simple way to realise how problematic is the situation is to draw the contingency table.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata %>% # contingency table\n    group_by(inscription, reminder) %>%\n    summarise(n = sum(total) ) %>%\n    spread(key = reminder, value = n) %>% data.frame\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  inscription X0 X1\n1           0 NA 51\n2           1 72 22\n```\n\n\n:::\n:::\n\n\nWe notice first that the amount of data in each condition is very disproportionnal and that we completely lack data for the condition `panel` and `no reminder`.\n\nWe generally refer to this situation (when two or more predictors are highly correlated) as **multi-collinearity**. It basically mean that the two predictor variables carry almost the same information. What the model comparison is telling us is then simply that there is no benefit in adding a second predictor if it brings the same information as the first predictor... makes sense no?\n\nThis redudancy can be illustrated by plotting the posterior samples of the two slopes each against the other:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nextract.samples(mod4) %>%\n    ggplot(aes(br, bi) ) +\n    geom_point(alpha = 0.6, color = \"steelblue\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nAll we can say is that there is obviously an influence of at least one of the predictors on the probability of the participants being present, but as they are confounded, we can not tell whether it is an effect of the reminder or an effect of the mode of inscription. However, if I had to guess, I would go for the reminder effect. Otherwise, instead of being writing a blog post, I would be writing a manuscript on the beneficial motivationnal aspects of registering participants IRL compared to registering them via online platforms.\n\n## Conclusions\n\nWell, know you now, sending a reminder e-mail *probably* increases the probability of participants being present, so don't be ~~busy~~ lazy.\n\n## References\n\n<details>\n  <summary>Click to expand</summary>\n\n<div markdown=\"1\">\n\nMcElreath, R. (2016). Statistical Rethinking. Chapman; Hall/CRC.\n\nWatanabe, S. (2010). Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory. *Journal of Machine Learning Research, 11*, 3571–3594.\n\n</div>\n\n</details>\n\n[^1]: See the [UCLA](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/) page for more details on the interpretation of the odds and log-odds in the context of logistic regression.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}