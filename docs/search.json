[
  {
    "objectID": "positions/index.html",
    "href": "positions/index.html",
    "title": "Open positions",
    "section": "",
    "text": "If you are interested in joining my team in Aix-en-Provence as a Master’s student, PhD student, or postdoctoral researcher, feel free to contact me at any time, even if no open position matching your profile is currently advertised."
  },
  {
    "objectID": "positions/index.html#postdoctoral-positions",
    "href": "positions/index.html#postdoctoral-positions",
    "title": "Open positions",
    "section": "Postdoctoral positions",
    "text": "Postdoctoral positions\nThere are currently no officially announced postdoctoral positions, but feel free to contact me at any time to inquire about opportunities."
  },
  {
    "objectID": "positions/index.html#phd-positions",
    "href": "positions/index.html#phd-positions",
    "title": "Open positions",
    "section": "PhD positions",
    "text": "PhD positions\nThere are currently no officially announced PhD positions, but feel free to contact me at any time to inquire about opportunities."
  },
  {
    "objectID": "positions/index.html#masters-degree-research-internships",
    "href": "positions/index.html#masters-degree-research-internships",
    "title": "Open positions",
    "section": "Master’s degree research internships",
    "text": "Master’s degree research internships\n\n\n\n\n\n\nUncovering the neural dynamics of inner speech from EEG signals\n\n\n\n\n\nSupervision: Ladislas Nalborczyk\nProblem description: TBD\nProject structure:\n\nUnderstanding the problem (literature work on inner speech and multivariate pattern analyses)\nImplementing (from partially existing code) and performing individual-level and group-level analyses (in Python using MNE-Python)\nDiscussing the results and writing a report (thesis)\n\nKey references:\n\nKing, J.-R., & Dehaene, S. (2014). Characterizing the dynamics of mental representations: The temporal generalization method. Trends in Cognitive Sciences, 18(4), 203–210. https://doi.org/10.1016/j.tics.2014.01.002\nLœvenbruck, H., Grandchamp, R., Rapin, L., Nalborczyk, L., Dohen, M., Perrier, P., Baciu, M., & Perrone-Bertolotti, M. (2018). A cognitive neuroscience view of inner language: To predict and to hear, see, feel. In P. Langland-Hassan & A. Vicente (Eds.), Inner speech: New voices (p. 37). Oxford University Press.\n\nComputer tools: Python, MNE-Python, scikit-learn, MVPA\nKey-words: Cognitive Neuroscience, Electroencephalography\n\n\n\n\n\n\n\n\n\nDoes the the corollary discharge provide the sensory content of inner speech?\n\n\n\n\n\nSupervision: Ladislas Nalborczyk\nProblem description: TBD\nProject structure:\n\nUnderstanding the problem (literature work on inner speech and internal models)\nImplementing the experiment (from partially existing code) in Python/PsychoPy\nCollecting the behavioural data\nAnalysing the behavioural data (in R)\nDiscussing the results and writing a report (thesis)\n\nKey references:\n\nDelem, M., Stauffert, N., Nguyen, N., Debarnot, U., Guillot, A., & Nalborczyk, L. (2024, March 27). Does the corollary discharge provide the sensory content of inner speech? A preregistered direct replication and extension of Scott (2013). https://doi.org/10.31234/osf.io/abps9\nScott, M. (2013). Corollary Discharge Provides the Sensory Content of Inner Speech. Psychological Science, 24(9), 1824–1830. https://doi.org/10.1177/0956797613478614\nLœvenbruck, H., Grandchamp, R., Rapin, L., Nalborczyk, L., Dohen, M., Perrier, P., Baciu, M., & Perrone-Bertolotti, M. (2018). A cognitive neuroscience view of inner language: To predict and to hear, see, feel. In P. Langland-Hassan & A. Vicente (Eds.), Inner speech: New voices (p. 37). Oxford University Press.\n\nComputer tools: R, Python, PsychoPy\nKey-words: Experimental Psychology, Psycholinguistics"
  },
  {
    "objectID": "blog/2018-07-12-corroboration1/index.html",
    "href": "blog/2018-07-12-corroboration1/index.html",
    "title": "The Meehlian Corroboration-Verisimilitude Theory of Science - Part I",
    "section": "",
    "text": "A scientific theory can be defined as a set of logical propositions that posits causal relationships between observable phenomena. These logical propositions are originally abstract and broad (e.g., “every object responds to the force of gravity in the same way”) but lead to concrete and specific predictions that are empirically testable (e.g., “the falling speed of two objects should be the same, all other things being equal”).\nThe concept of a scientific theory is not a unitary concept though. As an example, Meehl (1986) lists three kinds of theories:\n\nfunctional-dynamic theories which relate “states to states or events to events”. For instance, we say that when one variable changes, certain other variables change in such and such ways.\nstructural-compositional theories: the main idea is to explain what something is composed of, or what kind of parts it has and how they are put together.\nevolutionary theories that are about the history and/or development of things (e.g., Darwin’s theory, Wegener’s theory of continental drift, the fall of Rome, etc.).\n\nDespite this diversity of nature, and without any clear-cut consensus on what makes a good theory, philosophy of science provides some useful conceptual tools to “appraise” theories, to identify what makes them “strong” or “weak”, and to assess what makes a “strong test” or a “weak test” of a theory. Incidentally, describing these tools is the main goal of the current post, so let’s start.1\n\n\nThe first “problem” with theories is that we can not confirm them. According to Campbell (1990), the logical argument of science has the following form:\n\nIf Newton’s theory is true, then it should be observed that the tides have period B, the path of Mars shape C, the trajectory of a cannonball form D, etc.\nObservation confirms B, C, and D.\nTherefore Newton’s theory A is “true”.\n\nHowever, this argument is a fallacious argument known as the affirmation of the consequent. One way to realise it is to visualise it as a diagram (figure from Campbell, 1990):\n\n\n\n\n\nThe invalidity of this argument comes from the existence of the cross-hatched area, which contains other possible explanations (than Newton’s theory) for the observed data. However, the syllogism is not useless: we could have observed data that are inconsistent with B, C, and D, in turn impugning the “truth” of Newton’s theory. Also, if observations are confirmed, the theory remains one of the possible explanations for their existence (see our later discussion of the concept of corroboration). As put by Campbell (1990), there is “an important asymmetry between logically valid rejection and logically inconclusive confirmation”.\nThis, together with the Duhem–Quine problem (that we will discuss in length later on), makes any experimental corroboration (or falsification, as we will see) of theoretical predictions explicable by challenging the background assumptions, or the adequacy of the experimental apparatus, or asserting the presence of one of Meehl’s “crud factors”, etc. (Campbell, 1990).\n\n\n\nWe can not confirm theories, but maybe we can at least think of a way of disproving them? But what does it mean for a theory to be false? According to Popper’s view, a theory is falsifiable “if and only if there exists at least one potential falsifier—at least one possible basic statement that conflicts with it logically”. In other words, a theory can be considered as falsifiable if it can be shown to be false. Note that the falsifiability of early Popper concerns the problem of demarcation (i.e., what is science and what is pseudoscience), and defines pseudosciences as composed of non falsifiable theories (i.e., theories that does not allow the possibility of being disproved).\nBut when it comes to describe how science works (descriptive purposes) or to know how scientific enquiries should be lead (prescriptive purposes), science is usually not described by the falsification standard, as Popper himself recognized and argued. In fact, deductive falsification is impossible in nearly every scientific context (McElreath, 2016).\n\nIn the next sections, we discuss some of the reasons that prevent any theory to be strictly falsified (in a logical sense), namely: i) the distinction between theoretical and statistical models ii) the problem of measurement iii) the problem of continuous hypotheses, and iv) the Duhem-Quine problem.\n\n\nA statistical model is a device that connect theories to data. It can be defined as an instantiation of a theory as a set of probabilistic statements (Rouder, Morey, & Wagenmakers, 2016).\nTheoretical models and statistical models are usually not equivalent as many different theoretical models can correspond to the same probabilistic description, and reversely, different probabilistic descriptions can be derived from the same theoretical model. In other words, there is no one-to-one mapping between the two worlds, which render the induction from the statistical model to the theoretical model quite tricky.\nAs a consequence, observing consistent observations with some theory \\(T\\) does not “prove” in any way \\(T\\) (see previous section), as there could be many other theories that can predict these observations. Likewise, given that rejecting the straw-man null hypothesis can corroborate multiple different theories (as we could find many theories that predict something different from zero, to some extent), it acts as a very “weak corroborator” of any single theory.\n\n\n\nThe logic of falsification is pretty simple and rests on the power of the modus tollens2. This argument (whose exposition, for some reason, usually involves swans) can be presented as follows:\n\nIf my theory \\(T\\) is right then I should observe these data \\(D\\).\nI observe data that are not those I predicted \\(\\lnot D\\).\nTherefore, my theory is wrong \\(\\lnot T\\).\n\nThis argument is perfectly valid and works well for logical statements (statements that are either true or false). However, the first problem that arises when we try to apply this reasoning to the “real world” is the problem of observation error: observations are prone to error, especially at the boundaries of knowledge (McElreath, 2016).\nLet’s take an example from physics, related to the detection of faster than-light neutrinos (this example is discussed in the first chapter of McElreath, 2016). According to Einstein, neutrinos can not travel faster than the speed of light. Thus, any observation of faster-than-light neutrinos would act as a strong falsifier of Einstein’s special relativity.\nIn 2011 however, a large team of respected physicists announced the detection of faster-than-light neutrinos… What was the reaction of the scientific community? The dominant reaction was not to claim Einstein’s theory to be falsified but was instead: “How did this team mess up the measurement?” (McElreath, 2016). The team that made the measurement itself called for independent replications of their observations, out of surprise. Two years later, the community was unanimous that this result was measurement error and the original team indeed realised that the technical error involved a poorly attached cable that messed up the measurement.\nThis problem has been echoed recently in psychology, where it can be argued that the problem of measurement is significantly harder than in the natural sciences:\n\n“In psychology, measurement is especially difficult because what we want to measure often does not permit direct observation. We can directly observe the height of a person next to us on the bus, but we often have little insight into latent, psychological attributes such as intelligence, extraversion, or depression” (Fried & Flake, 2018).\n\nOverall, the key dilemma (with measurement) is to know whether the falsification is genuine or spurious. Given that measurement is difficult, any scientific conclusion that relies on some form of measurement is probabilistic, rather than either “valid” or “invalid”, “true” or “false”.\n\n\n\nAnother problem arises from a misapplication of deductive syllogistic reasoning (a misapplication of the modus tollens). The problem (the “permanent illusion”, as put by Gigerenzer, 1993) is that most scientific hypotheses are not really of the kind “all swans are white” but rather of the form:\n\n“Ninety percent of swans are white”.\n“If my hypothesis is correct, we should probably not observe a black swan”.\n\nGiven this hypothesis, what can we conclude if we observe a black swan? Not much. To understand why, let’s translate it first to a more common statement in psychological research (from Cohen, 1994):\n\nIf the null hypothesis is true, then these data are highly unlikely.\nThese data have occurred.\nTherefore, the null hypothesis is highly unlikely.\n\nBut because of the probabilistic premise (i.e., the “highly unlikely”) this conclusion is invalid. Why? Consider the following argument (still from Cohen, 1994, borrowed from Pollard & Richardson, 1987):\n\nIf a person is an American, he is probably not a member of Congress.\nThis person is a member of Congress.\nTherefore, he is probably not an American.\n\nThis conclusion is not sensible (the argument is invalid), because it fails to consider the alternative to the premise, which is that if this person were not an American, the probability of being a member of Congress would be 0.\nThis is formally exactly the same as:\n\nIf the null hypothesis is true, then these data are highly unlikely.\nThese data have occurred.\nTherefore, the null hypothesis is highly unlikely.\n\nWhich is as much invalid as the previous argument, because i) the premise (the hypothesis) is probabilistic/continuous rather than discrete/logical and ii) because it fails to consider the probability of the alternative.\nThus, even without measurement/observation error, this problem would prevent us from applying the modus tollens to our hypothesis, thus preventing any possibility of strict falsification.\n\n\n\nAgain another problem is known as the Duhem–Quine thesis/problem (aka the underdetermination problem). In practice, when an substantive theory \\(T\\) happens to be tested, some hidden assumptions are also put under examination. These involve auxiliary theories that help to connect the substantive theory with the “real world”, in order to make testable predictions (e.g., “both white and black swans go walk around a similar proportion of time, so that we are equally likely to observe them in the nature”). It also usually involves some auxiliary theories about the instruments we use (e.g., “the BDI is a valid instrument for measuring depressive symptoms”), and the empirical realisation of specific conditions describing the experimental particulars (Meehl, 1978; 1990; 1997).\nWhen we test a theory predicting that “if \\(O_{1}\\)” (some manipulation or predictor variable), “then \\(O_{2}\\)” (some observation, dependent variable), what we actually mean is that we should observe this relation, if and only if all of the above (i.e., the auxiliary theories, the instrument theories, the particulars, etc.) are true.\nThus, the logical structure of an empirical test of a theory \\(T\\) can be described as the following conceptual formula (Meehl, 1978; 1990; 1997):\n\\[(T \\land A_{t} \\land C_{p} \\land A_{i} \\land C_{n}) \\to (O_{1} \\supset O_{2})\\]\nwhere the “\\(\\land\\)” are conjunctions (“and”), the arrow “\\(\\to\\)” denotes deduction (“follows that …”), and the horseshoe “\\(\\supset\\)” is the material conditional (“If \\(O_{1}\\), Then \\(O_{2}\\)”). \\(A_{t}\\) is a conjunction of auxiliary theories, \\(Cp\\) is a ceteribus paribus clause (i.e., we assume there is no other factor exerting an appreciable influence that could obfuscate the main effect of interest), \\(A_{i}\\) is an auxiliary theory regarding instruments, and \\(C_{n}\\) is a statement about experimentally realised conditions (i.e., we assume that there is no systematic error/noise in the experimental settings).\nIn other words, we imply that a conjunction of all the elements on the left-side (including our substantive theory \\(T\\)) does imply the right side of the arrow, that is, “if \\(O1\\), then \\(O2\\)”. The falsificationist attitude of the modern psychologist would lead her to think that not observing this relation would falsify the substantive theory of interest, based on the valid fourth figure of the implicative syllogism (the modus tollens).\nHowever, although the modus tollens is a valid figure of the implicative syllogism for logical statements (e.g., “all swans are black”), the neatness of Popper’s classic falsifiability concept is fuzzed up by the acknowledgement of the actual form of an empirical test. Obtaining falsificative evidence during an empirical test does not only falsify the substantive theory \\(T\\), but it does falsify all the left-side of the above statement. In other words, what we have achieved by our laboratory or correlational “falsification” is a falsification of the combined claims \\(T \\land A_{t} \\land C_{p} \\land A_{i} \\land C_{n}\\), which is probably not what we had in mind when we did the experiment (Meehl, 1990)3.\nTo sum up, failing to observe a predicted outcome does not necessarily mean that the theory itself is wrong, but rather that the conjunction of the theory and the underlying assumptions at hand are invalid (Lakatos, 1978; Meehl, 1978, 1990).\nOne approach that tries to elucidate these relationships is known as perspectivism (McGuire, 1983), for which an introduction can be found in Świątkowski & Dompnier (2017):\n\nHere [in McGuire approach], the “hidden” auxiliary assumptions that condition the extent to which a theory is generalizable are not considered as a problem, but rather as a means in the “discovery process to make clear the meaning of the [theory], disclosing its hidden assumptions and thus clarifying circumstances under which the [theory] is true and those under which it is false” (McGuire, 1983, p. 7).\n\nAs a consequence of the above considerations (the last four sections), falsification in science is almost never logical, but is always consensual (McElreath, 2016). A theoretical claim is considered to be falsified only when multiple lines of converging evidence have been obtained, by independent teams of researchers, and usually after several years or decades of critical discussion. The “falsification of a theory” then appears as a social result, issued from the community of scientists, and (almost) never as a deductive falsification."
  },
  {
    "objectID": "blog/2018-07-12-corroboration1/index.html#we-can-not-confirm-them",
    "href": "blog/2018-07-12-corroboration1/index.html#we-can-not-confirm-them",
    "title": "The Meehlian Corroboration-Verisimilitude Theory of Science - Part I",
    "section": "",
    "text": "The first “problem” with theories is that we can not confirm them. According to Campbell (1990), the logical argument of science has the following form:\n\nIf Newton’s theory is true, then it should be observed that the tides have period B, the path of Mars shape C, the trajectory of a cannonball form D, etc.\nObservation confirms B, C, and D.\nTherefore Newton’s theory A is “true”.\n\nHowever, this argument is a fallacious argument known as the affirmation of the consequent. One way to realise it is to visualise it as a diagram (figure from Campbell, 1990):\n\n\n\n\n\nThe invalidity of this argument comes from the existence of the cross-hatched area, which contains other possible explanations (than Newton’s theory) for the observed data. However, the syllogism is not useless: we could have observed data that are inconsistent with B, C, and D, in turn impugning the “truth” of Newton’s theory. Also, if observations are confirmed, the theory remains one of the possible explanations for their existence (see our later discussion of the concept of corroboration). As put by Campbell (1990), there is “an important asymmetry between logically valid rejection and logically inconclusive confirmation”.\nThis, together with the Duhem–Quine problem (that we will discuss in length later on), makes any experimental corroboration (or falsification, as we will see) of theoretical predictions explicable by challenging the background assumptions, or the adequacy of the experimental apparatus, or asserting the presence of one of Meehl’s “crud factors”, etc. (Campbell, 1990)."
  },
  {
    "objectID": "blog/2018-07-12-corroboration1/index.html#we-can-not-strictly-falsify-them",
    "href": "blog/2018-07-12-corroboration1/index.html#we-can-not-strictly-falsify-them",
    "title": "The Meehlian Corroboration-Verisimilitude Theory of Science - Part I",
    "section": "",
    "text": "We can not confirm theories, but maybe we can at least think of a way of disproving them? But what does it mean for a theory to be false? According to Popper’s view, a theory is falsifiable “if and only if there exists at least one potential falsifier—at least one possible basic statement that conflicts with it logically”. In other words, a theory can be considered as falsifiable if it can be shown to be false. Note that the falsifiability of early Popper concerns the problem of demarcation (i.e., what is science and what is pseudoscience), and defines pseudosciences as composed of non falsifiable theories (i.e., theories that does not allow the possibility of being disproved).\nBut when it comes to describe how science works (descriptive purposes) or to know how scientific enquiries should be lead (prescriptive purposes), science is usually not described by the falsification standard, as Popper himself recognized and argued. In fact, deductive falsification is impossible in nearly every scientific context (McElreath, 2016).\n\nIn the next sections, we discuss some of the reasons that prevent any theory to be strictly falsified (in a logical sense), namely: i) the distinction between theoretical and statistical models ii) the problem of measurement iii) the problem of continuous hypotheses, and iv) the Duhem-Quine problem.\n\n\nA statistical model is a device that connect theories to data. It can be defined as an instantiation of a theory as a set of probabilistic statements (Rouder, Morey, & Wagenmakers, 2016).\nTheoretical models and statistical models are usually not equivalent as many different theoretical models can correspond to the same probabilistic description, and reversely, different probabilistic descriptions can be derived from the same theoretical model. In other words, there is no one-to-one mapping between the two worlds, which render the induction from the statistical model to the theoretical model quite tricky.\nAs a consequence, observing consistent observations with some theory \\(T\\) does not “prove” in any way \\(T\\) (see previous section), as there could be many other theories that can predict these observations. Likewise, given that rejecting the straw-man null hypothesis can corroborate multiple different theories (as we could find many theories that predict something different from zero, to some extent), it acts as a very “weak corroborator” of any single theory.\n\n\n\nThe logic of falsification is pretty simple and rests on the power of the modus tollens2. This argument (whose exposition, for some reason, usually involves swans) can be presented as follows:\n\nIf my theory \\(T\\) is right then I should observe these data \\(D\\).\nI observe data that are not those I predicted \\(\\lnot D\\).\nTherefore, my theory is wrong \\(\\lnot T\\).\n\nThis argument is perfectly valid and works well for logical statements (statements that are either true or false). However, the first problem that arises when we try to apply this reasoning to the “real world” is the problem of observation error: observations are prone to error, especially at the boundaries of knowledge (McElreath, 2016).\nLet’s take an example from physics, related to the detection of faster than-light neutrinos (this example is discussed in the first chapter of McElreath, 2016). According to Einstein, neutrinos can not travel faster than the speed of light. Thus, any observation of faster-than-light neutrinos would act as a strong falsifier of Einstein’s special relativity.\nIn 2011 however, a large team of respected physicists announced the detection of faster-than-light neutrinos… What was the reaction of the scientific community? The dominant reaction was not to claim Einstein’s theory to be falsified but was instead: “How did this team mess up the measurement?” (McElreath, 2016). The team that made the measurement itself called for independent replications of their observations, out of surprise. Two years later, the community was unanimous that this result was measurement error and the original team indeed realised that the technical error involved a poorly attached cable that messed up the measurement.\nThis problem has been echoed recently in psychology, where it can be argued that the problem of measurement is significantly harder than in the natural sciences:\n\n“In psychology, measurement is especially difficult because what we want to measure often does not permit direct observation. We can directly observe the height of a person next to us on the bus, but we often have little insight into latent, psychological attributes such as intelligence, extraversion, or depression” (Fried & Flake, 2018).\n\nOverall, the key dilemma (with measurement) is to know whether the falsification is genuine or spurious. Given that measurement is difficult, any scientific conclusion that relies on some form of measurement is probabilistic, rather than either “valid” or “invalid”, “true” or “false”.\n\n\n\nAnother problem arises from a misapplication of deductive syllogistic reasoning (a misapplication of the modus tollens). The problem (the “permanent illusion”, as put by Gigerenzer, 1993) is that most scientific hypotheses are not really of the kind “all swans are white” but rather of the form:\n\n“Ninety percent of swans are white”.\n“If my hypothesis is correct, we should probably not observe a black swan”.\n\nGiven this hypothesis, what can we conclude if we observe a black swan? Not much. To understand why, let’s translate it first to a more common statement in psychological research (from Cohen, 1994):\n\nIf the null hypothesis is true, then these data are highly unlikely.\nThese data have occurred.\nTherefore, the null hypothesis is highly unlikely.\n\nBut because of the probabilistic premise (i.e., the “highly unlikely”) this conclusion is invalid. Why? Consider the following argument (still from Cohen, 1994, borrowed from Pollard & Richardson, 1987):\n\nIf a person is an American, he is probably not a member of Congress.\nThis person is a member of Congress.\nTherefore, he is probably not an American.\n\nThis conclusion is not sensible (the argument is invalid), because it fails to consider the alternative to the premise, which is that if this person were not an American, the probability of being a member of Congress would be 0.\nThis is formally exactly the same as:\n\nIf the null hypothesis is true, then these data are highly unlikely.\nThese data have occurred.\nTherefore, the null hypothesis is highly unlikely.\n\nWhich is as much invalid as the previous argument, because i) the premise (the hypothesis) is probabilistic/continuous rather than discrete/logical and ii) because it fails to consider the probability of the alternative.\nThus, even without measurement/observation error, this problem would prevent us from applying the modus tollens to our hypothesis, thus preventing any possibility of strict falsification.\n\n\n\nAgain another problem is known as the Duhem–Quine thesis/problem (aka the underdetermination problem). In practice, when an substantive theory \\(T\\) happens to be tested, some hidden assumptions are also put under examination. These involve auxiliary theories that help to connect the substantive theory with the “real world”, in order to make testable predictions (e.g., “both white and black swans go walk around a similar proportion of time, so that we are equally likely to observe them in the nature”). It also usually involves some auxiliary theories about the instruments we use (e.g., “the BDI is a valid instrument for measuring depressive symptoms”), and the empirical realisation of specific conditions describing the experimental particulars (Meehl, 1978; 1990; 1997).\nWhen we test a theory predicting that “if \\(O_{1}\\)” (some manipulation or predictor variable), “then \\(O_{2}\\)” (some observation, dependent variable), what we actually mean is that we should observe this relation, if and only if all of the above (i.e., the auxiliary theories, the instrument theories, the particulars, etc.) are true.\nThus, the logical structure of an empirical test of a theory \\(T\\) can be described as the following conceptual formula (Meehl, 1978; 1990; 1997):\n\\[(T \\land A_{t} \\land C_{p} \\land A_{i} \\land C_{n}) \\to (O_{1} \\supset O_{2})\\]\nwhere the “\\(\\land\\)” are conjunctions (“and”), the arrow “\\(\\to\\)” denotes deduction (“follows that …”), and the horseshoe “\\(\\supset\\)” is the material conditional (“If \\(O_{1}\\), Then \\(O_{2}\\)”). \\(A_{t}\\) is a conjunction of auxiliary theories, \\(Cp\\) is a ceteribus paribus clause (i.e., we assume there is no other factor exerting an appreciable influence that could obfuscate the main effect of interest), \\(A_{i}\\) is an auxiliary theory regarding instruments, and \\(C_{n}\\) is a statement about experimentally realised conditions (i.e., we assume that there is no systematic error/noise in the experimental settings).\nIn other words, we imply that a conjunction of all the elements on the left-side (including our substantive theory \\(T\\)) does imply the right side of the arrow, that is, “if \\(O1\\), then \\(O2\\)”. The falsificationist attitude of the modern psychologist would lead her to think that not observing this relation would falsify the substantive theory of interest, based on the valid fourth figure of the implicative syllogism (the modus tollens).\nHowever, although the modus tollens is a valid figure of the implicative syllogism for logical statements (e.g., “all swans are black”), the neatness of Popper’s classic falsifiability concept is fuzzed up by the acknowledgement of the actual form of an empirical test. Obtaining falsificative evidence during an empirical test does not only falsify the substantive theory \\(T\\), but it does falsify all the left-side of the above statement. In other words, what we have achieved by our laboratory or correlational “falsification” is a falsification of the combined claims \\(T \\land A_{t} \\land C_{p} \\land A_{i} \\land C_{n}\\), which is probably not what we had in mind when we did the experiment (Meehl, 1990)3.\nTo sum up, failing to observe a predicted outcome does not necessarily mean that the theory itself is wrong, but rather that the conjunction of the theory and the underlying assumptions at hand are invalid (Lakatos, 1978; Meehl, 1978, 1990).\nOne approach that tries to elucidate these relationships is known as perspectivism (McGuire, 1983), for which an introduction can be found in Świątkowski & Dompnier (2017):\n\nHere [in McGuire approach], the “hidden” auxiliary assumptions that condition the extent to which a theory is generalizable are not considered as a problem, but rather as a means in the “discovery process to make clear the meaning of the [theory], disclosing its hidden assumptions and thus clarifying circumstances under which the [theory] is true and those under which it is false” (McGuire, 1983, p. 7).\n\nAs a consequence of the above considerations (the last four sections), falsification in science is almost never logical, but is always consensual (McElreath, 2016). A theoretical claim is considered to be falsified only when multiple lines of converging evidence have been obtained, by independent teams of researchers, and usually after several years or decades of critical discussion. The “falsification of a theory” then appears as a social result, issued from the community of scientists, and (almost) never as a deductive falsification."
  },
  {
    "objectID": "blog/2018-07-12-corroboration1/index.html#is-nhst-falsificationist",
    "href": "blog/2018-07-12-corroboration1/index.html#is-nhst-falsificationist",
    "title": "The Meehlian Corroboration-Verisimilitude Theory of Science - Part I",
    "section": "Is NHST falsificationist?",
    "text": "Is NHST falsificationist?\nA rampant belief among psychologists is that the use of NHST is nicely aligned with Popper’s philosophy of science (and implicitly, that this is something desirable). However, this is misguided for at least two reasons: i) it ignores recent post-Popperian developments in philosophy of science and ii) the parallel between Popper’s falsificationism and nil null hypothesis significance testing is unsound, as we will see below.\nThe logic of NHST can be summarised as follows: we assume the hypothesis of no effect (the null hypothesis), we generate an infinite number of samples under this hypothesis, and compare the data we observed in our experiment to the counterfactual distribution of data under the hypothesis of no difference. If the observed data appears sufficiently implausible under the distribution of “null-data” (where “sufficiently” corresponds to your alpha level), we can safely reject the null hypothesis, and consider this rejection as a corroboration of the alternative hypothesis (whatever the alternative is).\nIn other words, the only hypothesis under test (in the classical use of NHST) is the null hypothesis, which is actually (almost) never of genuine interest in psychology. To really align NHST with falsificationism, we would need to test predictions of our substantive theory of interest \\(T\\), and not the predictions of a straw-man null hypothesis4.\nAs put by Meehl (1986):\n\n“[…] we have been brainwashed by Fisherian statistics into thinking that refutation of H0 is a powerful way of testing substantive theories”.\n\nAside from this problem, Fidler et al. (2018) provide four ways in which NHST violates Popperian falsificationism, namely because i) the statistical null model is almost certainly wrong to some degree (the straw-man null hypothesis), ii) substantive hypotheses (our hypotheses of interest) are not put under examination, iii) substantive hypotheses are not developed enough to drive statistical hypotheses and iv) even if all the previous points were solved, we would still need to submit these hypotheses to severe tests, which would require well-powered and well-designed studies, which is not the current norm5."
  },
  {
    "objectID": "blog/2018-07-12-corroboration1/index.html#weak-versus-strong-hypothesis-testing",
    "href": "blog/2018-07-12-corroboration1/index.html#weak-versus-strong-hypothesis-testing",
    "title": "The Meehlian Corroboration-Verisimilitude Theory of Science - Part I",
    "section": "Weak versus strong hypothesis testing",
    "text": "Weak versus strong hypothesis testing\nThe use of NHST does not follow from Popper’s falsificationism partly because it does not subject the theory to a grave risk of falsification, but only to a very low danger. In other words, NHST in its current use is never submitting the underlying theory to a strong test (Meehl, 1967; 1990; 1997; Lakens, 2018).\nBut NHST can be used in either a strong or a weak way (Meehl, 1967; 1990), depending on the statistical hypothesis \\(H\\) that is being tested in order to appraise a substantitve theory \\(T\\) (Meehl, 1997). The weak use of NHST corresponds to the situation in which we try to corroborate our theory \\(T\\) by rejecting the (highly implausible) null hypothesis. This use of NHST can be described as weak, mostly because of what is known as the crud factor (i.e., the fact that, in the social sciences, everything is correlated), then everything could explain a non-null difference. As a consequence, refuting the null does not really corroborate our favourite alternative hypothesis6.\nThe strong use would entail to possess a theory able to predict a numerical value of the parameters, or a narrow range of tolerated values, or a specific function form (e.g., quadratic, cubic) relating the variables (Meehl, 1997). In these situations, using a significance test to evaluate the difference between the theoretical prediction and the observed numeric value could act as a risky Popperian test (Meehl, 1997)7.\nOf course, in some rare situations, the null hypothesis is genuinely of interest and therefore, trying to falsify the null might be the appropriate move. For instance, it is the case of theories that predict consistent behaviours across different situations (e.g., see Morey, Homer, & Proulx, 2018).\nIn the next post, we will discuss in more depth the idea of strong test of a theory by relying on what has been coined the Meehlian Corroboration-Verisimilitude Theory of Science (Campbell, 1990)."
  },
  {
    "objectID": "blog/2018-07-12-corroboration1/index.html#footnotes",
    "href": "blog/2018-07-12-corroboration1/index.html#footnotes",
    "title": "The Meehlian Corroboration-Verisimilitude Theory of Science - Part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI make no pretense of the originality of my remarks in this post. This should be considered as the compiled notes of my recent reading of (some of) Meehl’s work and more recent conceptual papers on the topic of metatheory and theory testing.↩︎\nThe modus tollens is known formally as \\(p \\land q, \\lnot q, \\therefore \\lnot p\\). In plain English, it means that if \\(p\\) implies \\(q\\), observing not \\(q\\) lets us deduce not \\(p\\).↩︎\nPut formally, negating the left-hand conjunction is logically equivalent to stating a disjunction of the conjuncts (i.e., either one or the other of the components of the left-side is false; Meehl, 1990).↩︎\nSee how the p-value procedure can be generalised to other hypotheses than the null in the Bayesian framework: http://www.barelysignificant.com/post/ppc/.↩︎\nThe average statistical power of psychological research has been estimated to be under 50% for the average effect size seen in psychology research (e.g., Szucs & Ioannidis, 2017).↩︎\nSee also Meehl (1997, p. 408), for an example of strong and weak hypothesis testing related to the history of the use of the chi-square test.↩︎\nNB: P-values can also be used to falsify range predictions in equivalence testing (Rogers, Howard, & Vessey, 1993; Lakens, Scheel, Isager, 2018). Equivalently, one could use the ROPE procedure (Kruschke, 2015).↩︎"
  },
  {
    "objectID": "blog/2017-08-05-absenteeism1/index.html",
    "href": "blog/2017-08-05-absenteeism1/index.html",
    "title": "Experimental absenteeism and logistic regression - Part I",
    "section": "",
    "text": "One of the greatest aspect of being a PhD student in experimental psychology is to have the immeasurable pleasure to carry experiments with human subjects. Sure, it comes with a few disagreements, like having to teach undergraduate students how to use online schedule appointers (e.g., doodle), or spending a few hours per day answering clumsy e-mails asking for the location of the experimental room… and last but not least, being in a constant state of ignorance about the presence or the absence of the so-much expected student-guinea-pig.\nAfter many speculative and fruitless conversations about the rate of students’ participation to psychological studies, I have finally decided to make use of the data harvested along my PhD studies to answer the ultimate question: can we predict the participation of students?"
  },
  {
    "objectID": "blog/2017-08-05-absenteeism1/index.html#which-strategy",
    "href": "blog/2017-08-05-absenteeism1/index.html#which-strategy",
    "title": "Experimental absenteeism and logistic regression - Part I",
    "section": "Which strategy?",
    "text": "Which strategy?\nOn a practical ground, I have took advantage of the fact that a friend and I had to recruit participants for some studies of our PhD. We then simply started to systematically write down which participants did or did not come to their appointment, as well as information about their participation, like the mode of recruitment (online versus IRL registration), the fact that we sent a reminder e-mail (or not), and the day of the week. To sum up, the basic question was to know whether we can estimate the probability that a registered participant will come to its appointment, based on the information we collected."
  },
  {
    "objectID": "blog/2017-08-05-absenteeism1/index.html#what-is-a-logistic-regression",
    "href": "blog/2017-08-05-absenteeism1/index.html#what-is-a-logistic-regression",
    "title": "Experimental absenteeism and logistic regression - Part I",
    "section": "What is a logistic regression?",
    "text": "What is a logistic regression?\nLogistic regression (also called a logit model) is used to model binary outcome variables (e.g., “absent” versus “present”), using the general regression framework.\nIn the logit model, the log-odds of the outcome \\(p_{i}\\) are modelled as a linear combination of the predictor variables:\n\\[logit(p_{i}) = log\\Big(\\frac{p_{i}}{1-p_{i}}\\Big)= \\alpha + \\beta _{1} x_{1} + \\cdots + \\beta _{n} x_{n}\\]\nThus, although the observed dependent variable is a dichotomic variable, the logistic regression estimates the log-odds, as a continuous variable, that the dependent variable is in one state or the other.\nTo clarify the mechanism of the logit model, we will go through a first simple example, trying to estimate the overall probability of participants being present."
  },
  {
    "objectID": "blog/2017-08-05-absenteeism1/index.html#step-1-getting-the-data",
    "href": "blog/2017-08-05-absenteeism1/index.html#step-1-getting-the-data",
    "title": "Experimental absenteeism and logistic regression - Part I",
    "section": "Step 1: Getting the data",
    "text": "Step 1: Getting the data\nData can be retrieved directly from GitHub with:\n\n(data &lt;- read.csv(\"http://raw.githubusercontent.com/lnalborczyk/old_blog/master/_posts/absenteeism/absence.csv\", stringsAsFactors = FALSE) )\n\n\n\n         day inscription reminder absence presence total\n1     Friday      doodle       no       7       11    18\n2     Friday      doodle      yes       0        2     2\n3     Friday       panel      yes       0       10    10\n4     Monday      doodle       no       5        4     9\n5     Monday      doodle      yes       2        6     8\n6     Monday       panel      yes       6       12    18\n7   Thursday      doodle       no       3       11    14\n8    Tuesday      doodle       no       4       10    14\n9    Tuesday      doodle      yes       1        7     8\n10   Tuesday       panel      yes       0        9     9\n11 Wednesday      doodle       no       6       11    17\n12 Wednesday      doodle      yes       0        4     4\n13 Wednesday       panel      yes       0       14    14\n\n\nThe inscription column describes the mode of inscription of students (i.e., whether they registered for the experiment online, or IRL) while the reminder column indicates whether a reminder e-mail was sent to the participant prior to the experiment. The three last columns represent counts of present, absent, and total number of participants, respectively, for a grand total of 145 participants."
  },
  {
    "objectID": "blog/2017-08-05-absenteeism1/index.html#step-2-writing-down-a-first-model",
    "href": "blog/2017-08-05-absenteeism1/index.html#step-2-writing-down-a-first-model",
    "title": "Experimental absenteeism and logistic regression - Part I",
    "section": "Step 2: Writing down a first model",
    "text": "Step 2: Writing down a first model\nThe first step of every Bayesian analysis (oops, did I mention that this post will be Bayesian?) is to specify the full probability model. It starts with describing the data-generating process from which the data is issued (i.e., the likelihood). In our case, observations \\(y_{i}\\) are distributed according to a Binomial distribution:\n\\[y_{i} \\sim \\mathrm{Binomial}(n_{i}, p_{i})\\]\nwhere \\(y_{i}\\) is a count, \\(p_{i}\\) is the probability of any particular “trial” and \\(n_{i}\\) is the total number of observations. Our aim will be to predict the probability \\(p_{i}\\) of each trial \\(i\\).\nWith this first model, we want to estimate the mean probability of tha participant being present. We can model this situation by an intercept-only model, as following:\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i})\\\\\nlogit(p_{i}) &= \\alpha\\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10)\\\\\n\\end{aligned}\n\\]\nIn this model, we state that the log-odds (or the logit) of \\(p_{i}\\) are given by a single parameter \\(\\alpha\\), the intercept, on which we assign a very vague prior (note that this prior is expressed in the log-odds space, and not directly in the space of the outcome)."
  },
  {
    "objectID": "blog/2017-08-05-absenteeism1/index.html#step-3-fitting-the-model-with-rethinking",
    "href": "blog/2017-08-05-absenteeism1/index.html#step-3-fitting-the-model-with-rethinking",
    "title": "Experimental absenteeism and logistic regression - Part I",
    "section": "Step 3: Fitting the model with rethinking",
    "text": "Step 3: Fitting the model with rethinking\nThroughout this post, we will use the rethinking package, as it allows to fit a model using almost the same syntax as the mathematical one. For instance, the model we described above would look like:\n\nlibrary(rethinking)\nlibrary(tidyverse)\nlibrary(BEST)\n\nmod1 &lt;-\n    rethinking::map(\n        alist(\n            presence &lt;- dbinom(total, p),\n            logit(p) &lt;- a,\n            a ~ dnorm(0, 10) ),\n        data = data)        \n\nThe two first lines state that we fit the model using the map function, which requires a model expressed as an alist object (i.e., a non-evaluated list object). The three next lines describe the model very similarly to the mathematical description.\nA brief summary of the model can be obtained using the precis function.\n\nprecis(mod1, prob = 0.95)\n\n      mean        sd      2.5%    97.5%\na 1.182719 0.1959511 0.7986619 1.566776\n\n\nwhich gives the mode of the posterior distribution for the intercept \\(\\alpha\\) along with the standard deviation of the distribution as well as the credible intervals. The intercept represents the estimated log-odds of being present, which can be translated back to the overall probability of being present, as we know that:\n\\[p = \\exp(\\alpha) / (1 + \\exp(\\alpha) )\\]\n\nexp(coef(mod1)) / (1 + exp(coef(mod1) ) )\n\n        a \n0.7654363 \n\n\nwhich is equivalent to using the plogis function:\n\nplogis(coef(mod1) )\n\n        a \n0.7654363 \n\n\nSo we already know that there is an estimated 0.77 probability that a registered participant will come to his appointment…and if the intercept represents the log-odds of being present, a simple exponential transformation allows to get back to odds:\n\nexp(coef(mod1) ) # odds of presence\n\n       a \n3.263235 \n\n\nwhich can be interpreted directly by saying that the odds of a registered participant being present are of 3.26 (or 3.26:1)1."
  },
  {
    "objectID": "blog/2017-08-05-absenteeism1/index.html#step-4-adding-predictors",
    "href": "blog/2017-08-05-absenteeism1/index.html#step-4-adding-predictors",
    "title": "Experimental absenteeism and logistic regression - Part I",
    "section": "Step 4: Adding predictors",
    "text": "Step 4: Adding predictors\nNice, but we said that the logistic regression allows to map a relationship between a binary outcome variable and a linear combination of predictor variables, so let’s add predictors.\n\nEffects of the reminder\nFor instance, we might be interested in knowing whether sending a reminder e-mail has an impact on the presence. We first start by dummy-coding our predictors.\n\ndata &lt;-\n    data %&gt;%\n    mutate(\n        reminder = ifelse(reminder == \"no\", 0, 1),\n        inscription = ifelse(inscription == \"panel\", 0, 1) )\n\nThe model that map the outcome and the effects of the reminder can be expressed as follows:\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha + \\beta \\times \\text{reminder} \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\end{aligned}\n\\]\nwhere the effects of the reminder on the log-odds of being present are realised through the slope \\(\\beta\\), which is also assigned a very vague prior. This model is fitted with rethinking using a very similar syntax to previously:\n\nmod2 &lt;-\n    rethinking::map(\n        alist(\n            presence &lt;- dbinom(total, p),\n            logit(p) &lt;- a + b * reminder,\n            a ~ dnorm(0, 10),\n            b ~ dnorm(0, 10) ),\n        data = data)\n\nFirst, we might want to compare the efficiency of these two models. The compare function allows to compare models fit on the same data using the Wattanabe Akaike Information Criterion (WAIC; Watanabe, 2010), a generalisation of the AIC, previously discussed on this blog here. This criterion can be seen as an approximation of the out-of-sample deviance, and in simple words, of the predictive abilities of the model (McElreath, 2016).\n\ncompare(mod1, mod2) # yeah, model 2 wins\n\n         WAIC       SE    dWAIC      dSE    pWAIC     weight\nmod2 48.33272 8.926933 0.000000       NA 3.534778 0.97616377\nmod1 55.75756 7.308822 7.424847 9.667886 1.942421 0.02383623\n\n\nThis comparison reveals that the second model is better in the sense of the WAIC (as for the deviance, the lower is the better), and that the parameter we added (i.e., the slope \\(\\beta\\)) improved the predictive abilities of the model.\nBy adding a slope in the model, the intercept now represents the log-odds for the no-reminder condition (as it was coded as 0), while the coefficient for reminder represents the log odds-ratio between the reminder and the no-reminder groups. We can then obtain the odds-ratio by simply exponentiating the slope:\n\nexp(coef(mod2)[2]) # odds ratio between no-reminder and reminder\n\n       b \n3.774536 \n\n\nwhich can be read as a propotionnal increase of 3.77 in the odds of being present when a reminder is sent. We also might be interested in the absolute change in probability, rather than the odds ratio. To answer this question, we first extract the samples from the posterior distribution estimated by this second model and store it in post1.\n\npost1 &lt;- extract.samples(mod2) # extracting posterior samples\n\nThen, from these posterior samples, we can compute almost all statistics of interest. For instance we can compute the probability of being present according to the reminder status.\n\np.no &lt;- plogis(post1$a) # mean probability of presence when no reminder\np.yes &lt;- plogis(post1$a + post1$b) # mean probability of presence when reminder\nplotPost(p.yes - p.no, compVal = 0, showMode = TRUE, xlab = \"\")      \n\n\n\n\n\n\n\n\nThis histogram depicts the posterior distribution of the difference of probability of being present by comparing the no-reminder and the reminder groups. In other words, it represents an estimation of the effect of the reminder, while the mode of the distribution informs us about the most probable value of increase in probability due to the reminder.\n\n\nEffects of the mode of inscription\nLikewise, we could be interested in the maybe more subtle effects of the mode of inscription on the probability of presence. This model is expressed very similarly as the previous one:\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha + \\beta \\times \\text{inscription} \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\end{aligned}\n\\]\nwhere \\(\\beta\\) now represents the effects of the mode of inscription (i.e., online versus IRL).\n\nmod3 &lt;-\n    rethinking::map(\n        alist(\n            presence &lt;- dbinom(total, p),\n            logit(p) &lt;- a + b * inscription,\n            a ~ dnorm(0, 10),\n            b ~ dnorm(0, 10) ),\n        data = data)\n\nand we compare it to the intercept-only model in the same way as before:\n\ncompare(mod1, mod3) # yeah, model 3 wins\n\n         WAIC       SE    dWAIC      dSE    pWAIC    weight\nmod3 54.26925 9.913801 0.000000       NA 4.898551 0.6515147\nmod1 55.52065 7.267908 1.251408 10.70296 1.794424 0.3484853\n\n\nthis time again the model comparison tells us that the inclusion of this predictor in the model improves the predictions abilities of the model.\n\npost2 &lt;- extract.samples(mod2)\np.panel &lt;- plogis(post2$a) # mean probability of presence for panel\np.doodle &lt;- plogis(post2$a + post2$b) # mean probability of presence for doodle\nplotPost(p.doodle - p.panel, compVal = 0, showMode = TRUE, xlab = \"\")    \n\n\n\n\n\n\n\n\nWe obtain an estimation of the effect similar to the effect of the reminder. In other words, the mode of inscription seems to be as much important as the reminder, to predict the presence of participants…"
  },
  {
    "objectID": "blog/2017-08-05-absenteeism1/index.html#building-the-full-model-multi-what",
    "href": "blog/2017-08-05-absenteeism1/index.html#building-the-full-model-multi-what",
    "title": "Experimental absenteeism and logistic regression - Part I",
    "section": "Building the full model: multi-what?",
    "text": "Building the full model: multi-what?\nWe are now going to buil a last model that includes both reminder and incription as predictors.\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha + \\beta_{r} \\times \\text{reminder} + \\beta_{i} \\times \\text{inscription} \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta_{r} &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta_{i} &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\end{aligned}\n\\]\nAs previously, the rethinking model follows a similar syntax, with the only new thing here is the compact specification of the prior for the two slopes, using c().\n\nmod4 &lt;- rethinking::map(\n    alist(\n        presence &lt;- dbinom(total, p),\n        logit(p) &lt;- a + br * reminder + bi * inscription,\n        a ~ dnorm(0, 10),\n        c(br, bi) ~ dnorm(0, 10) ),\n    data = data)\n\nAdding that the output of precis can be directly plotted.\n\nplot(precis(mod4, prob = 0.95) )\n\n\n\n\n\n\n\n\nThis figure shows the mean of the posterior distribution of each parameter of the model along with 95% credible intervals. Wait…why inscription does not seem to have an influence anymore? Let’s compare the four models we built to better understand what’s happening here.\n\ncompare(mod1, mod2, mod3, mod4)\n\n         WAIC       SE    dWAIC      dSE    pWAIC     weight\nmod2 48.21541 8.781045 0.000000       NA 3.443579 0.71913505\nmod4 50.60255 9.641261 2.387147 1.314572 4.630774 0.21799578\nmod3 53.80681 9.754853 5.591401 3.282625 4.667485 0.04391907\nmod1 55.48789 7.258086 7.272480 9.580430 1.773878 0.01895010\n\n\nOk so model 4 performs worse than the others, and the best model seems to be the second model (effet of reminder only). Why inscription does not seem to have an influence anymore, in the full model? Oh yeah… I forgot to tell you that we were two experimenters running experiments and collecting data about absenteeism. I am used to recruit participants only through doodle and this time I was too busy lazy to send a reminder to each participant (and so sometimes I forgot), while my colleague recruited participants through an IRL panel and sent an e-mail reminder to each participant. In other words, it means that the reminder and the inscription variables are almost perfectly counfounded (i.e., “correlated”).\nA simple way to realise how problematic is the situation is to draw the contingency table.\n\ndata %&gt;% # contingency table\n    group_by(inscription, reminder) %&gt;%\n    summarise(n = sum(total) ) %&gt;%\n    spread(key = reminder, value = n) %&gt;% data.frame\n\n  inscription X0 X1\n1           0 NA 51\n2           1 72 22\n\n\nWe notice first that the amount of data in each condition is very disproportionnal and that we completely lack data for the condition panel and no reminder.\nWe generally refer to this situation (when two or more predictors are highly correlated) as multi-collinearity. It basically mean that the two predictor variables carry almost the same information. What the model comparison is telling us is then simply that there is no benefit in adding a second predictor if it brings the same information as the first predictor… makes sense no?\nThis redudancy can be illustrated by plotting the posterior samples of the two slopes each against the other:\n\nextract.samples(mod4) %&gt;%\n    ggplot(aes(br, bi) ) +\n    geom_point(alpha = 0.6, color = \"steelblue\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nAll we can say is that there is obviously an influence of at least one of the predictors on the probability of the participants being present, but as they are confounded, we can not tell whether it is an effect of the reminder or an effect of the mode of inscription. However, if I had to guess, I would go for the reminder effect. Otherwise, instead of being writing a blog post, I would be writing a manuscript on the beneficial motivationnal aspects of registering participants IRL compared to registering them via online platforms."
  },
  {
    "objectID": "blog/2017-08-05-absenteeism1/index.html#conclusions",
    "href": "blog/2017-08-05-absenteeism1/index.html#conclusions",
    "title": "Experimental absenteeism and logistic regression - Part I",
    "section": "Conclusions",
    "text": "Conclusions\nWell, know you now, sending a reminder e-mail probably increases the probability of participants being present, so don’t be busy lazy."
  },
  {
    "objectID": "blog/2017-08-05-absenteeism1/index.html#references",
    "href": "blog/2017-08-05-absenteeism1/index.html#references",
    "title": "Experimental absenteeism and logistic regression - Part I",
    "section": "References",
    "text": "References\n\n\nClick to expand\n\n\nMcElreath, R. (2016). Statistical Rethinking. Chapman; Hall/CRC.\nWatanabe, S. (2010). Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory. Journal of Machine Learning Research, 11, 3571–3594."
  },
  {
    "objectID": "blog/2017-08-05-absenteeism1/index.html#footnotes",
    "href": "blog/2017-08-05-absenteeism1/index.html#footnotes",
    "title": "Experimental absenteeism and logistic regression - Part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the UCLA page for more details on the interpretation of the odds and log-odds in the context of logistic regression.↩︎"
  },
  {
    "objectID": "blog/2018-04-30-bf/index.html",
    "href": "blog/2018-04-30-bf/index.html",
    "title": "What does a Bayes factor look like?",
    "section": "",
    "text": "Statisticians love coins. Why? These toy examples are extremely useful to illustrate difficult concepts while still being expressible in a convenient mathematical framework. As an example, let’s say we want to estimate the bias \\(\\theta\\) of a coin (i.e., its probability of landing heads up).\nWe compare two models of the bias \\(\\theta\\), these two models differing only by their prior on \\(\\theta\\). For instance, I could bet with a friend that the coin is probably biased toward tails (i.e., that the coin would, on average, lands more often tails up than heads up), while my friend rather think that the coin is biased toward heads. While we think that the coin is probably slightly biased, neither of us is completely certain of the exact value of this bias. Fortunately, probability theory gives us a way to express our prediction (as well as our uncertainty) in the form of a probability distribution.\nFor convenience, we can write our predictions as two beta-binomial models (check this previous blogpost to know why a beta prior is a convenient choice for the coin example):\n\\[\n\\begin{align}\n\\mathcal{M_{1}} : y_{i} &\\sim \\mathrm{Binomial}(n, \\theta) \\\\\n\\theta &\\sim \\mathrm{Beta}(6, 10) \\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\mathcal{M_{2}} : y_{i} &\\sim \\mathrm{Binomial}(n, \\theta) \\\\\n\\theta &\\sim \\mathrm{Beta}(20, 12) \\\\\n\\end{align}\n\\]\nWhere \\(\\mathcal{M_{1}}\\) represents my predictions about \\(\\theta\\) while \\(\\mathcal{M_{2}}\\) represents the predictions of my friend. As usual, these two priors are better understood visually:\n\n\n\n\n\n\n\n\n\nMy prior (in blue) is centered on \\(\\frac{\\alpha}{\\alpha + \\beta} = 0.375\\) while the prior of my friend (in red) is centered on \\(\\frac{\\alpha}{\\alpha + \\beta} = 0.625\\). Note that my friend seems a bit more certain of its prediction than me, as the red prior is a bit more narrow than the blue prior.\nNow we can collect some data to test our respective hypotheses. We launch the coin 100 times and gather the following data:\n\n\n  [1] 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0\n [38] 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0\n [75] 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n\n\nThen, we would like to know under which hypothesis (i.e., under which prior assumption) these data are the more probable to appear. To answer this question, we can compare the marginal likehoods of the two above models."
  },
  {
    "objectID": "blog/2018-04-30-bf/index.html#model-comparison",
    "href": "blog/2018-04-30-bf/index.html#model-comparison",
    "title": "What does a Bayes factor look like?",
    "section": "",
    "text": "Statisticians love coins. Why? These toy examples are extremely useful to illustrate difficult concepts while still being expressible in a convenient mathematical framework. As an example, let’s say we want to estimate the bias \\(\\theta\\) of a coin (i.e., its probability of landing heads up).\nWe compare two models of the bias \\(\\theta\\), these two models differing only by their prior on \\(\\theta\\). For instance, I could bet with a friend that the coin is probably biased toward tails (i.e., that the coin would, on average, lands more often tails up than heads up), while my friend rather think that the coin is biased toward heads. While we think that the coin is probably slightly biased, neither of us is completely certain of the exact value of this bias. Fortunately, probability theory gives us a way to express our prediction (as well as our uncertainty) in the form of a probability distribution.\nFor convenience, we can write our predictions as two beta-binomial models (check this previous blogpost to know why a beta prior is a convenient choice for the coin example):\n\\[\n\\begin{align}\n\\mathcal{M_{1}} : y_{i} &\\sim \\mathrm{Binomial}(n, \\theta) \\\\\n\\theta &\\sim \\mathrm{Beta}(6, 10) \\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\mathcal{M_{2}} : y_{i} &\\sim \\mathrm{Binomial}(n, \\theta) \\\\\n\\theta &\\sim \\mathrm{Beta}(20, 12) \\\\\n\\end{align}\n\\]\nWhere \\(\\mathcal{M_{1}}\\) represents my predictions about \\(\\theta\\) while \\(\\mathcal{M_{2}}\\) represents the predictions of my friend. As usual, these two priors are better understood visually:\n\n\n\n\n\n\n\n\n\nMy prior (in blue) is centered on \\(\\frac{\\alpha}{\\alpha + \\beta} = 0.375\\) while the prior of my friend (in red) is centered on \\(\\frac{\\alpha}{\\alpha + \\beta} = 0.625\\). Note that my friend seems a bit more certain of its prediction than me, as the red prior is a bit more narrow than the blue prior.\nNow we can collect some data to test our respective hypotheses. We launch the coin 100 times and gather the following data:\n\n\n  [1] 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0\n [38] 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0\n [75] 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n\n\nThen, we would like to know under which hypothesis (i.e., under which prior assumption) these data are the more probable to appear. To answer this question, we can compare the marginal likehoods of the two above models."
  },
  {
    "objectID": "blog/2018-04-30-bf/index.html#bayes-factors",
    "href": "blog/2018-04-30-bf/index.html#bayes-factors",
    "title": "What does a Bayes factor look like?",
    "section": "Bayes factors",
    "text": "Bayes factors\nFormally, the Bayes factor is defined as a ratio of marginal likelihoods and is given by:\n\\[\n\\text{BF}_{12} = \\dfrac{p(D|H_{1})}{p(D|H_{2})} = \\dfrac{\\int p(\\theta|H_{1}) p(D|\\theta,H_{1}) \\text{d}\\theta}{\\int p(\\theta|H_{2}) p(D|\\theta,H_{2}) \\text{d}\\theta} = \\dfrac{\\int \\mathrm{Binomial}(n, \\theta)\\mathrm{Beta}(6, 10)\\text{d}\\theta}{\\int \\mathrm{Binomial}(n, \\theta)\\mathrm{Beta}(20, 12) \\text{d}\\theta}\n\\]\nIn other words, computing a Bayes factor is equivalent to multiplying the prior by the likelihood (the information contained in the data) for each possible value of \\(\\theta\\), summing all of the obtained values1, doing this for each model, and taking the ratio of these sums. This process is illustrated below.\n\n\n\n\n\nWe can read this animation starting by the lower left panel and going clockwise until the lower right panel. In the first panel (the lower left panel), the black curve represents the likelihood function of \\(\\theta\\), that indicates how likely the data are to appear for a given value of \\(\\theta\\). The two coloured curves represent the prior of either I (in blue) or my friend (in red).\nThe moving dots represent the density of either the priors or the likelihood at each possible value of \\(\\theta\\)2. The unnormalised posterior density (displayed in the upper left panel) is constructed by multiplying the probability assigned by the prior (the “height” of the blue/red dot) to the probability assigned by the likelihood function (the “height” of the black dot) for each possible value of \\(\\theta\\).\nWe can then “unfold” this density by computing its cumulative function (in the upper right panel). This curve simply represents the value of the unnormalised posterior for each value of \\(\\theta\\), to which we added the value of the unnormalised posterior for all the previous values of \\(\\theta\\) (by summing them). For instance, the value of the cumulative unnormalised posterior at \\(\\theta = 0.14\\) is equal to the value of the unnormalised posterior at \\(\\theta = 0.14\\), plus the value of the unnormalised posterior at all the previous values of \\(\\theta\\) between \\(0\\) and \\(0.14\\).\nThe Bayes factor (see the lower right panel) is then computed as the ratio of the cumulative unnormalised posterior at the upper limit of the interval of integration.\nLet’s say it again. The Bayes factor is the ratio of the cumulative unnormalised posterior at the upper limit of the interval of integration. In our case, it corresponds to the ratio of the heights of the cumulative distributions when \\(\\theta = 1\\) (i.e., approximately 8 divided by 3). The resulting Bayes factor of 2.68 means that the data are 2.68 times more likely under my prior than under the prior of my friend."
  },
  {
    "objectID": "blog/2018-04-30-bf/index.html#they-are-not-posterior-odds",
    "href": "blog/2018-04-30-bf/index.html#they-are-not-posterior-odds",
    "title": "What does a Bayes factor look like?",
    "section": "They are not posterior odds",
    "text": "They are not posterior odds\nBe careful to not interpret Bayes factors as posterior odds. Writing Bayes’ rule in the “odds form” (see below) lets us realise that Bayes factors indicate how much we whould update our prior odds, in the light of new incoming data. They do not indicate what is the most probable hypothesis, given the data (unless the prior odds are 1:1).\nLet’s take another example and compare two hypotheses:\n\n\\(H_{0}\\): there is no such thing as precognition.\n\\(H_{1}\\): precognition does exist.\n\nWe run an experiment and observe a \\(BF_{10} = 27\\). What is the posterior probability of H1? We can compute this probability by multiplying the Bayes factor to the prior odds of the two hypotheses. How to define the prior odds is a tricky question and this choice should be open to scrutiny and criticism. Here I use an arbitrary relatively skeptical prior of 1:1000 against H1.\n\\[\n\\underbrace{\\dfrac{p(H_{1}|D)}{p(H_{0}|D)}}_{posterior\\ odds} = \\underbrace{\\dfrac{27}{1}}_{Bayes\\ factor} \\times \\underbrace{\\dfrac{1}{1000}}_{prior\\ odds} = \\dfrac{27}{1000} = 0.027\n\\]\nThis result demonstrates that a relatively high Bayes factor (representing some relatively strong evidence) should always be considered in the light of what was knew before the data comes in. A priori highly improbable claims can still remain highly improbable, even after having been corroborated by the data."
  },
  {
    "objectID": "blog/2018-04-30-bf/index.html#conclusions",
    "href": "blog/2018-04-30-bf/index.html#conclusions",
    "title": "What does a Bayes factor look like?",
    "section": "Conclusions",
    "text": "Conclusions\nBayes factors are increasingly used in psychology. Unfortunately, this does not warrant increased understanding of what they are. This blogpost aims to illustrate and summarise what they are and how they can be interpreted as updating factors.\n\nThis post was greatly inspired by a series of blogposts from Alexander Etz and Jeff Rouder (see references below). I also thank Fabian Dablander for providing valuable feedback on an earlier version of this post."
  },
  {
    "objectID": "blog/2018-04-30-bf/index.html#references",
    "href": "blog/2018-04-30-bf/index.html#references",
    "title": "What does a Bayes factor look like?",
    "section": "References",
    "text": "References\n\n\nClick to expand\n\n\nEtz, A. (2015, August 9). Understanding Bayes: Visualization of the Bayes Factor [Blog post]. Retrieved from https://alexanderetz.com/2015/08/09/understanding-bayes-visualization-of-bf/\nEtz, A. (2015, April 15). Understanding Bayes: A Look at the Likelihood [Blog post]. Retrieved from https://alexanderetz.com/2015/04/15/understanding-bayes-a-look-at-the-likelihood/\nRouder, J. (2016, January 24). Roll Your Own: How to Compute Bayes Factors For Your Priors [Blog post]. Retrieved from http://jeffrouder.blogspot.be/2016/01/what-priors-should-i-use-part-i.html"
  },
  {
    "objectID": "blog/2018-04-30-bf/index.html#footnotes",
    "href": "blog/2018-04-30-bf/index.html#footnotes",
    "title": "What does a Bayes factor look like?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBasically, an integral can be conceptualised as a sum on a continuous range (i.e., a sum on a infinite number of points).↩︎\nThese densities are scaled to the likelihood for aesthetics purposes.↩︎"
  },
  {
    "objectID": "blog/2017-09-22-absenteeism2/index.html",
    "href": "blog/2017-09-22-absenteeism2/index.html",
    "title": "Experimental absenteeism and logistic regression - Part II",
    "section": "",
    "text": "In a previous post, we tried to determine whether we could predict the presence of students that registered for psychological experiments, based on their mode of enrolment and the sending of a reminder by e-mail. As these two factors were confounded, we will focus in the current post on evaluating the effect of the reminder only, based on a bigger dataset gathered by several researchers.\nLet’s say that I have convinced ten of my colleagues to systematically send a reminder to one half of the participants of their next study, and no reminder to the other half.1 As my colleagues love high-powered studies, each of them aimed at recruiting approximately 200 participants per experiment, while this sample size could vary a little bit from one researcher to another (+/- 20%)."
  },
  {
    "objectID": "blog/2017-09-22-absenteeism2/index.html#step-1-getting-the-data",
    "href": "blog/2017-09-22-absenteeism2/index.html#step-1-getting-the-data",
    "title": "Experimental absenteeism and logistic regression - Part II",
    "section": "Step 1: Getting the data",
    "text": "Step 1: Getting the data\n\nlibrary(tidyverse)\n(data &lt;-\n    read.csv(\n        \"http://raw.githubusercontent.com/lnalborczyk/old_blog/master/_posts/absenteeism/absence2.csv\",\n        stringsAsFactors = FALSE) )\n\n\n\n   reminder researcher presence absence total\n1        -1          1       16      86   102\n2        -1          2       53      59   112\n3        -1          3       31      65    96\n4        -1          4       61      34    95\n5        -1          5       34      49    83\n6        -1          6       34      54    88\n7        -1          7        3      77    80\n8        -1          8       38      54    92\n9        -1          9       31      65    96\n10       -1         10       23      58    81\n11        1          1       96       6   102\n12        1          2      109       3   112\n13        1          3       87       9    96\n14        1          4       92       3    95\n15        1          5       65      18    83\n16        1          6       82       6    88\n17        1          7       64      16    80\n18        1          8       81      11    92\n19        1          9       89       7    96\n20        1         10       76       5    81\n\n\nThe reminder column indicates whether a reminder e-mail was sent to the participant prior to the experiment (coded as 1), while the researcher column indicates which researcher ran the study, from 1 to 10. The last three columns represent the counts of present, absent, and total number of participants, for a grand total of 1850 participants."
  },
  {
    "objectID": "blog/2017-09-22-absenteeism2/index.html#step-2-introducing-brms",
    "href": "blog/2017-09-22-absenteeism2/index.html#step-2-introducing-brms",
    "title": "Experimental absenteeism and logistic regression - Part II",
    "section": "Step 2: Introducing brms",
    "text": "Step 2: Introducing brms\nIn this first section, we will introduce the brms package (Bürkner, 2017), and fit a first simple model to try to predict the mean log-odds of a participant being present, through a linear combination of an intercept \\(\\alpha\\) and a slope \\(\\beta\\), the latter allowing to quantify the effect of the reminder.\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha + \\beta \\times \\text{reminder} \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\end{aligned}\n\\]\nOne great advantage of brms is that it allows to specify models using an lme4-like syntax, where the left side of the formula describes the outcome to be predicted and the right side describes the predictors (both constant and varying effects). When trying to predict aggregated binomial data, the outcome that is modeled is the number of successes (in our case, presence) out of the total number of trials, which is expressed in brms as sucesses|trials(total)2.\nAs previously, we can retrieve the grand mean probability of presence by transforming back the intercept, as we know that \\(p = \\exp(\\alpha) / (1 + \\exp(\\alpha) )\\).\n\na &lt;- fixef(mod1)[1] # extracting the intercept\nexp(a) / (1 + exp(a) ) # equivalent to plogis(a)\n\n[1] 0.6988809\n\n\nA summary of this model can be obtained using the posterior_summary() function, which provides the mean of the posterior distribution along with its standard error and credible intervals.\n\nposterior_summary(x = mod1, pars = \"^b_\")\n\n             Estimate  Est.Error      Q2.5     Q97.5\nb_Intercept 0.8419746 0.06770814 0.7099141 0.9777148\nb_reminder  1.4599305 0.06791294 1.3270085 1.5903859\n\n\nAlternatively, brms (in combination with bayesplot) offers a nice method to plot brmsfit objects. Below, we plot an histogram of samples from the posterior distribution for both the intercept \\(\\alpha\\) and the slope \\(\\beta\\), along with traceplots.\n\nmod1 %&gt;%\n    plot(\n        combo = c(\"hist\", \"trace\"), widths = c(1, 1.5),\n        theme = theme_bw(base_size = 12) )\n\n\n\n\n\n\n\n\nFrom this first model we can also compute (as previously) the odds ratio (OR), between the no-reminder and the reminder conditions, by simply exponentiating the slope. In our case, the OR is approximately equal to 4.32 (95% HDI [3.79, 4.94]), meaning that it is 4.32 times more likely that participants will be present if a reminder is sent prior to the experiment.\nThere is one major issue though with this analysis, related to the nested structure of the data. Each researcher evaluated the effects of the reminder in his study, and we could expect each study to have its own baseline level of presence probability, perhaps related to the specificities of the study, or to the field of research, the university, etc. In other words, we could expect participants of the same cluster (i.e., the same study, ran by the same researcher) to be more similar to each others, than participants of different clusters. In other words, observations are interdependant. In this situation, we can not run a standard logistic regression analysis because this violates one of the most important assumptions in the linear model, namely the assumption of independence of the residuals (Sommet & Morselli, 2017). Multilevel models (MLMs) allow to disentangle the effects intrinsic to a specific cluster and the between-clusters effects, by allowing parameters to vary by cluster."
  },
  {
    "objectID": "blog/2017-09-22-absenteeism2/index.html#step-3-varying-the-intercept",
    "href": "blog/2017-09-22-absenteeism2/index.html#step-3-varying-the-intercept",
    "title": "Experimental absenteeism and logistic regression - Part II",
    "section": "Step 3: Varying the intercept",
    "text": "Step 3: Varying the intercept\nWith the following model, we will estimate the mean probability of presence, only specifying an intercept (this model is sometimes called the unconditionnal mean model), that we will allow to vary by researcher.\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha_{researcher_{[i]}} \\\\\n\\alpha_{researcher} &\\sim \\mathrm{Normal}(\\alpha, \\sigma) \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\sigma &\\sim \\mathrm{HalfCauchy}(0, 10) \\\\\n\\end{aligned}\n\\]\nFrom this formulation we can see that the log-odds are now allowed to vary by cluster (i.e., by researcher), and that we are also estimating the parameters of the distribution from which these intercepts are issued (i.e., the population of intercepts, described in the third line of the model). This way, the model can learn information both at the level of the researcher, and at the level of the population of researchers, thus fighting the anterograde amnesia of the constant-effects models (see McElreath, 2016, page 355).\n\nposterior_summary(x = mod2, pars = c(\"^b_\", \"^sd_\") )\n\n                          Estimate Est.Error      Q2.5    Q97.5\nb_Intercept              0.5503923 0.2270845 0.1525630 1.003360\nsd_researcher__Intercept 0.5148700 0.1868776 0.2904222 1.030763\n\n\nWe can interpret the variation of the intercept \\(\\alpha\\) between researchers by considering the intra-class correlation (ICC)3, which goes from 0 if the grouping conveys no information to 1 if all levels of a cluster are identical (Gelman, 2006, p. 258). In other words, ICC = 0 indicates perfect independence of residuals: the observations do not depend on cluster membership. When the ICC is not different from zero or negligible, one could consider running traditional one-level regression analysis. However, ICC = 1 indicates perfect interdependence of residuals: the observations only vary between clusters (Sommet & Morselli, 2017).\nThe ICC is usually expressed as \\(\\frac{\\tau^{2}}{\\tau^{2} + \\sigma^{2}}\\), where \\(\\tau^{2}\\) denotes the variance of the distribution of the varying effects. However, in the context of logistic regression, we do not have residuals (i.e., the \\(\\sigma^{2}\\)) on the first level… A first approach to compute the ICC in multilevel logistic regression is known as the latent variable approach, as we assume that the true underlying variable is continuous but that we can only observe a binary response that indicates whether the underlying variable is greater or less than a given threshold. In the logistic regression model, the underlying continuous variable will come from a logistic distribution, with a variance of \\(\\frac{\\pi^2}{3}\\), and hence we substitute this for the level 1 variance, resulting in the formula: \\(\\frac{\\tau^{2}}{\\tau^{2} + \\frac{\\pi^2}{3}}\\) when using a logit link (Austin & Merlo, 2017; Browne, Subramanian, Jones, & Goldstein, 2005; Sommet & Morselli, 2017).\nNote though that this method of estimating the ICC can differ considerably from other methods (e.g., the simulation method, wait for the next post…) as we assume that the level 1 variance is fixed and independent of the predictor variables.\n\n# extracting tau^2\ntau2 &lt;- posterior_summary(x = mod2, pars = \"^sd_\")\ntau2 &lt;- tau2[1]^2\n\n# computing the ICC\n(ICC &lt;- tau2 / (tau2 + (pi^2 / 3) ) )\n\n[1] 0.07456938\n\n\nIn our case, the ICC is equal to 0.0746, indicating that 7.46% of the chances of being present is explained by between-study differences, and conversely, that 7.46% is explained by within-study differences.\nAnother way to visualise the variability of the varying intercepts is to plot them. In the following plot, we use the ggjoy package (Wilke, 2017) to represent the posterior distribution of the estimated mean probability of presence for each researcher along with raw data estimation (the black crosses), while the vertical dotted line represents the grand mean probability of presence.\n\n\n\n\n\n\n\n\n\nThis plot reveals the phenomenon of shrinkage, that is the phenomenon through which the model expresses its skepticism toward extreme values. The mean predicted probability of presence for each researcher (the varying intercept) will differ from the raw estimation in an amount that is dependant to the distance between this raw proportion and the grand mean probability (and the precision of the estimation for this particular cluster). The more the data seem weird, the more the model’s estimation will be shrunk to the grand mean."
  },
  {
    "objectID": "blog/2017-09-22-absenteeism2/index.html#step-4-varying-the-slope",
    "href": "blog/2017-09-22-absenteeism2/index.html#step-4-varying-the-slope",
    "title": "Experimental absenteeism and logistic regression - Part II",
    "section": "Step 4: Varying the slope",
    "text": "Step 4: Varying the slope\nIn the same manner that the mean probability of presence might be different from researcher to researcher, one might ask whether the effects of the reminder are identical between researchers. In the next model, we will then allow the slope to vary by researcher too.\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha_{researcher_{[i]}} + \\beta_{researcher_{[i]}} \\times \\text{reminder}_{i} \\\\\n\\begin{bmatrix}\n\\alpha_{\\text{researcher}} \\\\\n\\beta_{\\text{researcher}} \\\\\n\\end{bmatrix}\n&\\sim \\mathrm{MVNormal}\\bigg(\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\textbf{S}\\bigg) \\\\\n\\textbf{S} &=\n\\begin{pmatrix}\n\\sigma_{\\alpha} & 0 \\\\\n0 & \\sigma_{\\beta} \\\\\n\\end{pmatrix}\n\\textbf{R} \\begin{pmatrix}\n\\sigma_{\\alpha} & 0 \\\\\n0 & \\sigma_{\\beta} \\\\\n\\end{pmatrix} \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 10) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 10) \\\\\n(\\sigma_{\\alpha}, \\sigma_{\\beta}) &\\sim \\mathrm{HalfCauchy}(0, 10) \\\\\n\\textbf{R} &\\sim \\mathrm{LKJcorr}(2) \\\\\n\\end{aligned}\n\\]\nWe chose to model the varying intercept and the varying slope as issued from the same multivariate normal distribution (on the third line), allowing to estimate the correlation between them (for more details see McElreath, 2016). One reason to do this might be that a low mean probability of presence for a particular study, could be associated with a stronger / weaker effect of the reminder for this particular study. For instance, we could imagine that some researchers paid their participants, and some others did not, and that the latter kind of study would have a lower mean probability of presence, but would exhibit a stronger effect of the reminder (i.e., we would expect a negative correlation between the intercept and the slope).\nNote that one way to model both varying intercept and varying slope without modelling the correlation between these two terms is to specify a double pipe in the formula like (1+reminder||researcher).\n\nposterior_summary(x = mod3, pars = c(\"^b_\", \"cor\", \"^sd_\") )\n\n                                      Estimate Est.Error       Q2.5     Q97.5\nb_Intercept                          0.8566714 0.3203371  0.2405395 1.5435218\nb_reminder                           1.6206629 0.1897438  1.2606856 2.0156438\ncor_researcher__Intercept__reminder -0.1686038 0.3225240 -0.7531804 0.4862646\nsd_researcher__Intercept             0.8959518 0.2903186  0.4793149 1.6133910\nsd_researcher__reminder              0.4822635 0.1874389  0.2222145 0.9512427\n\n\nThis output indeed indicates a negative correlation between the varying intercept and the varying slope but relatively weak and associated with a lot of uncertainty (\\(\\rho=-0.19\\), 95% HDI [-0.75, 0.47]).\nOne way to compare the models we fitted would be to compare their predictive abilities. It can be done with the LOO function, which uses leave-one-out cross-validation (Gelfand, Dey, & Chang 1992; Vehtari, Gelman, & Gabry, 2015).\n\nbrms::LOO(mod1, mod2, mod3, compare = FALSE)\n\nOutput of model 'mod1':\n\nComputed from 2000 by 20 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -125.7 30.0\np_loo        14.1  4.6\nlooic       251.4 60.0\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.3, 1.0]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     19    95.0%   41      \n   (0.7, 1]   (bad)       1     5.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'mod2':\n\nComputed from 2000 by 20 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo   -593.2  51.9\np_loo       271.7  20.9\nlooic      1186.5 103.8\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.7, 1.0]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)      0      0.0%  &lt;NA&gt;    \n   (0.7, 1]   (bad)       0      0.0%  &lt;NA&gt;    \n   (1, Inf)   (very bad) 20    100.0%  &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'mod3':\n\nComputed from 2000 by 20 log-likelihood matrix.\n\n         Estimate  SE\nelpd_loo    -64.1 3.2\np_loo        15.3 2.2\nlooic       128.2 6.3\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)      4    20.0%   123     \n   (0.7, 1]   (bad)      14    70.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad)  2    10.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nThis comparison reveals that the third model has the best predictive abilities, indicating that it was savvy to include a varying slope. Below we plot the posterior predictions of this last model, at the group-level as well at the population-level (in base plot, because we love base plot).\n\n# extracting posterior samples from the last model\npost &lt;- posterior_samples(mod3, \"b\")\n\n# plotting the raw data\nplot(data$prob ~ data$reminder, las = 1,\n    pch = 1, cex = 0.75,\n    xlab = \"reminder\", ylab = \"p(presence)\")\n\n# adding a grid\nabline(h = seq(0, 1, 0.2), v = seq(-1, 1, 0.2),\n    col = adjustcolor(\"gray\", alpha.f = 0.2), lty = 1)\n\n# extracting coefficients\ncoefs &lt;- rbind(\n  data.frame(t(matrix(fixef(mod3)[, 1]) ) ) %&gt;%\n    rename(intercept = X1, slope = X2),\n  data.frame(coef(mod3) %&gt;% data.frame %&gt;% select(contains(\"Estimate\") ) ) %&gt;%\n    rename(intercept = 1, slope = 2)\n  )\n\n# plotting population-level predictions\nx_plot &lt;- seq(-1, 1, length.out = 2000)\ny_plot &lt;- plogis(coefs[1, 1] + coefs[1, 2] * x_plot)\n\nlines(x_plot, y_plot, lwd = 2, col = \"steelblue\")\n\n# plotting group-level predictions\nfor(i in 2:nrow(coefs) ){\n    \n    x_plot &lt;- seq(-1, 1, length.out = 2000)\n    y_plot &lt;- plogis(coefs[i, 1] + coefs[i, 2] * x_plot)\n    lines(x_plot, y_plot, lwd = 1,\n        col = adjustcolor(\"steelblue\", alpha.f = 0.25) )\n}\n\n\n\n\n\n\n\n\nThe uncertainty associated with the estimation of the intercept and the slope at the population-level can be appreciated by superimposing many samples from the posterior (see also this blogpost from Matti Vuorre for much nicer plots and how to plot credible intervals at the individual-level).\n\n# plotting the raw data\nplot(data$prob ~ data$reminder, las = 1,\n    pch = 1, cex = 0.75,\n    xlab = \"reminder\", ylab = \"p(presence)\")\n\n# adding a grid\nabline(h = seq(0, 1, 0.2), v = seq(-1, 1, 0.2),\n    col = adjustcolor(\"gray\", alpha.f = 0.2), lty = 1)\n\n# plotting group-level predictions (surimposing 1 out of 10 posterior samples)\nfor(i in seq(1, nrow(post), 10) ){\n    \n    x_plot &lt;- seq(-1, 1, length.out = 2000)\n    y_plot &lt;- plogis(post[i,1] + post[i,2] * x_plot)\n    \n    lines(x_plot, y_plot, lwd = 0.75,\n        col = adjustcolor(\"steelblue\", alpha.f = 0.05) )\n        \n}"
  },
  {
    "objectID": "blog/2017-09-22-absenteeism2/index.html#conclusions",
    "href": "blog/2017-09-22-absenteeism2/index.html#conclusions",
    "title": "Experimental absenteeism and logistic regression - Part II",
    "section": "Conclusions",
    "text": "Conclusions\nTo sum up, the effect of the reminder, while non negligeable, appears as quite variable accross different studies ran by different researchers. The baseline probability of presence by researcher appears as even more variable. Thus, predictions of our models could be improved by incorporating more information in the model, for instance by taking into account the similarity between different teams (e.g., teams of the same lab, or studies realised in the same field of research), by refining the structure of the model and adding additionnal levels."
  },
  {
    "objectID": "blog/2017-09-22-absenteeism2/index.html#references",
    "href": "blog/2017-09-22-absenteeism2/index.html#references",
    "title": "Experimental absenteeism and logistic regression - Part II",
    "section": "References",
    "text": "References\n\n\nClick to expand\n\n\nBrowne, W. J., Subramanian, S. V., Jones, K., Goldstein, H. (2005). Variance partitioning in multilevel logistic models that exhibit overdispersion. Journal of the Royal Statistical Society—Series A, 168(3):599–613.\nBürkner, P.-C. (2017). brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software, 80(1), 1-28. doi:10.18637/jss.v080.i01\nClaus O. Wilke (2017). ggjoy: Joyplots in ‘ggplot2’. R package version 0.2.0. https://CRAN.R-project.org/package=ggjoy\nEvans, M., Hastings, N., Peacock, B. (1993). Statistical Distributions. John Wiley and Sons: New York, NY.\nGelfand, A.E., Dey, D.K., Chang, H. (1992). Model Determination Using Predictive Distributions with Implementation via Sampling-Based Methods. Technical report, DTIC Document.\nMcElreath, R. (2016). Statistical Rethinking. Chapman; Hall/CRC.\nRobinson, D. (2017). broom: Convert Statistical Analysis Objects into Tidy Data Frames. R package version 0.4.2. https://CRAN.R-project.org/package=broom\nSommet, N., & Morselli, D. (2017). Keep Calm and Learn Multilevel Logistic Modeling: A Simplified Three-Step Procedure Using Stata, R, Mplus, and SPSS. International Review of Social Psychology, 30(1), 203–218, DOI: https://doi.org/10.5334/irsp.90\nVehtari, A., Gelman, A., Gabry, J. (2015). Efficient Implementation of Leave-One-Out Cross- Validation and WAIC for Evaluating Fitted Bayesian Models. Unpublished manuscript, pp. 1–22. URL http://www.stat.columbia.edu/~gelman/research/unpublished/loo_ stan.pdf."
  },
  {
    "objectID": "blog/2017-09-22-absenteeism2/index.html#footnotes",
    "href": "blog/2017-09-22-absenteeism2/index.html#footnotes",
    "title": "Experimental absenteeism and logistic regression - Part II",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nObviously I do not possess such impressive persuasion skills, thus we will work with simulated data.↩︎\nThere are several ways to specify priors in brms, that can be retrieved by typing ?prior in the console.↩︎\nThe ICC is also known as the Variance Partitioning Coefficient.↩︎"
  },
  {
    "objectID": "blog/2025-01-07-commonality/index.html",
    "href": "blog/2025-01-07-commonality/index.html",
    "title": "Making sense of commonality analysis",
    "section": "",
    "text": "What proportion of variance in intelligence (e.g., IQ) can be uniquely attributed to genetic factors, independent of socio-economic influences? How do neuroimaging data from different modalities (e.g., fMRI and M/EEG) relate to one another and to theoretical models?\nThese questions highlight the critical role of shared variance in understanding complex systems. Commonality analysis provides a valuable tool for addressing such questions by partitioning the explained variance (\\(R^{2}\\)) in multiple regression into distinct components. It identifies how much variance is uniquely attributable to each predictor and how much arises from shared contributions among predictors. This approach helps to clarify multivariate relationships and assess the relative importance of each independent variable (Seibold & McPhee, 1979).\nIn this post, I will illustrate this method with practical examples in R and visual aids.\nDisclaimer: This post reflects my personal effort to gain a better understanding of commonality analysis and should be interpreted with caution. I do not claim to have any special expertise on this topic."
  },
  {
    "objectID": "blog/2025-01-07-commonality/index.html#footnotes",
    "href": "blog/2025-01-07-commonality/index.html#footnotes",
    "title": "Making sense of commonality analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor this reason, the absolute value of the semi-partial correlation of \\(X\\) with \\(Y\\) is always less than or equal to that of the partial correlation of \\(X\\) with \\(Y\\).↩︎"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blogposts",
    "section": "",
    "text": "Making sense of commonality analysis\n\n\n\n\n\n\nStatistics\n\n\nRSA\n\n\nCognitive neuroscience\n\n\n\nUsing commonality analysis to identify unique and shared (common) variance across three or more variables.\n\n\n\n\n\n2025-01-07\n\n\nLadislas Nalborczyk\n\n\n31 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Meehlian Corroboration-Verisimilitude Theory of Science - Part II\n\n\n\n\n\n\nPhilosophy of science\n\n\nPaul Meehl\n\n\n\nThe second part of my compiled reading notes on Meehl’s metatheory and related meta-peregrinations.\n\n\n\n\n\n2018-07-20\n\n\nLadislas Nalborczyk\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Meehlian Corroboration-Verisimilitude Theory of Science - Part I\n\n\n\n\n\n\nPhilosophy of science\n\n\nPaul Meehl\n\n\n\nMy compiled reading notes on Meehl’s metatheory and related meta-peregrinations.\n\n\n\n\n\n2018-07-12\n\n\nLadislas Nalborczyk\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does a Bayes factor look like?\n\n\n\n\n\n\nR\n\n\nData visualisation\n\n\nBayes factor\n\n\nBayesian\n\n\n\nAn attempt to illustrate what a Bayes factor looks like, using GIFs.\n\n\n\n\n\n2018-05-22\n\n\nLadislas Nalborczyk\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nChecking the asumption of independence in binomial trials using posterior predictive checking\n\n\n\n\n\n\nTutorial\n\n\nR\n\n\nBayesian\n\n\nPPC\n\n\n\nAs put by Gelman et al. (2013, page 148): ‘because a probability model can fail to reflect the process that generated the data in any number of ways, posterior predictive p-values can be computed for a variety of test quantities in order to evaluate more than one possible model failure’.\n\n\n\n\n\n2018-01-23\n\n\nLadislas Nalborczyk\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nUsing R to make sense of the generalised linear model\n\n\n\n\n\n\nTutorial\n\n\nR\n\n\nGLM\n\n\n\nWhat is the difference between the errors and the residuals? What does it mean for a model to predict something? What is a link function? In the current post, we use four R functions (the predict, fitted, residuals and simulate functions) to illustrate the mechanisms and assumptions of the generalised linear model.\n\n\n\n\n\n2018-01-15\n\n\nLadislas Nalborczyk\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nThree methods for computing the intra-class correlation in multilevel logistic regression\n\n\n\n\n\n\nR\n\n\nBayesian\n\n\nbrms\n\n\nIntra-class correlation\n\n\nMultilevel regression\n\n\n\nIn the current post, we present and compare three methods of obtaning an estimation of the ICC in multilevel logistic regression models.\n\n\n\n\n\n2017-10-08\n\n\nLadislas Nalborczyk\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe saga of the summer 2017, a.k.a. ‘the alpha wars’\n\n\n\n\n\n\nNHST\n\n\nalpha wars\n\n\n\nA short summary of the “alpha wars” that occurred during the summer of year 2017.\n\n\n\n\n\n2017-09-30\n\n\nLadislas Nalborczyk\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nExperimental absenteeism and logistic regression - Part II\n\n\n\n\n\n\nR\n\n\nBayesian\n\n\nbrms\n\n\nLogistic regression\n\n\nMultilevel model\n\n\n\nThis post continues our exploration of the logistic regression model by extending it to a multilevel logistic regression model, using the brms package.\n\n\n\n\n\n2017-09-22\n\n\nLadislas Nalborczyk\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nExperimental absenteeism and logistic regression - Part I\n\n\n\n\n\n\nR\n\n\nBayesian\n\n\nLogistic regression\n\n\n\nThis post aims to assess the average probability of participant presence in psychological experiments and, in the meantime, to introduce Bayesian logistic regression using R and the rethinking package.\n\n\n\n\n\n2017-08-05\n\n\nLadislas Nalborczyk\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhy the Akaike information criterion is as much ‘Bayesian’ as the Bayesian information criterion\n\n\n\n\n\n\nBayesian\n\n\nModel comparison\n\n\n\nAccording to Rubin (1984), a Bayesianly justifiable analysis is one that “treats known values as observed values of random variables, treats unknown values as unobserved random variables, and calculates the conditional distribution of unknowns given knowns and model specifications using Bayes’ theorem”.\n\n\n\n\n\n2016-11-04\n\n\nLadislas Nalborczyk\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ladislas Nalborczyk",
    "section": "",
    "text": "Mail\n  \n  \n    \n     GitHub\n  \n  \n      Google Scholar\n  \n  \n      Bluesky\n  \n\n  \n  \n\n\n\n\nI am a computational cognitive neuroscientist interested in the conscious experience of inner speech and its neural underpinnings. My research combines experimental (e.g., psychophysics, EMG, M/EEG, TMS) and computational (e.g., mathematical modelling, machine learning) methods to understand how complex patterns of neural activity (in both biological and artificial neural networks) give rise to algorithms supporting the mental simulation of speech.\n\n\n\nIn parallel, I also work on the development and dissemination of rigorous experimental and statistical methods. Besides, I feel very concerned about the issue of making our research more open, reproducible, and sustainable.\n\n\n\n\nIf you are a scientist who wants to collaborate with me, a PhD student or a postdoctoral researcher interested in my research, please reach out to me! Please also check out the open positions in my team, which includes a list of thesis topics for Master students."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Ladislas Nalborczyk",
    "section": "",
    "text": "I am a computational cognitive neuroscientist interested in the conscious experience of inner speech and its neural underpinnings. My research combines experimental (e.g., psychophysics, EMG, M/EEG, TMS) and computational (e.g., mathematical modelling, machine learning) methods to understand how complex patterns of neural activity (in both biological and artificial neural networks) give rise to algorithms supporting the mental simulation of speech.\n\n\n\nIn parallel, I also work on the development and dissemination of rigorous experimental and statistical methods. Besides, I feel very concerned about the issue of making our research more open, reproducible, and sustainable."
  },
  {
    "objectID": "index.html#working-together",
    "href": "index.html#working-together",
    "title": "Ladislas Nalborczyk",
    "section": "",
    "text": "If you are a scientist who wants to collaborate with me, a PhD student or a postdoctoral researcher interested in my research, please reach out to me! Please also check out the open positions in my team, which includes a list of thesis topics for Master students."
  },
  {
    "objectID": "publications/2018-loevenbruck-etal/index.html",
    "href": "publications/2018-loevenbruck-etal/index.html",
    "title": "A cognitive neuroscience view of inner language: To predict and to hear, see, feel",
    "section": "",
    "text": "Inner verbalisation can be willful, when we deliberately engage in inner speech (e.g., mental rehearsing, counting, list making) or more involuntary, when unbidden verbal thoughts occur. It can either be expanded (fully phonologically specified) or condensed (cast in a prelinguistic format). Introspection and empirical data suggest that willful expanded inner speech recruits the motor system and involves auditory, proprioceptive, tactile as well as perhaps visual sensations. We present a neurocognitive predictive control model, in which willful inner speech is considered as deriving from multisensory goals arising from sensory cortices. An inverse model transforms desired sensory states into motor commands which are specified in motor regions and inhibited by prefrontal cortex. An efference copy of these motor commands is transformed by a forward model into simulated multimodal acts (inner phonation, articulation, gesture). These simulated acts provide predicted multisensory percepts that are processed in sensory regions and perceived as an inner voice unfolding over time. The comparison between desired sensory states and predicted sensory end states provides the sense of agency, of feeling in control of one’s inner speech. Three types of inner verbalisation can be accounted for in this framework: unbidden thoughts, willful expanded inner speech, and auditory verbal hallucination."
  },
  {
    "objectID": "publications/2018-loevenbruck-etal/index.html#abstract",
    "href": "publications/2018-loevenbruck-etal/index.html#abstract",
    "title": "A cognitive neuroscience view of inner language: To predict and to hear, see, feel",
    "section": "",
    "text": "Inner verbalisation can be willful, when we deliberately engage in inner speech (e.g., mental rehearsing, counting, list making) or more involuntary, when unbidden verbal thoughts occur. It can either be expanded (fully phonologically specified) or condensed (cast in a prelinguistic format). Introspection and empirical data suggest that willful expanded inner speech recruits the motor system and involves auditory, proprioceptive, tactile as well as perhaps visual sensations. We present a neurocognitive predictive control model, in which willful inner speech is considered as deriving from multisensory goals arising from sensory cortices. An inverse model transforms desired sensory states into motor commands which are specified in motor regions and inhibited by prefrontal cortex. An efference copy of these motor commands is transformed by a forward model into simulated multimodal acts (inner phonation, articulation, gesture). These simulated acts provide predicted multisensory percepts that are processed in sensory regions and perceived as an inner voice unfolding over time. The comparison between desired sensory states and predicted sensory end states provides the sense of agency, of feeling in control of one’s inner speech. Three types of inner verbalisation can be accounted for in this framework: unbidden thoughts, willful expanded inner speech, and auditory verbal hallucination."
  },
  {
    "objectID": "publications/2024-nalborczyk-et-al/index.html",
    "href": "publications/2024-nalborczyk-et-al/index.html",
    "title": "Towards formal models of inhibitory mechanisms involved in motor imagery: a commentary on Bach et al. (2022)",
    "section": "",
    "text": "A vast body of research suggests that the primary motor cortex is involved in motor imagery. This raises the issue of inhibition: how is it possible for motor imagery not to lead to motor execution? Bach et al. (2022) suggest that the motor execution threshold may be ‘upregulated’ during motor imagery to prevent execution. Alternatively, it has been proposed that, in parallel to excitatory mechanisms, inhibitory mechanisms may be actively suppressing motor output during motor imagery. These theories are verbal in nature, with well-known limitations. Here, we describe a toy-model of the inhibitory mechanisms thought to be at play during motor imagery to start disentangling predictions from competing hypotheses."
  },
  {
    "objectID": "publications/2024-nalborczyk-et-al/index.html#abstract",
    "href": "publications/2024-nalborczyk-et-al/index.html#abstract",
    "title": "Towards formal models of inhibitory mechanisms involved in motor imagery: a commentary on Bach et al. (2022)",
    "section": "",
    "text": "A vast body of research suggests that the primary motor cortex is involved in motor imagery. This raises the issue of inhibition: how is it possible for motor imagery not to lead to motor execution? Bach et al. (2022) suggest that the motor execution threshold may be ‘upregulated’ during motor imagery to prevent execution. Alternatively, it has been proposed that, in parallel to excitatory mechanisms, inhibitory mechanisms may be actively suppressing motor output during motor imagery. These theories are verbal in nature, with well-known limitations. Here, we describe a toy-model of the inhibitory mechanisms thought to be at play during motor imagery to start disentangling predictions from competing hypotheses."
  },
  {
    "objectID": "publications/2022-nalborczyk-etal/index.html",
    "href": "publications/2022-nalborczyk-etal/index.html",
    "title": "The role of motor inhibition during covert speech production",
    "section": "",
    "text": "Covert speech is accompanied by a subjective multisensory experience with auditory and kinaesthetic components. An influential hypothesis states that these sensory percepts result from a simulation of the corresponding motor action that relies on the same internal models recruited for the control of overt speech. This simulationist view raises the question of how it is possible to imagine speech without executing it. In this perspective, we discuss the possible role(s) played by motor inhibition during covert speech production. We suggest that considering covert speech as an inhibited form of overt speech maps naturally to the purported progressive internalisation of overt speech during childhood. We further argue that the role of motor inhibition may differ widely across different forms of covert speech (e.g., condensed vs. expanded covert speech) and that considering this variety helps reconciling seemingly contradictory findings from the neuroimaging literature."
  },
  {
    "objectID": "publications/2022-nalborczyk-etal/index.html#abstract",
    "href": "publications/2022-nalborczyk-etal/index.html#abstract",
    "title": "The role of motor inhibition during covert speech production",
    "section": "",
    "text": "Covert speech is accompanied by a subjective multisensory experience with auditory and kinaesthetic components. An influential hypothesis states that these sensory percepts result from a simulation of the corresponding motor action that relies on the same internal models recruited for the control of overt speech. This simulationist view raises the question of how it is possible to imagine speech without executing it. In this perspective, we discuss the possible role(s) played by motor inhibition during covert speech production. We suggest that considering covert speech as an inhibited form of overt speech maps naturally to the purported progressive internalisation of overt speech during childhood. We further argue that the role of motor inhibition may differ widely across different forms of covert speech (e.g., condensed vs. expanded covert speech) and that considering this variety helps reconciling seemingly contradictory findings from the neuroimaging literature."
  },
  {
    "objectID": "publications/2017-nalborczyk-etal/index.html",
    "href": "publications/2017-nalborczyk-etal/index.html",
    "title": "Orofacial electromyographic correlates of induced verbal rumination",
    "section": "",
    "text": "Rumination is predominantly experienced in the form of repetitive verbal thoughts. Verbal rumination is a particular case of inner speech. According to the Motor Simulation view, inner speech is a kind of motor action, recruiting the speech motor system. In this framework, we predicted an increase in speech muscle activity during rumination as compared to rest. We also predicted increased forehead activity, associated with anxiety during rumination. We measured electromyographic activity over the orbicularis oris superior and inferior, frontalis and flexor carpi radialis muscles. Results showed increased lip and forehead activity after rumination induction compared to an initial relaxed state, together with increased self-reported levels of rumination. Moreover, our data suggest that orofacial relaxation is more effective in reducing rumination than non-orofacial relaxation. Altogether, these results support the hypothesis that verbal rumination involves the speech motor system, and provide a promising psychophysiological index to assess the presence of verbal rumination."
  },
  {
    "objectID": "publications/2017-nalborczyk-etal/index.html#abstract",
    "href": "publications/2017-nalborczyk-etal/index.html#abstract",
    "title": "Orofacial electromyographic correlates of induced verbal rumination",
    "section": "",
    "text": "Rumination is predominantly experienced in the form of repetitive verbal thoughts. Verbal rumination is a particular case of inner speech. According to the Motor Simulation view, inner speech is a kind of motor action, recruiting the speech motor system. In this framework, we predicted an increase in speech muscle activity during rumination as compared to rest. We also predicted increased forehead activity, associated with anxiety during rumination. We measured electromyographic activity over the orbicularis oris superior and inferior, frontalis and flexor carpi radialis muscles. Results showed increased lip and forehead activity after rumination induction compared to an initial relaxed state, together with increased self-reported levels of rumination. Moreover, our data suggest that orofacial relaxation is more effective in reducing rumination than non-orofacial relaxation. Altogether, these results support the hypothesis that verbal rumination involves the speech motor system, and provide a promising psychophysiological index to assess the presence of verbal rumination."
  },
  {
    "objectID": "publications/2023-nalborczyk-etal/index.html",
    "href": "publications/2023-nalborczyk-etal/index.html",
    "title": "Distinct neural mechanisms support inner speaking and inner hearing",
    "section": "",
    "text": "Humans have the ability to mentally examine speech. This covert form of speech production is often accompanied by sensory (e.g., auditory) percepts. However, the cognitive and neural mechanisms that generate these percepts are still debated. According to a prominent proposal, inner speech has at least two distinct phenomenological components: inner speaking and inner hearing. We used transcranial magnetic stimulation to test whether these two phenomenologically distinct processes are supported by distinct neural mechanisms. We hypothesised that inner speaking relies more strongly on an online motor-to-sensory simulation that constructs a multisensory experience, whereas inner hearing relies more strongly on a memory-retrieval process, where the multisensory experience is reconstructed from stored motor-to-sensory associations. Accordingly, we predicted that the speech motor system will be involved more strongly during inner speaking than inner hearing. This would be revealed by modulations of TMS evoked responses at muscle level following stimulation of the lip primary motor cortex. Overall, data collected from 31 participants corroborated this prediction, showing that inner speaking increases the excitability of the primary motor cortex more than inner hearing. Moreover, this effect was more pronounced during the inner production of a syllable that strongly recruits the lips (vs. a syllable that recruits the lips to a lesser extent). These results are compatible with models assuming a central role of the primary motor cortex for inner speech production and contribute to clarify the neural implementation of the fundamental ability of silently speaking in one’s mind."
  },
  {
    "objectID": "publications/2023-nalborczyk-etal/index.html#abstract",
    "href": "publications/2023-nalborczyk-etal/index.html#abstract",
    "title": "Distinct neural mechanisms support inner speaking and inner hearing",
    "section": "",
    "text": "Humans have the ability to mentally examine speech. This covert form of speech production is often accompanied by sensory (e.g., auditory) percepts. However, the cognitive and neural mechanisms that generate these percepts are still debated. According to a prominent proposal, inner speech has at least two distinct phenomenological components: inner speaking and inner hearing. We used transcranial magnetic stimulation to test whether these two phenomenologically distinct processes are supported by distinct neural mechanisms. We hypothesised that inner speaking relies more strongly on an online motor-to-sensory simulation that constructs a multisensory experience, whereas inner hearing relies more strongly on a memory-retrieval process, where the multisensory experience is reconstructed from stored motor-to-sensory associations. Accordingly, we predicted that the speech motor system will be involved more strongly during inner speaking than inner hearing. This would be revealed by modulations of TMS evoked responses at muscle level following stimulation of the lip primary motor cortex. Overall, data collected from 31 participants corroborated this prediction, showing that inner speaking increases the excitability of the primary motor cortex more than inner hearing. Moreover, this effect was more pronounced during the inner production of a syllable that strongly recruits the lips (vs. a syllable that recruits the lips to a lesser extent). These results are compatible with models assuming a central role of the primary motor cortex for inner speech production and contribute to clarify the neural implementation of the fundamental ability of silently speaking in one’s mind."
  },
  {
    "objectID": "publications/2019-nalborczyk-etal/index.html",
    "href": "publications/2019-nalborczyk-etal/index.html",
    "title": "An introduction to Bayesian multilevel models using brms",
    "section": "",
    "text": "Bayesian multilevel models are increasingly used to overcome the limitations of frequentist approaches in the analysis of complex structured data. This paper introduces Bayesian multilevel modelling for the specific analysis of speech data, using the brms package developed in R. In this tutorial, we provide a practical introduction to Bayesian multilevel modelling, by reanalysing a phonetic dataset containing formant (F1 and F2) values for five vowels of Standard Indonesian (ISO 639-3:ind), as spoken by eight speakers (four females), with several repetitions of each vowel. We first give an introductory overview of the Bayesian framework and multilevel modelling. We then show how Bayesian multilevel models can be fitted using the probabilistic programming language Stan and the R package brms, which provides an intuitive formula syntax. Through this tutorial, we demonstrate some of the advantages of the Bayesian framework for statistical modelling and provide a detailed case study, with complete source code for full reproducibility of the analyses."
  },
  {
    "objectID": "publications/2019-nalborczyk-etal/index.html#abstract",
    "href": "publications/2019-nalborczyk-etal/index.html#abstract",
    "title": "An introduction to Bayesian multilevel models using brms",
    "section": "",
    "text": "Bayesian multilevel models are increasingly used to overcome the limitations of frequentist approaches in the analysis of complex structured data. This paper introduces Bayesian multilevel modelling for the specific analysis of speech data, using the brms package developed in R. In this tutorial, we provide a practical introduction to Bayesian multilevel modelling, by reanalysing a phonetic dataset containing formant (F1 and F2) values for five vowels of Standard Indonesian (ISO 639-3:ind), as spoken by eight speakers (four females), with several repetitions of each vowel. We first give an introductory overview of the Bayesian framework and multilevel modelling. We then show how Bayesian multilevel models can be fitted using the probabilistic programming language Stan and the R package brms, which provides an intuitive formula syntax. Through this tutorial, we demonstrate some of the advantages of the Bayesian framework for statistical modelling and provide a detailed case study, with complete source code for full reproducibility of the analyses."
  },
  {
    "objectID": "publications/2021-beffara-etal/index.html",
    "href": "publications/2021-beffara-etal/index.html",
    "title": "A fully automated, transparent, reproducible, and blind protocol for sequential analyses",
    "section": "",
    "text": "Despite many cultural, methodological, and technical improvements, one of the major obstacle to results reproducibility remains the pervasive low statistical power. In response to this problem, a lot of attention has recently been drawn to sequential analyses. This type of procedure has been shown to be more efficient (to require less observations and therefore less resources) than classical fixed-N procedures. However, these procedures are submitted to both intrapersonal and interpersonal biases during data collection and data analysis. In this tutorial, we explain how automation can be used to prevent these biases. We show how to synchronise open and free experiment software programs with the Open Science Framework and how to automate sequential data analyses in R. This tutorial is intended to researchers with beginner experience with R but no previous experience with sequential analyses is required."
  },
  {
    "objectID": "publications/2021-beffara-etal/index.html#abstract",
    "href": "publications/2021-beffara-etal/index.html#abstract",
    "title": "A fully automated, transparent, reproducible, and blind protocol for sequential analyses",
    "section": "",
    "text": "Despite many cultural, methodological, and technical improvements, one of the major obstacle to results reproducibility remains the pervasive low statistical power. In response to this problem, a lot of attention has recently been drawn to sequential analyses. This type of procedure has been shown to be more efficient (to require less observations and therefore less resources) than classical fixed-N procedures. However, these procedures are submitted to both intrapersonal and interpersonal biases during data collection and data analysis. In this tutorial, we explain how automation can be used to prevent these biases. We show how to synchronise open and free experiment software programs with the Open Science Framework and how to automate sequential data analyses in R. This tutorial is intended to researchers with beginner experience with R but no previous experience with sequential analyses is required."
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Selected publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nMotor inhibition prevents motor execution during typing imagery: Evidence from an action-mode switching paradigm\n\n\n\n\n\n\nMotor imagery\n\n\nMotor inhibition\n\n\nCognitive modelling\n\n\n\nExamining sequential effects in an action-mode switching paradigm (typing execution and typing imagery).\n\n\n\n\n\n2024-11-04\n\n\nLadislas Nalborczyk, F.-X. Alario, Marieke Longcamp\n\n\n\n\n\n\n\n\n\n\n\n\nTowards formal models of inhibitory mechanisms involved in motor imagery: a commentary on Bach et al. (2022)\n\n\n\n\n\n\nMotor imagery\n\n\nMotor inhibition\n\n\nCognitive modelling\n\n\n\nModelling the inhibitory mechanisms underlying motor imagery.\n\n\n\n\n\n2024-01-12\n\n\nLadislas Nalborczyk, Marieke Longcamp, Thibault Gajdos, Mathieu Servant, F.-X. Alario\n\n\n\n\n\n\n\n\n\n\n\n\nDistinct neural mechanisms support inner speaking and inner hearing\n\n\n\n\n\n\nInner speech\n\n\nMotor imagery\n\n\nSurface electromyography\n\n\nTranscranial magnetic stimulation\n\n\n\nExperimental paper in which we used transcranial magnetic stimulation to test the involvement of the primary motor cortex during inner speaking and inner hearing.\n\n\n\n\n\n2023-10-10\n\n\nLadislas Nalborczyk, Marieke Longcamp, Mireille Bonnard, Victor Serveau, Laure Spieser, F.-X. Alario\n\n\n\n\n\n\n\n\n\n\n\n\nTime perception: When randomisation hurts\n\n\n\n\n\n\nTime perception\n\n\nSequential effects\n\n\nRandomisation\n\n\n\nShort journal club about the unexpected consequences of randomisation in psychological research.\n\n\n\n\n\n2023-01-24\n\n\nLadislas Nalborczyk\n\n\n\n\n\n\n\n\n\n\n\n\nThe role of motor inhibition during covert speech production\n\n\n\n\n\n\nInner speech\n\n\nMotor imagery\n\n\nMotor simulation\n\n\nMotor inhibition\n\n\n\nPerspective paper on the role of motor inhibition during covert speech production.\n\n\n\n\n\n2022-03-09\n\n\nLadislas Nalborczyk, Ursula Debarnot, Marieke Longcamp, Aymeric Guillot, F.-X. Alario\n\n\n\n\n\n\n\n\n\n\n\n\nA fully automated, transparent, reproducible, and blind protocol for sequential analyses\n\n\n\n\n\n\nSequential analyses\n\n\nStatistical modelling\n\n\nBlind analyses\n\n\nAutomated analyses\n\n\nExperimenter bias\n\n\n\nTutorial paper on automated sequential analyses.\n\n\n\n\n\n2021-03-11\n\n\nBrice Beffara, Amélie Bret, Ladislas Nalborczyk\n\n\n\n\n\n\n\n\n\n\n\n\nCan we decode phonetic features in inner speech using surface electromyography?\n\n\n\n\n\n\nInner speech\n\n\nMotor imagery\n\n\nElectromyography\n\n\nMachine learning\n\n\n\nExperimental paper in which we tried to decode the phonetic content of inner speech from surface electromyography.\n\n\n\n\n\n2020-05-27\n\n\nLadislas Nalborczyk, Romain Grandchamp, Ernst Koster, Marcela Perrone-Bertolotti, Hélène Lœvenbruck\n\n\n\n\n\n\n\n\n\n\n\n\nAction effects on visual perception of distances: A multilevel Bayesian meta-analysis\n\n\n\n\n\n\nBayesian\n\n\nMeta-analysis\n\n\nMotor simulation\n\n\n\nExamining sequential effects in an action-mode switching paradigm (typing execution and typing imagery).\n\n\n\n\n\n2020-04-09\n\n\nLisa Molto, Ladislas Nalborczyk, Richard Palluel-Germain, Nicolas Morgado\n\n\n\n\n\n\n\n\n\n\n\n\nAn introduction to Bayesian multilevel models using brms\n\n\n\n\n\n\nBayesian\n\n\nbrms\n\n\nMultilevel models\n\n\nStatistical modelling\n\n\n\nTutorial paper on Bayesian multilevel modelling in R using the brms package.\n\n\n\n\n\n2019-05-21\n\n\nLadislas Nalborczyk, Cédric Batailler, Hélène Lœvenbruck, Anne Vilain, Paul Bürkner\n\n\n\n\n\n\n\n\n\n\n\n\nA cognitive neuroscience view of inner language: To predict and to hear, see, feel\n\n\n\n\n\n\nInner speech\n\n\nMotor control\n\n\nPredictive control\n\n\nMotor simulation\n\n\n\nBook chapter reviewing core findinds in the cognitive neuroscience study of inner speech and proposing a novel neurocognitive model of (voluntary) inner speech production.\n\n\n\n\n\n2018-10-18\n\n\nHélène Lœvenbruck, Romain Grandchamp, Lucile Rapin, Ladislas Nalborczyk, Marion Dohen, Pascal Perrier, Monica Baciu, Marcela Perrone-Bertolotti\n\n\n\n\n\n\n\n\n\n\n\n\nOrofacial electromyographic correlates of induced verbal rumination\n\n\n\n\n\n\nInner speech\n\n\nRumination\n\n\nElectromyography\n\n\nMotor simulation\n\n\n\nExperimental paper in which we used surface electromyography as a physiological marker of induced mental rumination.\n\n\n\n\n\n2017-07-20\n\n\nLadislas Nalborczyk, Marcela Perrone-Bertolotti, Céline Baeyens, Romain Grandchamp, Mircea Polosan, Elsa Spinelli, Ernst Koster, Hélène Lœvenbruck\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/2020-molto-etal/index.html",
    "href": "publications/2020-molto-etal/index.html",
    "title": "Action effects on visual perception of distances: A multilevel Bayesian meta-analysis",
    "section": "",
    "text": "Previous studies have suggested that action constraints influence visual perception of distances. For instance, the greater the effort to cover a distance, the longer people perceive this distance to be. The present multilevel Bayesian meta-analysis (37 studies with 1,035 total participants) supported the existence of a small action-constraint effect on distance estimation, Hedges’s g = 0.29, 95% credible interval = [0.16, 0.47]. This effect varied slightly according to the action-constraint category (effort, weight, tool use) but not according to participants’ motor intention. Some authors have argued that such effects reflect experimental demand biases rather than genuine perceptual effects. Our meta-analysis did not allow us to dismiss this possibility, but it also did not support it. We provide field-specific conventions for interpreting action-constraint effect sizes and the minimum sample sizes required to detect them with various levels of power. We encourage researchers to help us update this meta-analysis by directly uploading their published or unpublished data to our online repository (https://osf.io/bc3wn/)."
  },
  {
    "objectID": "publications/2020-molto-etal/index.html#abstract",
    "href": "publications/2020-molto-etal/index.html#abstract",
    "title": "Action effects on visual perception of distances: A multilevel Bayesian meta-analysis",
    "section": "",
    "text": "Previous studies have suggested that action constraints influence visual perception of distances. For instance, the greater the effort to cover a distance, the longer people perceive this distance to be. The present multilevel Bayesian meta-analysis (37 studies with 1,035 total participants) supported the existence of a small action-constraint effect on distance estimation, Hedges’s g = 0.29, 95% credible interval = [0.16, 0.47]. This effect varied slightly according to the action-constraint category (effort, weight, tool use) but not according to participants’ motor intention. Some authors have argued that such effects reflect experimental demand biases rather than genuine perceptual effects. Our meta-analysis did not allow us to dismiss this possibility, but it also did not support it. We provide field-specific conventions for interpreting action-constraint effect sizes and the minimum sample sizes required to detect them with various levels of power. We encourage researchers to help us update this meta-analysis by directly uploading their published or unpublished data to our online repository (https://osf.io/bc3wn/)."
  },
  {
    "objectID": "publications/2020-nalborczyk-etal/index.html",
    "href": "publications/2020-nalborczyk-etal/index.html",
    "title": "Can we decode phonetic features in inner speech using surface electromyography?",
    "section": "",
    "text": "Although having a long history of scrutiny in experimental psychology, it is still controversial whether wilful inner speech (covert speech) production is accompanied by specific activity in speech muscles. We present the results of a preregistered experiment looking at the electromyographic correlates of both overt speech and inner speech production of two phonetic classes of nonwords. An automatic classification approach was undertaken to discriminate between two articulatory features contained in nonwords uttered in both overt and covert speech. Although this approach led to reasonable accuracy rates during overt speech production, it failed to discriminate inner speech phonetic content based on surface electromyography signals. However, exploratory analyses conducted at the individual level revealed that it seemed possible to distinguish between rounded and spread nonwords covertly produced, in two participants. We discuss these results in relation to the existing literature and suggest alternative ways of testing the engagement of the speech motor system during wilful inner speech production."
  },
  {
    "objectID": "publications/2020-nalborczyk-etal/index.html#abstract",
    "href": "publications/2020-nalborczyk-etal/index.html#abstract",
    "title": "Can we decode phonetic features in inner speech using surface electromyography?",
    "section": "",
    "text": "Although having a long history of scrutiny in experimental psychology, it is still controversial whether wilful inner speech (covert speech) production is accompanied by specific activity in speech muscles. We present the results of a preregistered experiment looking at the electromyographic correlates of both overt speech and inner speech production of two phonetic classes of nonwords. An automatic classification approach was undertaken to discriminate between two articulatory features contained in nonwords uttered in both overt and covert speech. Although this approach led to reasonable accuracy rates during overt speech production, it failed to discriminate inner speech phonetic content based on surface electromyography signals. However, exploratory analyses conducted at the individual level revealed that it seemed possible to distinguish between rounded and spread nonwords covertly produced, in two participants. We discuss these results in relation to the existing literature and suggest alternative ways of testing the engagement of the speech motor system during wilful inner speech production."
  },
  {
    "objectID": "publications/2025-nalborczyk-et-al/index.html",
    "href": "publications/2025-nalborczyk-et-al/index.html",
    "title": "Motor inhibition prevents motor execution during typing imagery: Evidence from an action-mode switching paradigm",
    "section": "",
    "text": "Motor imagery is accompanied by a subjective multisensory experience. This sensory experience is thought to result from the deployment of internal models developed for the execution and monitoring of overt actions. If so, how is it that motor imagery does not lead to overt execution? It has been proposed that inhibitory mechanisms may prevent execution during imagined actions such as imagined typing. To test this hypothesis, we combined an experimental with a modelling approach. We conducted an experiment in which participants (N = 49) were asked to alternate between overt (executed) and covert (imagined) typing. We predicted that motor inhibition should lead to longer reaction and movement times when the current trial is preceded by an imagined vs. an executed trial. This prediction was borne out by movement times, but not by reaction times. We introduced and fitted an algorithmic model of motor imagery to disentangle potentially distinct inhibitory mechanisms underlying these effects. Results from this analysis suggest that motor inhibition may affect different aspects of the latent activation function (e.g., the shape of the activation function or the motor execution threshold) with distinct consequences on reaction times and movement times. Overall, these results suggest that typing imagery involves the inhibition of motor commands related to typing acts. Preregistration, complete source code, and reproducible analyses are available at https://osf.io/y9a3k/."
  },
  {
    "objectID": "publications/2025-nalborczyk-et-al/index.html#abstract",
    "href": "publications/2025-nalborczyk-et-al/index.html#abstract",
    "title": "Motor inhibition prevents motor execution during typing imagery: Evidence from an action-mode switching paradigm",
    "section": "",
    "text": "Motor imagery is accompanied by a subjective multisensory experience. This sensory experience is thought to result from the deployment of internal models developed for the execution and monitoring of overt actions. If so, how is it that motor imagery does not lead to overt execution? It has been proposed that inhibitory mechanisms may prevent execution during imagined actions such as imagined typing. To test this hypothesis, we combined an experimental with a modelling approach. We conducted an experiment in which participants (N = 49) were asked to alternate between overt (executed) and covert (imagined) typing. We predicted that motor inhibition should lead to longer reaction and movement times when the current trial is preceded by an imagined vs. an executed trial. This prediction was borne out by movement times, but not by reaction times. We introduced and fitted an algorithmic model of motor imagery to disentangle potentially distinct inhibitory mechanisms underlying these effects. Results from this analysis suggest that motor inhibition may affect different aspects of the latent activation function (e.g., the shape of the activation function or the motor execution threshold) with distinct consequences on reaction times and movement times. Overall, these results suggest that typing imagery involves the inhibition of motor commands related to typing acts. Preregistration, complete source code, and reproducible analyses are available at https://osf.io/y9a3k/."
  },
  {
    "objectID": "blog/2018-01-23-ppc/index.html",
    "href": "blog/2018-01-23-ppc/index.html",
    "title": "Checking the asumption of independence in binomial trials using posterior predictive checking",
    "section": "",
    "text": "What is a posterior predictive check ? According to Gelman et al. (2013, page 151), “Bayesian predictive checking generalizes classical hypothesis testing by averaging over the posterior distibution of the unknown parameters vector \\(\\theta\\) rather than fixing it at some estimate \\(\\hat{\\theta}\\)”.\nTo explore this idea in more details, we are going to extend an example presented in Gelman et al. (2013, page 147) to a case study I have already discussed in two previous blogposts (here and here). Let’s say I am recruiting participants for a psychology study that is lasting for approximately half an hour. If everything goes smoothly, I can manage to recruit 2 participants per hour, and doing it between 9am and 6pm (having the first participant at 9am, the second one at 9.30am and the last one at 5.30pm) for a whole week (from Monday to Friday) would give me 90 potential participants."
  },
  {
    "objectID": "blog/2018-01-23-ppc/index.html#posterior-predictive-checking",
    "href": "blog/2018-01-23-ppc/index.html#posterior-predictive-checking",
    "title": "Checking the asumption of independence in binomial trials using posterior predictive checking",
    "section": "",
    "text": "What is a posterior predictive check ? According to Gelman et al. (2013, page 151), “Bayesian predictive checking generalizes classical hypothesis testing by averaging over the posterior distibution of the unknown parameters vector \\(\\theta\\) rather than fixing it at some estimate \\(\\hat{\\theta}\\)”.\nTo explore this idea in more details, we are going to extend an example presented in Gelman et al. (2013, page 147) to a case study I have already discussed in two previous blogposts (here and here). Let’s say I am recruiting participants for a psychology study that is lasting for approximately half an hour. If everything goes smoothly, I can manage to recruit 2 participants per hour, and doing it between 9am and 6pm (having the first participant at 9am, the second one at 9.30am and the last one at 5.30pm) for a whole week (from Monday to Friday) would give me 90 potential participants."
  },
  {
    "objectID": "blog/2018-01-23-ppc/index.html#beta-binomial-model",
    "href": "blog/2018-01-23-ppc/index.html#beta-binomial-model",
    "title": "Checking the asumption of independence in binomial trials using posterior predictive checking",
    "section": "Beta-Binomial model",
    "text": "Beta-Binomial model\nWe know that some participants won’t show up to the time slot they registered for. I am interested in knowing the mean probability of presence, that we will call \\(\\theta\\).\nThis sequence of binary outcomes (presence vs. absence) \\(y_{1}, \\dots, y_{n}\\) is modelled as a serie of independent trials with common probability of success (presence) \\(\\theta\\), which is attributed a conjugate Beta prior, with shape parameters \\(\\alpha\\) and \\(\\beta\\) (encoded in the second line of our model).\n\\[\n\\begin{aligned}\ny &\\sim \\mathrm{Binomial}(n, \\theta) \\\\\n\\theta &\\sim \\mathrm{Beta}(\\alpha, \\beta) \\\\\n\\end{aligned}\n\\]\nWe could choose to give \\(\\theta\\) a uniform prior between 0 and 1 (to express our total ignorance about its value), but based on previous experiments I carried out, I know that participants tend to be present with a probability around \\(\\frac{1}{2}\\). Thus, we will choose a probability distribution that represents this prior knowledge (here a \\(\\mathrm{Beta}(2,2)\\), see the first figure for an illustration).\n\n# Checking the assumption of independence in binomial trials\n# Example inspired from Gelman et al. (2013, page 147)\nlibrary(tidyverse)\n\ny &lt;- # getting the data\n    c(\n        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0\n        )"
  },
  {
    "objectID": "blog/2018-01-23-ppc/index.html#the-problem",
    "href": "blog/2018-01-23-ppc/index.html#the-problem",
    "title": "Checking the asumption of independence in binomial trials using posterior predictive checking",
    "section": "The problem",
    "text": "The problem\nOur model is assuming independent trials, i.e., it’s assuming that the presence of a participant is independent of the presence of another participant, which is akin to say that the model is assuming no autocorrelation in the serie. Autocorrelation would be evidence that the model is flawed.\nOne way to estimate the degree of autocorrelation in the serie is to simply count the number of switches between presence and absence (i.e., between zeros and ones). An abnormally low number of switches (for a particular \\(n\\) and \\(\\theta\\)) would be evidence that some autocorrelation is present. Thus, the number of switches becomes a test quantity \\(T(y)\\), which describes the degree of autocorrelation in the serie, and a way of testing the assumptions of our model.\n\n# function to determine the number of switches in a numerical vector\n\nnb_switches &lt;- function(x) as.numeric(sum(diff(x) != 0) )\n\n# determining the number of switches Ty in observed data y\n\n(Ty &lt;- nb_switches(y) )\n\n[1] 28\n\n\nWe observed 28 switches in our data. To know whether this number is surprising, given our number of observation and the mean probability of presence, we will use use posterior predictive checking. But first, we need to compute the posterior distribution \\(p(\\theta | y)\\)."
  },
  {
    "objectID": "blog/2018-01-23-ppc/index.html#computing-the-posterior-distribution",
    "href": "blog/2018-01-23-ppc/index.html#computing-the-posterior-distribution",
    "title": "Checking the asumption of independence in binomial trials using posterior predictive checking",
    "section": "Computing the posterior distribution",
    "text": "Computing the posterior distribution\nWe know that the posterior density under this model is given by:\n\\[ p(\\theta | y) \\sim \\mathrm{Beta}(\\alpha + y, \\beta + n - y) \\] where \\(y\\) is the number of successes and \\(n\\) is the total number of observations. In our case, the posterior distribution of \\(\\theta\\) given \\(y\\) is then \\(\\mathrm{Beta}(2 + 55, 2 + 90 - 55) = \\mathrm{Beta}(57, 37)\\), which is plotted below.\n\n###############################################################\n# computing the posterior\n#######################################################\n\nn &lt;- length(y) # number of trials\nz &lt;- sum(y) # number of 1s\na &lt;- b &lt;- 2 # parameters of the beta prior\n\ngrid &lt;- seq(from = 0, to = 1, by = 0.01) # defines grid\n\n################################################\n# analytic derivation of the posterior\n#########################################\n\nprior &lt;- dbeta(grid, a, b)\nposterior &lt;- dbeta(grid, z + a, n - z + b)\n\ndata.frame(theta = grid, prior = prior, posterior = posterior) %&gt;%\n    gather(type, value, prior:posterior) %&gt;%\n    ggplot(aes(x = theta, y = value, colour = type, fill = type) ) +\n    geom_area(alpha = 0.8, position = \"identity\", size = 1) +\n    theme_bw(base_size = 12) +\n    scale_fill_grey() +\n    scale_colour_grey() +\n    ylab(\"\")\n\n\n\n\n\n\n\n\nThe mean of the posterior distribution is given by \\(\\dfrac{\\alpha + y}{\\alpha + \\beta + n}\\), and is equal to (a + z) / (a + b + n) = 0.606, which can be interpreted as the mean probability of presence."
  },
  {
    "objectID": "blog/2018-01-23-ppc/index.html#conjugacy",
    "href": "blog/2018-01-23-ppc/index.html#conjugacy",
    "title": "Checking the asumption of independence in binomial trials using posterior predictive checking",
    "section": "Conjugacy",
    "text": "Conjugacy\nThis example allows us to define what conjugacy is. Formally, if \\(\\mathcal{F}\\) is a class of sampling distributions \\(p(y|\\theta)\\), and \\(\\mathcal{P}\\) is a class of prior distributions for \\(\\theta\\), then the class \\(\\mathcal{P}\\) is conjugate fo \\(\\mathcal{F}\\) if\n\\[p(\\theta|y) \\in \\mathcal{P} \\text{ for all } p(\\cdot | \\theta) \\in \\mathcal{F} \\text{ and } p(\\cdot) \\in \\mathcal{P}\\]\n(Gelman et al., 2013, page 35). In other words, a prior is called a conjugate prior if, when converted to a posterior by being multiplied by the likelihood, it keeps the same form. In our case, the Beta prior is a conjugate prior for the Binomial likelihood, because the posterior is a Beta distribution as well."
  },
  {
    "objectID": "blog/2018-01-23-ppc/index.html#posterior-predictive-checking-1",
    "href": "blog/2018-01-23-ppc/index.html#posterior-predictive-checking-1",
    "title": "Checking the asumption of independence in binomial trials using posterior predictive checking",
    "section": "Posterior predictive checking",
    "text": "Posterior predictive checking\nHow posterior predictive checking can help us to assess whether the assumption of indendence is respected in our observed data \\(y\\) ? Well, our model is actually assuming independence, so we could ask our model to generate new observations, or replications, called \\(y^{rep}\\), to see whether these replications differ from the observed data. If they do, it would mean that the observed data are not well described by a model that is assuming independence.\nThis is done in two steps. First, we generate possible values of \\(\\theta\\) from its posterior distribution (i.e., from a \\(\\mathrm{Beta}(57, 37)\\) distribution). Then, for each of these \\(\\theta\\) values, we generate a new set of observations \\(y^{rep}\\) from a Binomial distribution.\n\n########################################################\n# posterior predictive checks\n#######################################\n\nnsims &lt;- 1e4 # number of replicated samples\n\n# generating nsims theta values from posterior\n\nthetas &lt;- rbeta(nsims, a + z, b + n - z)\n\n# generating nsims new datasets (Yrep)\n\nYrep &lt;-\n    sapply(\n        # for each theta\n        1:length(thetas),\n        # generating samples\n        function(i) sample(\n            c(0, 1),\n            # of the same length as y\n            length(y),\n            replace = TRUE,\n            # with prob of presence equals to theta\n            # and prob of absence equals to 1 - theta\n            prob = c(thetas[i], 1 - thetas[i])\n            )\n        )\n\nThen, we can compute the number of switches (our test quantity) in each replicated sample, to check whether the number of switches computed on datasets generated under the assumption of independence differ from the number of switches computed on the observed dataset \\(y\\). We call the test quantities computed on replicated samples \\(T(y^{rep})\\).\n\n# for each new Yrep sample, computing the number of switches Trep, and\n# comparing it to observed number of switches Ty\n\nTrep &lt;- apply(Yrep, 2, nb_switches)\n\nTrep %&gt;%\n    BEST::plotPost(\n        compVal = Ty, breaks = 20,\n        col = \"#E6E6E6\", xlab = expression(T(y^rep) ) )\n\n\n\n\n\n\n\n\nThis histogram reveals that the mean number of switches accross the nsims replications is about 42.09, and the green vertical dotted line represents the position of \\(T(y)\\) in the distribution of \\(T(y^{rep})\\) values.\nTo know whether the observed number of switches is surprising given the assumptions of our model (represented by its predictions), we can count the number of replications that lead to a greater number of switches than the number of switches \\(T(y)\\) in the observed data.\n\nsum(Trep &gt; Ty)\n\n[1] 9927\n\n\nOr we can compute a Bayesian p-value as (Gelman et al., 2013, page 146):\n\\[p_{B} = \\text{Pr}(T(y^{rep}, \\theta) \\geq T(y, \\theta) | y)\\]\n\n1 - sum(Trep &gt; Ty) / nsims # equivalent to sum(Trep &lt;= Ty) / nsims\n\n[1] 0.0073\n\n\nWhich gives the probability of observing this number of switches under our model. What does it mean ? Does it mean that our model is wrong ? Well, not exactly. Models are neither right or wrong (see Crane & Martin, 2018). But our model does not seem to capture the full story, it does not seem to give a good representation of the process that generated our data (which is arguably one of the characteristics that contribute to the soundness of a model).\nMore precisely, it misses the point that the probabilities of successive participants being present are not independent. This, in our case, seems to be due to temporal fluctuations of this probability throughout the day. For instance, the probability of a participant being present seems to be the lowest early in the morning or late in the afternoon, as well as between 12am and 2pm. This temporal dependency could be better taken into account by using gaussian process regression models, that generalise the varying effect strategy of multilevel models to continuous variables. In other words, it would allow to take into account that participants coming to the lab at similar hours (e.g., 9am and 9.30am) are more similar (in their probability of being present) than participants coming at very different hours (e.g., 9am and 3pm)."
  },
  {
    "objectID": "blog/2018-01-23-ppc/index.html#conclusions",
    "href": "blog/2018-01-23-ppc/index.html#conclusions",
    "title": "Checking the asumption of independence in binomial trials using posterior predictive checking",
    "section": "Conclusions",
    "text": "Conclusions\nIn this post we aimed to introduce the idea of posterior predictive checking by recycling an elegant and simple example from Gelman et al. (2013). It should be noted however that this kind of check can be done for any test quantity of interest (e.g., the mean or the max of a distribution, or its dispersion).\nAs put by Gelman et al. (2013, page 148), “because a probability model can fail to reflect the process that generated the data in any number of ways, posterior predictive p-values can be computed for a variety of test quantities in order to evaluate more than one possible model failure”.\nSo come on, let’s make p-values great again, they are not doomed to be used only as a point-null hypothesis test."
  },
  {
    "objectID": "blog/2018-01-23-ppc/index.html#references",
    "href": "blog/2018-01-23-ppc/index.html#references",
    "title": "Checking the asumption of independence in binomial trials using posterior predictive checking",
    "section": "References",
    "text": "References\n\n\nClick to expand\n\n\nCrane, H., & Martin, R. (2018, January 10). Is statistics meeting the needs of science? Retrieved from psyarxiv.com/q2s5m\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis, Third Edition. CRC Press."
  },
  {
    "objectID": "blog/2017-10-10-icc/index.html",
    "href": "blog/2017-10-10-icc/index.html",
    "title": "Three methods for computing the intra-class correlation in multilevel logistic regression",
    "section": "",
    "text": "In a previous post, we introduced the mutilevel logistic regression model and implemented it in R, using the brms package. We tried to predict the presence of students that registered for psychological experiments. We also discussed the use of the intra-class correlation (ICC) –also known as the variance partitioning coefficient (VPC)–, as a mean to quantifies the proportion of observed variation in the outcome that is attributable to the effect of clustering.\nHowever, the computation and the interpretation of the ICC in the context of the logistic regression are not straightforward. In the current post, we will then present and compare three methods of obtaining an estimation of the ICC in multilevel logistic regression models."
  },
  {
    "objectID": "blog/2017-10-10-icc/index.html#getting-the-data",
    "href": "blog/2017-10-10-icc/index.html#getting-the-data",
    "title": "Three methods for computing the intra-class correlation in multilevel logistic regression",
    "section": "Getting the data",
    "text": "Getting the data\nWe will use a dataset contained in the rethinking package, which is used and discussed several times in the Statistical Rethinking book (McElreath, 2016).\n\nlibrary(rethinking)\ndata(UCBadmit)\ndata &lt;- UCBadmit\ndata$gender &lt;- ifelse(data$applicant.gender==\"female\", -0.5, 0.5)\ndata$dept_id &lt;- coerce_index(data$dept)\ndata\n\n   dept applicant.gender admit reject applications gender dept_id\n1     A             male   512    313          825    0.5       1\n2     A           female    89     19          108   -0.5       1\n3     B             male   353    207          560    0.5       2\n4     B           female    17      8           25   -0.5       2\n5     C             male   120    205          325    0.5       3\n6     C           female   202    391          593   -0.5       3\n7     D             male   138    279          417    0.5       4\n8     D           female   131    244          375   -0.5       4\n9     E             male    53    138          191    0.5       5\n10    E           female    94    299          393   -0.5       5\n11    F             male    22    351          373    0.5       6\n12    F           female    24    317          341   -0.5       6\n\n\nThese are graduate school applications to 6 different academic departments at UC Berkeley. The admit and the reject columns indicate the number of admission and rejections, respectively. The applications column is the total nuber of applications (i.e., the sum of admit and reject). We would like to estimate whether there is a gender bias in admissions."
  },
  {
    "objectID": "blog/2017-10-10-icc/index.html#fitting-the-model",
    "href": "blog/2017-10-10-icc/index.html#fitting-the-model",
    "title": "Three methods for computing the intra-class correlation in multilevel logistic regression",
    "section": "Fitting the model",
    "text": "Fitting the model\nWe will then fit a model that include gender as a predictor, to estimate the associattion between gender and the probability of admission. However, as the probability of admission can vary considerably between departments, and as the number of application of males and females can also vary according to the department, we might want to include a varying intercept by department. Thus, we will estimate the grand mean probability of admission, while still allowing each department to have an independant probability of admission.\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Binomial}(n_{i}, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha_{dept[i]} + \\beta \\times \\text{gender}_{i}\\\\\n\\alpha_{dept} &\\sim \\mathrm{Normal}(\\alpha, \\tau) \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 5) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, 5) \\\\\n\\tau &\\sim \\mathrm{HalfCauchy}(0, 5) \\\\\n\\end{aligned}\n\\]\nAlthough we ignored it in the last post, noteworthy here is that we have to define priors on the log-odds scale… In order to get an intuition of what it means, it might be useful to visualise these priors on both the log-odds scale and the probability scale (i.e., what we are really interested in). To this end, we will use an home-made function called prior_scales, that allows to plot priors on both scales.\n\nprior_scales &lt;- function(prior, ...) {\n    \n    library(LaplacesDemon)\n    \n    # extracting the distribution and its arguments\n    prior_type &lt;- get(paste0(\"r\", sub(\"\\\\(.*\", \"\", prior) ) )\n    prior_args &lt;- gsub(\".*\\\\((.*)\\\\).*\", \"\\\\1\", prior)\n    \n    # drawing n samples from the prior\n    n &lt;- 1e6\n    sim &lt;- eval(parse(text = paste(\"prior_type(n,\", prior_args, \")\") ) )\n    \n    # plotting the prior in the log-odds scale\n    dens(sim, col = \"steelblue\", lwd = 2, main = prior, xlab = \"log-odds\")\n    \n    # plotting the prior in the probability scale\n    dens(plogis(sim), col = \"steelblue\", lwd = 2, main = prior, xlab = \"probability\")\n}\n\n\npar(mfrow = c(2, 2) )\nprior_scales(prior = \"norm(0,5)\")\nprior_scales(prior = \"halfcauchy(5)\")\n\n\n\n\n\n\n\n\nAs we can see, in the weird logit world, the normal and halfcauchy priors tend to favour extreme values. To prevent this, McElreath (2016, page 363) suggests to use exponential priors for the variance components, instead of the normal or halfcauchy priors.\n\npar(mfrow = c(1, 2) )\nprior_scales(prior = \"exp(2)\")\n\n\n\n\n\n\n\n\nThe model with the exponential prior can be fitted easily with brms, as follows.\n\nlibrary(brms)\n\nprior1 &lt;- c(\n    prior(normal(0, 5), class = Intercept, coef = \"\"),\n    prior(normal(0, 5), class = b, coef = \"gender\"),\n    prior(exponential(2), class = sd) )\n\nmodel1 &lt;- brm(\n    admit | trials(applications) ~ 1 + gender + (1 | dept_id),\n    family = binomial(link = \"logit\"),\n    prior = prior1,\n    data = data,\n    iter = 1e3,\n    cores = parallel::detectCores(),\n    control = list(adapt_delta = 0.95)\n    )\n\nWe can easily obtain a summary of the model as follows:\n\nsummary(model1, parameters = c(\"^b_\", \"^sd_\"), prob = 0.95)\n\n Family: binomial \n  Links: mu = logit \nFormula: admit | trials(applications) ~ 1 + gender + (1 | dept_id) \n   Data: data (Number of observations: 12) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nMultilevel Hyperparameters:\n~dept_id (Number of levels: 6) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.21      0.33     0.73     2.01 1.01      434      613\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.60      0.51    -1.60     0.51 1.01      368      368\ngender       -0.09      0.08    -0.25     0.07 1.00     1179      879\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs previously, we can interpret the variation of the intercept \\(\\alpha\\) between departments by considering the ICC, which goes from 0 if the grouping conveys no information to 1 if all levels of a cluster are identical (Gelman, 2006, p. 258). In other words, ICC = 0 indicates perfect independence of residuals: the observations do not depend on cluster membership. When the ICC is not different from zero or negligible, one could consider running traditional one-level regression analysis. On the contrary, ICC = 1 indicates perfect interdependence of residuals: the observations only vary between clusters (Sommet & Morselli, 2017)."
  },
  {
    "objectID": "blog/2017-10-10-icc/index.html#what-is-the-problem",
    "href": "blog/2017-10-10-icc/index.html#what-is-the-problem",
    "title": "Three methods for computing the intra-class correlation in multilevel logistic regression",
    "section": "What is the problem?",
    "text": "What is the problem?\nThe ICC is usually expressed as \\(\\dfrac{\\tau^{2}}{\\tau^{2} + \\sigma^{2}}\\), where \\(\\tau^2\\) denotes the variance of the distribution of the varying effects, and \\(\\sigma^{2}\\) the variance of the residuals. However, in the context of logistic regression, there is no direct estimation of the residuals \\(\\sigma^2\\) on the first level. Unlike in the normal case, the level 1 variance depends on the expected value, as \\(var(p_{ij}) = p_{ij}(1-p_{ij})\\), and the fixed predictor in the model depends on the value of gender. Therefore, as we are considering a function of the predictor variable gender, a simple ICC is not available, even though there is only a single level 2 variance. Furthermore, the level 2 variance is measured on the logistic scale and so is not directly comparable to the level 1 variance (Goldstein, 2010).\nIn the following, we consider three different approaches to approximate the ICC. Basically, these procedures convert both the between-cluster and the within-cluster variances to the same scale, to allows the subsequent computation of the ICC."
  },
  {
    "objectID": "blog/2017-10-10-icc/index.html#method-1-the-latent-variable-approach",
    "href": "blog/2017-10-10-icc/index.html#method-1-the-latent-variable-approach",
    "title": "Three methods for computing the intra-class correlation in multilevel logistic regression",
    "section": "Method 1: The latent variable approach",
    "text": "Method 1: The latent variable approach\nThe latent variable approach considers the observed binary response to represent a thresholded continuous variable where we observe 0 below the threshold and 1 above.\nIn a logit model we have an underlying logistic distribution for such a variable. We know that the logistic distribution has variance \\(\\pi^{2} / 3 = 3.29\\). We can then take this as the level 1 variance so that now both the level 1 and 2 variances are on the same scale. From there, the ICC is given by the simple formula \\(\\dfrac{\\tau^{2}}{\\tau^{2} + \\frac{\\pi^2}{3}}\\).\n\n# extracting tau^2 for the varying intercept\ntau2 &lt;- brms::VarCorr(model1)[[1]]$sd[1]^2\n\n# computing the ICC for the intercept\nICC1 &lt;- tau2 / (tau2 + (pi^2 / 3) )\nICC1\n\n[1] 0.3070678\n\n\nNote though that when there are predictors in the model, the ICC should have a conditional interpretation: of the residual variation in outcomes that remains after accounting for the variables in the model, it is the proportion that is attributable to systematic differences between clusters (i.e., in our example between departments).\nGoldstein et al. (2002) suggest that the above approach to evaluating the ICC is only appropriate when the binary response can be conceptualized as the discretization of an underlying continuous latent variable (e.g., pass/fail on a test is a binary representation of an underlying continuous latent variable denoting the test score). For a binary outcome such as mortality, they suggest that such an assumption may not be warranted as it is unobservable1.\nOn the other hand, one can assume that there is an underlying propensity of dying and that an individual dies when he/she reaches a certain threshold… However, Goldstein et al. (2002) described a simulation-based approach that does not require this assumption, and that we will present in the next section."
  },
  {
    "objectID": "blog/2017-10-10-icc/index.html#method-2-simulation",
    "href": "blog/2017-10-10-icc/index.html#method-2-simulation",
    "title": "Three methods for computing the intra-class correlation in multilevel logistic regression",
    "section": "Method 2: Simulation",
    "text": "Method 2: Simulation\nWhile very useful, a characteristic of this simulation-based approach is that it is dependent on specific covariate patterns. Thus, one could conceivably have a different value of the ICC for each distinct covariate pattern (this could be of substantive interest in and of itself). In the following example, we then compute the ICC for every value of the gender predictor, that is for females and for males. The proposed algorithm is as follows.\n\nSimulate a large number \\(N\\) of varying effects from the varying effects distribution that was estimated by the multilevel logistic regression model: \\(\\alpha_{dept} \\sim \\mathrm{Normal}(\\alpha, \\tau)\\).\nFor a specific covariate pattern (i.e., for a particular chosen value of gender), use each of the simulated random effects drawn previously to compute the predicted probability \\(p_{ij}\\) of the outcome. For each of these computed probabilities, compute the Level 1 variance: \\(v_{ij}=p_{ij}(1-p_{ij})\\).\nThe ICC is then evaluated as: \\(\\dfrac{Var(p_{ij})}{Var(p_{ij})+\\frac{1}{N}\\sum_{i=1}^{N}v_{ij}}\\).\n\n\n# extracting the model estimates\nest &lt;- brms::fixef(model1)[,1]\n\n# starting from hell\nset.seed(666)\n# number of simulations\nN &lt;- 1e5\n\n# drawing varying effects from the estimated distribution of varying effects\na_dpt &lt;- rnorm(N, mean = est[1], sd = sqrt(tau2) )\n\n# computing the ICC for females\n# probability of the outcome\npA &lt;- exp(a_dpt + est[2] * -0.5) / (1 + exp(a_dpt + est[2] * -0.5) )\n# compute the Bernoulli level-1 residual variance\nvA &lt;- pA * (1 - pA)\n# mean of Bernoulli variances\nsA &lt;- mean(vA)\n# compute the ICC\nICC2.f &lt;- var(pA) / (var(pA) + sA)\n\n# computing the ICC for males\n# probability of the outcome\npA &lt;- exp(a_dpt + est[2] * 0.5) / (1 + exp(a_dpt + est[2] * 0.5) )\n# compute the Bernoulli level-1 residual variance\nvA &lt;- pA * (1 - pA)\n# mean of Bernoulli variances\nsA &lt;- mean(vA)\n# compute the ICC\nICC2.m &lt;- var(pA) / (var(pA) + sA)\n\nc(ICC2.f, ICC2.m)\n\n[1] 0.2192562 0.2175437\n\n\nWe see that this method provides estimates slightly inferior to the latent variable approach, and indicates no noticeable differences between females and males."
  },
  {
    "objectID": "blog/2017-10-10-icc/index.html#method-3-model-linearisation-from-goldstein-browne-rasbah-2002",
    "href": "blog/2017-10-10-icc/index.html#method-3-model-linearisation-from-goldstein-browne-rasbah-2002",
    "title": "Three methods for computing the intra-class correlation in multilevel logistic regression",
    "section": "Method 3: Model linearisation (from Goldstein, Browne, & Rasbah, 2002)",
    "text": "Method 3: Model linearisation (from Goldstein, Browne, & Rasbah, 2002)\nRecall that we try to estimate here the probability of admission \\(p_{ij}\\). Using a first order Taylor expansion (e.g., Goldstein & Rasbash, 1996; Goldstein, 2010), we can rewrite our model and evaluate \\(p_{ij}\\) at the mean of the distribution of the level 2 varying effect, that is, for the logistic model\n\\[p_{ij}=\\exp(\\alpha+\\beta\\times gender_{i}) /(1+\\exp(\\alpha+\\beta\\times gender_{i}))\\]\nso that we have\n\\[var(y_{ij}|gender_{i})=\\tau^{2}p_{ij}^2([1+\\exp(\\alpha+\\beta\\times gender_{i})]^{-2}+p_{ij}(1-p_{ij})\\] and\n\\[\\text{ICC}=\\dfrac{\\tau^{2}p_{ij}^2([1+\\exp(\\alpha+\\beta\\times gender_{i})]^{-2}}{\\tau^{2}p_{ij}^{2}[1+\\exp(\\alpha+\\beta\\times gender_{i})]^{-2}+p_{ij}(1-p_{ij})}\\]\nBelow, we use this method to evaluate the ICC for both females (i.e., gender=-0.5) and males (i.e., gender=0.5).\n\n# evaluating pi at the mean of the distribution of the level 2 varying effect\np &lt;- exp(est[1] + est[2] * -0.5) / (1 + exp(est[1] + est[2] * -0.5) )\n# computing var(p)\nsig1 &lt;- p * (1 - p)\n# computing var(yij)\nsig2 &lt;- tau2 * p^2 * (1 + exp(est[1] + est[2] * -0.5) )^(-2)\n# computing the ICC\nICC3.f &lt;- sig2 / (sig1 + sig2)\n\n# evaluating pi at the mean of the distribution of the level 2 varying effect\np &lt;- exp(est[1] + est[2] * 0.5) / (1 + exp(est[1] + est[2] * 0.5) )\n# computing pi'\nsig1 &lt;- p * (1 - p)\n# computing var(yij)\nsig2 &lt;- tau2 * p^2 * (1 + exp(est[1] + est[2] * 0.5) )^(-2)\n# computing the ICC\nICC3.m &lt;- sig2 / (sig1 + sig2)\n\nc(ICC3.f, ICC3.m)\n\nIntercept Intercept \n0.2526226 0.2475386 \n\n\nHere again, the amount of variation by department seem to be similar for both females and males. Below we summarise the results of the three methods.\n\n\n\n\n\n\n\nmethod 1\nmethod 2\nmethod 3\n\n\n\n\nICC\n0.3070678\n0.2192562\n0.2526226"
  },
  {
    "objectID": "blog/2017-10-10-icc/index.html#conclusions",
    "href": "blog/2017-10-10-icc/index.html#conclusions",
    "title": "Three methods for computing the intra-class correlation in multilevel logistic regression",
    "section": "Conclusions",
    "text": "Conclusions\nNow that you reached the end of this post, I should confess that there exists packages out there that implement these three methods, and even more. The icc function in the sjstats package allow to compute the ICC for models fitted with lme4, while the ICCbin package offer 16 different methods to compute the ICC for binary responses. The iccbin function of the aod package implements three of the four methods described by Goldstein et al. (2002).\nIn conclusion, it is interesting to note that the VPC and the ICC cease to be the same thing if we allow the slope to vary (Goldstein et al., 2002). We can find more on this topic in Kreft & De Leeuw (1998, page 63):\n\n“The concept of intra-class correlation is based on a model with a random intercept only. No unique intra-class correlation can be calculated when a random slope is present in the model. The value of the between variance in models with a random slope and a random intercept is a combination of slope and intercept variance (and covariance). We know from the discussion of the basic RC model that the variance of the slope (and, as a consequence, the value of the covariance) is related to the value of the explanatory variable x. Thus the intra-class correlation between individuals will be different, in models with random slopes, for individuals with different x-values. As a result the intra-class correlation is no longer uniquely defined”.\n\nBut maybe we should keep this for a future post."
  },
  {
    "objectID": "blog/2017-10-10-icc/index.html#references",
    "href": "blog/2017-10-10-icc/index.html#references",
    "title": "Three methods for computing the intra-class correlation in multilevel logistic regression",
    "section": "References",
    "text": "References\n\n\nClick to expand\n\n\nAustin, P. C., & Merlo, J. (2017). Intermediate and advanced topics in multilevel logistic regression analysis. Statistics in Medicine, 36, 3257-3277.\nBürkner, P.-C. (2017). brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software, 80(1), 1-28. doi:10.18637/jss.v080.i01\nGoldstein, H. and Rasbash, J. (1996). Improved approximations for multilevel models with binary responses. Journal of the Royal Statistical Society, A. 159: 505-13.\nGoldstein, H., Browne, W., & Rasbash, J. (2002). Partitioning variation in generalised linear multilevel models. Understanding Statistics, 1:223–232.\nGoldstein, H. (2010). Multilevel Statistical Models, 4th Edition. John Wiley & Sons, Ltd, Chichester, UK.\nMcElreath, R. (2016). Statistical Rethinking. Chapman; Hall/CRC.\nSnijders, T., & Bosker, R. (1999). Multilevel Analysis. Sage."
  },
  {
    "objectID": "blog/2017-10-10-icc/index.html#footnotes",
    "href": "blog/2017-10-10-icc/index.html#footnotes",
    "title": "Three methods for computing the intra-class correlation in multilevel logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee also Snijders and Bosker (1999, Chapter 14) for a further discussion.↩︎"
  },
  {
    "objectID": "blog/2017-09-30-summersaga/index.html",
    "href": "blog/2017-09-30-summersaga/index.html",
    "title": "The saga of the summer 2017, a.k.a. ‘the alpha wars’",
    "section": "",
    "text": "The adequacy of Null-Hypothesis-Significance-Testing (NHST) -and particularly of the dichotomised use of p-values- to scientific progress has long been questioned (e.g., Bakan, 1966; Gigerenzer, Krauss, & Vitouch, 2004; Kline, 2004; Lambdin, 2012). Despite this wealth of critics, the p-value stays one of the favourite statistical tool of the experimentalist. As an optimistic person though, I like to see recent institutional changes such as the ASA statement on the use of the p-values as an encouraging sign (Wassertein & Lazar, 2016). However, statements such as the ASA statement do not obtain a majority of agreements, neither within statisticians nor within applied scientists. Moreover, efforts to distancing p-values from scientific daily practice might be hindered by alternative usages of NHST that have been suggested during the last years (e.g., the New Statistics of Cumming, 2012).1\nThis summer was very prolific in regards to this debate, because many new ideas on NHST were presented and discussed (in pre-prints but also in blogposts). Noteworthy here is the rhythm of the debate, that has been insanely increased by the use of social media and by preprint sharing. In this post I will then try to summarise the highlights of this exciting saga."
  },
  {
    "objectID": "blog/2017-09-30-summersaga/index.html#round-i",
    "href": "blog/2017-09-30-summersaga/index.html#round-i",
    "title": "The saga of the summer 2017, a.k.a. ‘the alpha wars’",
    "section": "Round I",
    "text": "Round I\nThe first bomb of this summer was posted online as a preprint on July 22, on PsyArxiv. This short proposal, co-authored by 72 researchers of several disciplines (including respected statisticians), mainly suggested to change the default threshold for statistical significance from 0.05 to 0.005. According to them, “This simple step would immediately improve the reproducibility of scientific research in many fields”. They also suggested to rename p-values falling between 0.005 and 0.05 as “suggestive” in place of the current “significant”.\nThe proposition of this particular threshold is motivated by a comparison of the p-value and the Bayes Factor (BF), showing that p-values around 0.05 are usually equivalent to BF10 in the range of 2.5-3.4, which is usually regarded as very weak evidence. In contrast, lowering the threshold to 0.005 would lead to equivalent BF10s in the range of 14-26, which corresponds to well… quite more convincing evidence.\nTheir second point is that lowering the threshold to 0.05 would lead to a more reasonable rate (lower) of false positive. The following figure shows the false positive rate as a function of power, for different levels of significance and different prior odds that there is a true effect.\n\n\n\n\n\n\n“For many, the calculations illustrated by Fig. 2 may be unsettling. For example, the false positive rate is greater than 33% with prior odds of 1:10 and a P value threshold of 0.05, regardless of the level of statistical power. Reducing the threshold to 0.005 would reduce this minimum false positive rate to 5%. Similar reductions in false positive rates would occur over a wide range of statistical powers.”\n\nThis proposal also implies (and this is maybe the most crucial point) that sample sizes should increase (of approximately 70% on average), in order to keep power constant and to not inflates the rate of false negative."
  },
  {
    "objectID": "blog/2017-09-30-summersaga/index.html#round-ii",
    "href": "blog/2017-09-30-summersaga/index.html#round-ii",
    "title": "The saga of the summer 2017, a.k.a. ‘the alpha wars’",
    "section": "Round II",
    "text": "Round II\nImmediately after the publication of this preprint, the rebellion started to organise online, with Daniel Lakens as its head. Suddenly, a wealth of blogposts explaining why this proposal was a terrible idea appeared (see for instance here, here, here, or here). Three days later, Daniel Lakens launched on Twitter a massive appeal to a generalised rebellion, asking help to whoever wanted to join the fight.\n\n\n{{% tweet \"890089098219835393\" %}}\n\n\nThis collaborative work gave birth on September 18 to the second bomb of the summer entitled Justify Your Alpha, and co-authored by Daniel Lakens and 87 alphabetically ordered authors.\nThe main argument of this proposal is that p-values should be used in a Neyman-Person spirit, meaning as a way of balancing type-I and type-II errors. Accordingly, they suggest that the significance threshold should be calibrated according to study-specific objectives, and stressing that researchers should transparently report and justify all choices they make when designing a study, including the alpha level. On how to choose this threshold, they add that this decision should be based on statistical decision theory, where costs and benefits are compared against a utility function (Neyman & Pearson, 1933; Skipper, Guenther, & Nass, 1967).\nFinally, their three main concerns are summarised in the last paragraph of the comment:\n\n“Although we agree with Benjamin et al. (2017) that the relatively high rate of non-replication in the scientific literature is a cause for concern, we do not believe that redefining statistical significance is a desirable solution: (1) there is not enough evidence that a blanket threshold of p&lt;.005 will improve replication sufficiently to be worth the additional cost in data collection, (2) the justifications given for the new threshold are not strong enough to warrant the widespread implementation of such a policy, and (3) there are realistic concerns that a p&lt;.005 threshold will have negative consequences for science, which should be carefully examined before a change in practice is instituted. Instead of a narrower focus on p-value thresholds, we call for a broader mandate whereby all justifications of key choices in research design and statistical practice are pre-registered whenever possible, fully accessible, and transparently evaluated.”"
  },
  {
    "objectID": "blog/2017-09-30-summersaga/index.html#round-iii",
    "href": "blog/2017-09-30-summersaga/index.html#round-iii",
    "title": "The saga of the summer 2017, a.k.a. ‘the alpha wars’",
    "section": "Round III",
    "text": "Round III\nThe last contribution to date comes from McShane, Gal, Gelman, Robert, & Tackett, and was published on September 21 on Gelman’s website. This contribution is kind of killing the above debate, because they mainly argue for entirely abandoning statistical significance.\nInstead of the current exclusive reliance on p-values to draw inferences from empirical data, they suggest that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with the neglected factors as just one among many pieces of evidence. Although this idea was admittedly already present in the proposal of round II (Lakens et al., 2017), their conclusion diverges quite considerably from the conclusion of Lakens et al., as they recommend to entirely abandon NHST. This suggestion is partly based on the well-known criticisms of NHST as based on a non-existing, non-realistic nil hypothesis, that is summarised in these lines:\n\n“Given that effects are small and variable and measurements are noisy, the sharp point null hypothesis of zero effect and zero systematic error used in the overwhelming majority of applications is itself implausible (Berkson, 1938, Edwards et al., 1963, Bakan, 1966, Tukey, 1991, Cohen, 1994, Gelman et al., 2014, McShane and Böckenholt, 2014, Gelman, 2015). Consequently, Cohen (1994) has derided this null hypothesis as the “nil hypothesis” and lampoons it as “always false,” and Tukey (1991) notes that two treatments are “always different.” Indeed, even were an effect truly zero, experimental realities dictate that the effect would not be exactly zero in any study designed to test it”.\n\nAnother of their concern is the problem of the dichotimisation of evidence into statistically significant and non-significant results, that according to the authors, would lead to biases when reasoning from such statistics, leading to confuse the statistical and the practical significance of a result. They also insist on the urge to consider, along with the p-value, what they call the neglected factors, that is, prior and related evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain.\nIn conclusion and maybe paradoxically (considering the title of the paper), they say We have no desire to “ban” p-values. Instead, we offer two concrete recommendations— one for editors and reviewers and one for authors— for how, in practice, the p-value can be demoted from its threshold screening role and instead be considered as just one among many pieces of evidence.\nFinally, the idea of abandoning statistical significance is not really new and we have to note that this idea was already suggested at the end of the paper of Benjamin et al., (2017), as they said that Many of us agree that there are better approaches to statistical analyses than null hypothesis significance testing […] even after the significance threshold is changed, many of us will continue to advocate for alternatives to null hypothesis significance testing. However, this comment has the merits of explicitly claiming that significance testing should be abandoned, and of having made a great use of the popularity of the current debate to spread this idea.\nMoreover, in the meantime, a comment has been published in Nature Human Behaviour (Amrhein & Greenland, 2017), also pushing to remove statistical thresholds, and arguing that a lowered threshold for significance would only accentuate the pitfalls that the first proposal wanted to highlight, by unreasonably increasing our confidence in findings with p&lt;.005."
  },
  {
    "objectID": "blog/2017-09-30-summersaga/index.html#update-november-16th-2017",
    "href": "blog/2017-09-30-summersaga/index.html#update-november-16th-2017",
    "title": "The saga of the summer 2017, a.k.a. ‘the alpha wars’",
    "section": "Update (November 16th, 2017)",
    "text": "Update (November 16th, 2017)\nSince the first upload of this post, a new fifty-authors comment has been submitted to Nature Human Behaviour, for which a preprint can be found on PeerJ. While this comment (lead by David Trafimow and Valentin Amrhein) is echoing some of the propositions put forward by McShane et al., it also brings some new arguments into the light. For instance, it highlights the point made by Trafimow & Earp (2017) about type-I and type-II errors, namely, that their relative importance might differ across study, researchers, or disciplines (a point similar to the proposal lead by Daniel Lakens), and that the domain to which they apply is often blurry and usually remains to be defined. They also stress that the relative importance of type-I and type-II errors depends on a large variety of factors, rendering undesirable any fixed recommendation concerning the alpha level.\nConcerning replicability, they argue that shifting from the usual threshold to the .005 one would increase the importance of the population effect size in obtaining statistical significance. Indeed, if sample sizes remain similar, such a proposal would favour huge population effect sizes and make smaller effect sizes more difficult to obtain. Based on a tasty example from the history of physics, they demonstrate that replicability should not depend on the population effect size. They add:\n\n“Any proposal that features p-value rejection criteria forces the replication probability to be impacted by the population effect size, and should be rejected”.\n\nFinally, they relay the idea presented in Trafimow (2017) & Trafimow & MacDonald (2017), of using a priori inferential statistics, in replacement for NHST. They also stress the importance of considering multiple sources of information for inference (i.e., not relying on a single index, such as a p-value or a Bayes Factor), and suggest that statistical inference should not be based on single studies."
  },
  {
    "objectID": "blog/2017-09-30-summersaga/index.html#conclusions",
    "href": "blog/2017-09-30-summersaga/index.html#conclusions",
    "title": "The saga of the summer 2017, a.k.a. ‘the alpha wars’",
    "section": "Conclusions",
    "text": "Conclusions\nIn this short blogpost, I have tried to summarise the main official contributions that have been published these last months around the alpha-wars. It is highly probable that a lot of new contributions will add to this list, and so this post may be edited / updated regularly."
  },
  {
    "objectID": "blog/2017-09-30-summersaga/index.html#references",
    "href": "blog/2017-09-30-summersaga/index.html#references",
    "title": "The saga of the summer 2017, a.k.a. ‘the alpha wars’",
    "section": "References",
    "text": "References\n\n\nClick to expand\n\n\nAmrhein, V., & Greenland, S. (2017, September 25). Remove, rather than redefine, statistical significance. Nature Human Behaviour, doi:10.1038/s41562-017-0224-0\nBakan, D. (1966). The test of significance in psychological research. Psychological Bulletin, 66(6), 423–437. doi:10.1037/h0020412\nBenjamin, D. J., Berger, J., Johannesson, M., Nosek, B. A., Wagenmakers, E.-J., Berk, R., … Johnson, V. (2017, July 22). Redefine statistical significance. Retrieved from psyarxiv.com/mky9j\nCumming, G. (2012). Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis. New York: Routledge.\nGigerenzer, G., Krauss, S., & Vitouch, O. (2004). The Null Ritual What You Always Wanted to Know About Significance Testing but Were Afraid to Ask. The Sage Handbook of Methodology for the Social Sciences, 391–408. doi:10.4135/9781412986311.n21\nKline, R. (2004). What’s Wrong With Statistical Tests–And Where We Go From Here. doi:10.1037/10693-003\nLakens, D., Adolfi, F. G., Albers, C. J., Anvari, F., Apps, M. A. J., Argamon, S. E., … Zwaan, R. A. (2017, September 18). Justify Your Alpha: A Response to “Redefine Statistical Significance”. Retrieved from psyarxiv.com/9s3y6\nLambdin, C. (2012). Significance tests as sorcery: Science is empirical–significance tests are not. Theory & Psychology, 22(1), 67–90. doi:10.1177/0959354311429854\nMcShane, B. B., Gal, D., Gelman, A., Robert, C., & Tackett1, J. L. (2017, September 21). Abandon Statistical Significance. Retrieved from http://www.stat.columbia.edu/~gelman/research/unpublished/abandon.pdf\nTrafimow, D. (2017). Using the coefficient of confidence to make the philosophical switch from a posteriori to a priori inferential statistics. Educational and Psychological Measurement 77, 831–854.\nTrafimow, D., & Earp, B. D. (2017). Null hypothesis significance testing and the use of P values to control the Type I error rate: The domain problem. New Ideas in Psychology 45, 19– 27. http://dx.doi.org/10.1016/j.newideapsych.2017.01.002.\nTrafimow, D., & MacDonald, J. A. (2017). Performing inferential statistics prior to data collection. Educational and Psychological Measurement 77, 204–219.\nTrafimow, D., Amrhein, V., Areshenkoff, C. N., Barrera-Causil, C., Beh, E. J., Bilgic, Y., . . . Marmolejo- Ramos, F. (submitted). Manipulating the Alpha Level Cannot Cure Significance Testing. Comments on “Redefine Statistical Significance”. Retrieved from https://peerj.com/preprints/3411/\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA’s statement on p-values: Context, process, and purpose. The American Statistician, 70, 129-133."
  },
  {
    "objectID": "blog/2017-09-30-summersaga/index.html#footnotes",
    "href": "blog/2017-09-30-summersaga/index.html#footnotes",
    "title": "The saga of the summer 2017, a.k.a. ‘the alpha wars’",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne could asks whether the New Statistics really brings something new (e.g., see this blogpost), or something useful (e.g., see this Twitter’ thread).↩︎"
  },
  {
    "objectID": "blog/2018-07-23-corroboration2/index.html",
    "href": "blog/2018-07-23-corroboration2/index.html",
    "title": "The Meehlian Corroboration-Verisimilitude Theory of Science - Part II",
    "section": "",
    "text": "NB: I strongly recommend reading the first part of this series before moving to the second part."
  },
  {
    "objectID": "blog/2018-07-23-corroboration2/index.html#how-to-appraise-a-theory",
    "href": "blog/2018-07-23-corroboration2/index.html#how-to-appraise-a-theory",
    "title": "The Meehlian Corroboration-Verisimilitude Theory of Science - Part II",
    "section": "How to appraise a theory?",
    "text": "How to appraise a theory?\n\nLakatosian defense\nOne of the main issue we have to deal with is to know how we should react to an apparent falsification of our substantive theory. If we were to follow some kind of folk strict falsificationism, any inconsistent data would be a sufficient argument to abandon our favourite theory. However, this is usually not how the scientific method is conducted (as we discussed in the previous blogpost). In some context, it might seem legitimate to “stick” with our theory, despite an apparent falsification.\nThe concept of Lakatosian defense refers to one of the strategies that let us “believe” in our substantive theory, despite an apparent falsification. The way this strategy works is by making first a distinction between the hard core of our substantive theory (i.e., \\(T\\)), and the protective belt, that includes the peripheral portions of \\(T\\), as well as all the other conjuncts on the left side of the conceptual formula presented in the previous post (i.e., the theoretical auxiliaries \\(A_{t}\\), the instrumental auxiliaries \\(A_{i}\\), the ceteris paribus clause \\(C_{p}\\), the experimental conditions \\(C_{n}\\), and finally the observations \\(O_{1}, O_{2}\\)). The Lakatosian defense then consists in avoiding directing “the arrow of the modus tollens” toward the hard core of the theory, but to direct it to the protective belt. In simple words, instead of considering the apparent falsification as a falsification of \\(T\\), we consider it as a falsification of some other conjuncts of the “left-side”.\n\nI formulate my version of the Lakatos principle thus: Accepting the neo-Popperian view that it is inadvisable to persist in defending a theory against apparent falsifications by ad hoc adjustments (three kinds), the rationale for defending by non-ad hoc adjustments lies in the theory having accumulated credit by strong successes, having lots of money in the bank (Meehl, 1990).\n\nAccording to Meehl, such a Lakatosian defense might be warranted by the hard core of the theory having high verisimilitude. It would be rational to engage in a Lakatosian defense already has “money in the bank, an impressive track record” (Meehl, 1990, p.122), meaning that the theory has performed well in the past. And the way a theory gets money in the bank is through Salmon’s principle, that we discuss in the next section.\n\n\nSalmonian damn strange coincidences\nTo sum up, Meehl says that it is acceptable to engage in a Lakatosian defense when the theory has performed sufficiently well in the past, when it has accumulated enough “money in the bank”. And the way a theory can earn money is through Salmon’s principle of “damn strange coincidences” (Salmon, 1984), that we formulate below:\n\nSalmon’s principle: The main way a theory gets money in the bank is by predicting facts that, absent the theory, would be antecedently improbable.\n\nFor instance, if I have a meteorological theory that successfully predicts that it will rain sometime next April, and that this prediction is confirmed by the data, the scientific community will not be much impressed. If my theory enables me to correctly predict which of 5 days in April it rains, they will be more impressed. And if I predict how many millimeters of rainfall there will be on each of these 5 days, they will begin to take my theory very seriously (Meehl, 1990).\nWe can all feel why this idea of “antecedent implausibility” makes some predictions stronger than others. This principle is useful as a way of appraising theories and assessing the strength of the tests we confront them to. But to assess how strange is a coincidence, we need to take into account the range of a priori plausible numerical values. Clearly, depending on whether we investigate the variability of IQ scores (expressed in points of IQ) of the effects of arsenic dosage in some drug (in mg), a difference or a prediction error of 0.1 might not have the same interpretation (or consequence). To take this into account, Meehl introduces the concept of Spielraum.\n\n\nThe concept of Spielraum\nThe concept of Spielraum comes from the Vienna Circle, that borrowed the term from Von Kries and is the German word for “field”, “range”, “scope”, and represents the range a priori plausible values (Meehl, 1990). The basic insight behind the idea of Spielraum is to realise that to appraise the power/quality of an interval prediction (e.g., “the IQ of this person should fall in the 111-114 interval”), we need to consider the width of the possible values of the outcome.\n\nThe notion of accuracy (in prediction) is a relative term, usually uninterpretable with respect to theory corroboration without the a priori range (Meehl, 1990).\n\nThe specification of the limits of the Spielraum can come either from variables definitions (e.g., this scale has a minimum of 0 and a maximum of 20), physical facts (e.g., human height can only be between X and Y cm, where X and Y are the heights of the shortest and tallest human alive, respectively), as well as from our “background knowledge” (Meehl, 1997, p.420).\nPut simply, the precision of an interval prediction (i.e., the range of values “tolerated” by a theory) should be interpreted in relation to the range of values that are considered as a priori plausible. Keeping this in mind, we should be ready now to discuss Meehl’s corroboration index.\n\n\nCloseness and theory tolerance: the corroboration index\nIn order to “numerify” the track record of a theory, Meehl (1990) introduces its corroboration index \\(C_{i}\\), computed as follows:\n\\[C_{i} = Closeness \\cdot Intolerance\\]\nwhere the closeness corresponds to the inverse of the deviation (the error) between the observed data (the mean of the sample6) and the edge of the range of values tolerated by the theory, in relation to the Spielraum,7 that is:\n\\[Closeness = 1 - (Deviation / Spielraum) \\] The intolerance of the theory is defined as the inverse of the tolerance, which is the range of values tolerated by the theory (e.g., if my theory predicts that an individual X should have an IQ between 111 and 114, the tolerance of my theory is 3), in relation to the Spielraum:8\n\\[Intolerance = 1 - (Tolerance / Spielraum) \\]\nTo sum up, the corroboration index can be seen as resulting from an interaction between the closeness of the prediction (i.e., by how much the theory missed the data, relatively to the a priori plausible values), and the risk the theory took in its prediction (i.e., the width of the range of tolerated values).\nThe main contribution of this index seems to lay in its ability to give a continous indication of the degree of corroboration, in contrast to the discrete Popperian falsification. As put by Meehl (1990, p.128):\n\nThis is similar to Popper’s original emphasis on corroboration being a function of risk, except that here again it is not yes-or-no falsification but Salmon’s principle that we wish to numerify. The revised methodology retains the Popperian emphasis on riskiness, but now instead of asking “Did I pass the test, which was stiff?” we ask, “How close did I come?”\n\nBelow we illustrate how theory strength (the spread of the values it tolerates) and predictive accuracy (the distance between the sample mean and the edge of the predicted interval) jointly determine the corroboration index. The range of values tolerated by the theory is depicted by the gray shaded area (whose bounds are represented by the gray dotted vertical lines), why the mean of the sample is represented by the vertical dashed pink line.\n\n\n\n\n\nThese two rows illustrate what can be considered as a strong versus weak (in relation to each other) theory for three possible relations with data (in column), namely, when theoretical predictions match the data (the first column), when the predictions barely miss the data (the second column), or when they miss the data by a considerably large amount (the third column).\nThe Shiny app below lets you explore this index in more details by playing with different parameters to see how it affects the corroboration index (the corroboration index is recomputed every time you generate a new sample).\n\n\nThis index is designed in such a way that if a very strong theory has an intolerance of 1 (i.e., it tolerates only a very narrow interval or a single point) and a closeness of 1, then it would have a corroboration index of 1. In the opposite, a weak directional prediction (of the kind we usually find in psychology) would have a theory tolerance of half the Spielraum, that is, an \\(Intolerance = 1/2\\). For this latter kind of theory the corroboration index will be 0.5 when it hits and will necessarily be inferior to 0.5 when it misses the data.9\nMeehl also offers another tool: the cumulative index, which permits to evaluate the track record of a theory, by simply taking its average corroboration index on a series of experiments: \\(C = \\frac{1}{N} \\Sigma C_{i}\\), where \\(N\\) is the number of experiments. Then, we can record the track record of a theory by using this index, combined with the number of experiment, thus “(1, 10)” (Meehl, 1990, p.129). Alternatively, one can present a series of corroboration indexes over a series of \\(N\\) studies by reporting the number of studies, the average corroboration index as well as the standard deviation of these indexes (\\(N, M_{c}, \\sigma_{C}\\); Meehl, 1990b).10\nOverall, Meehl’s proposal invites thinking about a way of quantifying the degree of corroboration of theories by jointly considering the strength of their predictions and their predictive accuracy, in relation to the Spielraum (i.e., the range of a priori plausible values). Although it is not clearly explicit in Meehl’s work whether this index is designed to be used as a conceptual tool only, some of his writings seems to suggest that this index could be used in applied settings as a complement to classical statistics (e.g., Meehl, 1990, p.416-418). For instance, Meehl makes a distinction between three approaches to theory testing: i) the weak use of significance tests to provide weak “confirmation” of weak theories, ii) the strong use of significance testing to discorroborate strong theories and iii) a third strategy that would bypass significance testing by using the above Salmonian principle to corroborate strong theories (Meehl, 1990, p.117). He continues by saying that he is (of course) advocating for the third approach, suggesting that he might envision a use for this index in applied settings."
  },
  {
    "objectID": "blog/2018-07-23-corroboration2/index.html#footnotes",
    "href": "blog/2018-07-23-corroboration2/index.html#footnotes",
    "title": "The Meehlian Corroboration-Verisimilitude Theory of Science - Part II",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhere the term of corroboration, coming from the German Bewährung, has been chosen by Popper to be “probability-neutral”, that is, with no known relation to any word or concept used in probability theory.↩︎\nSee the conceptual formula of the empirical test of a theory, discussed in the previous blogpost.↩︎\nSee also Mayo and Spanos (2010), for an account of the connection between frequentist statistical testing and Popper’s ideas of tests severity.↩︎\nI mean most quantitative psychological theories but see Meehl (1990, pp. 114–115) for some examples of theories that can literally be true.↩︎\nSorry if this sentence reads a bit heavy. Hopefully, the rest of the post will make it more understandable.↩︎\nOne could question Meehl’s choice of the mean of the sample as the best estimate to use.↩︎\nConsidering the error in relation to the Spielraum can also be considered one way to numerify Serlin and Lapsley’s (1985) “good enough” principle (Meehl, 1990, p.128).↩︎\nConcerning the IQ example, we might consider as a priori plausible IQ scores between 3 sigmas from the mean, that is, between 55 and 145 (Meehl, 1990, p.128).↩︎\nMeehl also shows how to normalise this index between 0 and 1 so that a weak theory is attributed an index of 0 and a strong theory an index of 1.↩︎\nThough the index as presented in this post is intended to work for point or interval predictions, Meehl (1990, p.130–134) also explains how to generalise this procedure to function-form predictions.↩︎"
  },
  {
    "objectID": "blog/2016-11-04-aicbic/index.html",
    "href": "blog/2016-11-04-aicbic/index.html",
    "title": "Why the Akaike information criterion is as much ‘Bayesian’ as the Bayesian information criterion",
    "section": "",
    "text": "According to Rubin (1984), a Bayesianly justifiable analysis is one that:\nThe Bayesian approach is then characterised by the explicit use of probability to model the uncertainty, which is done by assigning probability distributions to unknowns (the prior), then by updating them the information contained in the data (the likelihood), to lead to updated probability distributions (the posterior)."
  },
  {
    "objectID": "blog/2016-11-04-aicbic/index.html#model-based-inference",
    "href": "blog/2016-11-04-aicbic/index.html#model-based-inference",
    "title": "Why the Akaike information criterion is as much ‘Bayesian’ as the Bayesian information criterion",
    "section": "Model-based inference",
    "text": "Model-based inference\nWhen one writes a paper (or a blogpost) about model selection, it is of good taste to begin with the famous Box’ statement that “Essentially, all models are wrong but some are useful” (Box & Drapers, 1987).\nOk, so all models are wrong, in the sense that all models we can build are not exactly like the model which has generated the data (i.e., the full reality). But still, we can surely construct models that are better than others. Interestingly, as outlined by Burnham & Anderson (2004), the concept of a “good model”, depends on the sample size, as smaller effect sizes can often only be revealed as sample size increases. Then, the set of models aiming to explain the data at hand has to take into account the size of the effects.\nMore generally, we can outline three general principles guiding model-based inference in science:\n\nSimplicity and parsimony (Occam’s razor). Model selection is a classic bias-variance trade-off.\nMultiple working hypotheses (Chamberlin, 1890). At any point in time there must be several hypotheses (models) under consideration, but number of alternative should be kept small.\nStrength of evidence. Providing quantitative information to judge the “strength of evidence” is central to science (e.g., see Royall’s book on the likelihood-based approach).\n\nSteps of the model selection approach usually consist in establishing a set of \\(R\\) relevant models, ranking these models (and attributing them weights), and choosing the best model from the set to make an inference from this best model.\nWait, how can we rank models? We need tools that account for the basic principles (presented above) that make a model a good model, like how well a model fit the observed data or how parsimonious is a model…"
  },
  {
    "objectID": "blog/2016-11-04-aicbic/index.html#akaike-information-criterion",
    "href": "blog/2016-11-04-aicbic/index.html#akaike-information-criterion",
    "title": "Why the Akaike information criterion is as much ‘Bayesian’ as the Bayesian information criterion",
    "section": "Akaike information criterion",
    "text": "Akaike information criterion\nIn 1951, Kullback and Leibler published a now-famous paper that quantified the meaning of information as related to Fisher’s concept of sufficient statistics (Kullback & Leibler, 1951). They developped the Kullback-Leibler divergence (or K-L information) that measures the information that is lost when approximating reality. K-L information can also be conceptualised as a distance between full reality and a specific model (Burnham & Anderson, 2004).\nTwo decades later, Akaike (1973) showed that this distance can be estimated by finding the parameters values that maximise the probability of the data given the model (i.e., the maximised likelihood or MLE). He used this relationship to derive a criterion known as the Akaike Information Criterion (AIC), that is\n\\[AIC=-2\\log (\\mathcal{L}(\\hat{\\theta}\\vert data))+2K\\]\nwhere \\(\\theta\\) is the set of model parameters, \\(\\mathcal{L}(\\hat{\\theta}\\vert data)\\) the likelihood of the candidate model given the data when evaluated at the MLE of \\(\\theta\\), and \\(K\\) is the number of estimated parameters in the candidate model. The first component, \\(-2log(\\mathcal{L}(\\hat{\\theta}\\vert data))\\) is known as the deviance of the candidate model.\nBasically, we can see that AIC then account for the goodness-of-fit of a model (i.e., the strength of evidence for this model), but penalises it for having too much parameters, that is for not being parsimonious. Clearly, the smaller AIC, the better.\nIt is important to acknowledge that when \\(n/K\\) &gt; about 40, the “small sample AIC” (second-order bias correction), called AICc, should be used (Burnham & Anderson, 2004).\nAbsolute AIC values are not directly interpretable as they contain arbitrary constants and are much affected by sample size. We then need to rescale AIC or AICc to:\n\\[\\Delta_{i}= AIC_{i}-AIC_{min}.\\]\nHence, \\(\\Delta_{i}\\) is the information loss experienced if we are using fitted model \\(g_{i}\\) rather than the best model, \\(g_{min}\\), for inference.\nMoreover, the simple transformation \\(exp(-\\Delta_{i}/2)\\), for \\(i=1,2,...,R\\), provides the likelihood of the model (Akaike, 1981) given the data: \\(\\mathcal{L}(g_{i}\\vert data)\\).\nIt is convenient to normalise the model likelihoods such that they sum to 1 in order to treat them as probabilities. Hence, we use:\n\\[w_{i}=\\dfrac{exp(-\\Delta_{i}/2)}{\\sum_{r=1}^{R}exp(-\\Delta_{r}/2)}.\\]\nThe \\(w_{i}\\), called Akaike weights, are useful as the weight of evidence in favor of model \\(g_{i}(\\cdot \\vert \\theta)\\), as being the actual K-L best model in the set."
  },
  {
    "objectID": "blog/2016-11-04-aicbic/index.html#bayesian-information-criterion",
    "href": "blog/2016-11-04-aicbic/index.html#bayesian-information-criterion",
    "title": "Why the Akaike information criterion is as much ‘Bayesian’ as the Bayesian information criterion",
    "section": "Bayesian information criterion",
    "text": "Bayesian information criterion\nThe Bayesian Information Criterion (BIC), was introduced by Schwarz (1978) as a competitor to the AIC. Schwarz derived the BIC to serve as an asymptotic approximation to a transformation of the Bayesian posterior probability of a candidate model. The computation of BIC is based on the empirical log-likelihood and does not require the specification of priors.\n\\[BIC=-2\\log (\\mathcal{L}(\\hat{\\theta}|data))+K \\log (n)\\]\nWe can see that both AIC and BIC measure the same goodness-of-fit but the penalty term of BIC is more stringent than the penalty term of AIC (for \\(n \\geq 8\\), \\(k \\cdot \\log (n)\\) exceeds \\(2k\\)). Consequently, BIC tends to favor smaller models than AIC.\nAs with \\(\\Delta AIC_{i}\\), we define \\(\\Delta BIC_{i}\\) as the difference of BIC for model \\(g_{i}\\) and the minimum BIC value. More complete usage entails computing posterior model probabilities, as:\n\\[P(g_{i}\\vert data) = \\dfrac{\\exp(-\\frac{1}{2}\\Delta BIC_{i})}{\\sum_{r=1}^{R}\\exp(-\\frac{1}{2}\\Delta BIC_{r})}\\]\n(Raftery, 1995). The above posterior model probabilities are based on assuming that prior model probabilities are all \\(1/R\\). Most applications of BIC use it in a frequentist “spirit” and hence ignore issues of prior and posterior model probabilities (Burnham & Anderson, 2004)."
  },
  {
    "objectID": "blog/2016-11-04-aicbic/index.html#aic-as-a-bayesian-result-mathematical-derivations-taken-from-burnham-and-anderson-2002-2004",
    "href": "blog/2016-11-04-aicbic/index.html#aic-as-a-bayesian-result-mathematical-derivations-taken-from-burnham-and-anderson-2002-2004",
    "title": "Why the Akaike information criterion is as much ‘Bayesian’ as the Bayesian information criterion",
    "section": "AIC as a Bayesian result (mathematical derivations taken from Burnham and Anderson, 2002, 2004)",
    "text": "AIC as a Bayesian result (mathematical derivations taken from Burnham and Anderson, 2002, 2004)\nWe said before that the BIC arises in a context when one assumes equal priors on models but the BIC statistic can be used more generally with any set of model priors. Let \\(p_{i}\\) be the prior probability placed on model \\(g_{i}\\). Then the Bayesian posterior model probability is approximated as:\n\\[P(g_{i}\\vert data) = \\dfrac{\\exp(-\\frac{1}{2}\\Delta BIC_{i})p_{i}}{\\sum_{r=1}^{R}\\exp(-\\frac{1}{2}\\Delta BIC_{r})p_{r}}.\\]\nTo get back to Akaike weights (described above) from there we use the model prior:\n\\[p_{i}= B \\cdot \\exp(\\frac{1}{2}\\Delta BIC_{i})\\cdot \\exp (\\frac{1}{2}\\Delta AIC_{i})\\]\nwhere \\(B\\) is a normalising constant. Clearly,\n\\[\\exp(-\\frac{1}{2}\\Delta BIC_{i})\\cdot \\exp (\\frac{1}{2}\\Delta BIC_{i})\\cdot \\exp(-\\frac{1}{2}\\Delta AIC_{i}) = \\exp (-\\frac{1}{2}\\Delta AIC_{i});\\]\nhence, with this prior probability distribution we get:\n\\[P(g_{i}\\vert data) = \\dfrac{\\exp(-\\frac{1}{2}\\Delta BIC_{i})p_{i}}{\\sum_{r=1}^{R}\\exp(-\\frac{1}{2}\\Delta BIC_{r})p_{r}}=\\dfrac{\\exp(-\\frac{1}{2}\\Delta AIC_{i})}{\\sum_{r=1}^{R}\\exp(-\\frac{1}{2}\\Delta AIC_{r})}=w_{i},\\]\nwhich is the Akaike weight for model \\(g_{i}\\).\nThe prior probability on models \\(p_{i}\\) can then be expressed in a simple form as:\n\\[p_{i}= C \\cdot \\exp(\\frac{1}{2}K_{i}\\log (n)-K_{i})\\]\nwhere:\n\\[C=\\dfrac{1}{\\sum_{r=1}^{R}\\exp (\\frac{1}{2}K_{r}\\log (n)-K_{r})}.\\]\nThus, formally, the Akaike weights from AIC are (for large samples) Bayesian posterior model probabilities for this particular prior. Burnham & Anderson (2002, 2004) call this prior the K-L model prior.\nTo sum up, AIC can then be justified as Bayesian using a savvy prior (i.e., a prior that is an increasing function of \\(n\\) and a decreasing function of \\(K\\)). Thus, AIC model selection is just as much a Bayesian procedure as BIC model selection is. The difference is in the prior distribution placed on the model set."
  },
  {
    "objectID": "blog/2016-11-04-aicbic/index.html#author-note",
    "href": "blog/2016-11-04-aicbic/index.html#author-note",
    "title": "Why the Akaike information criterion is as much ‘Bayesian’ as the Bayesian information criterion",
    "section": "Author note",
    "text": "Author note\nPlease correct me if you identify mistakes, I am probably wrong on many aspects of what is discussed above…"
  },
  {
    "objectID": "blog/2018-01-20-glm/index.html",
    "href": "blog/2018-01-20-glm/index.html",
    "title": "Using R to make sense of the generalised linear model",
    "section": "",
    "text": "Modern science rests on several foundational pillars. Among these is the ability to construct sufficiently solid theoretical abstractions that are able to explain concrete observable aspects of the world. Once the applicability range of a theoretical model has been defined, it is usually compared to another model that is similar in its goal (i.e., another model that aims to explain similar aspects of the world). However, we rarely directly compare theoretical models. Instead, we are brought to compare statistical models that aim to represent theories.\nAccording to Rouder, Morey, & Wagenmakers (2016), “Models are devices that connect theories to data. A model is an instantiation of a theory as a set of probabilistic statements”. One common and convenient example of such instantiation is the linear model, which –in its general form– allows to predict parameter(s) of a distribution, which is supposed to reflect the distribution from which the observed data is issued (the data generation process).\nBut what does it mean for a model to predict something? In the current post, I focus on four R functions (the predict, fitted, residuals and simulate functions), exploring the similarities and differences between these functions, to illustrate the mechanisms and assumptions of the generalised linear model.\nThe usual linear model is of the following form.\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Normal}(\\mu_{i}, \\sigma) \\\\\n\\mu_{i} &= \\alpha + \\beta \\cdot \\text{x}_{i} \\\\\n\\end{aligned}\n\\]\nWhere, in Bayesian terms, the first line of the model corresponds to the likelihood of the model, which is the assumption made about the data generation process. We make the assumption that the outcomes \\(y_{i}\\) are normally distributed around a mean \\(\\mu_{i}\\) with some error \\(\\sigma\\). This is equivalent to say that the errors are normally distributed around \\(0\\).\nOf course, the distributional assumption is not restricted to be Gaussian, and can be adapted to whatever distribution that makes sense in consideration of the data at hand. The linear aspect of the linear model actually refers to the second line of the above description, in which one tries to predict parameters of the distribution (e.g., \\(\\mu_{i}\\) or \\(\\sigma\\)) by a linear combination of some predictor variable(s). Generalising to other distributions, the generalised linear model can be rewritten as:\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{D}(f(\\eta_{i}), \\theta) \\\\\n\\eta &= \\mathbf{X} \\beta \\\\\n\\end{aligned}\n\\]\nWhere the response \\(y_{i}\\) is predicted through the linear combination \\(\\eta\\) of predictors transformed by the inverse link function \\(f\\), assuming a certain distribution \\(D\\) for \\(y\\) (also called the family), and family-specific parameters \\(\\theta\\) (Bürkner, 2017).\nBelow, we illustrate a simple Gaussian linear model using the Howell1 dataset from the rethinking package (McElreath, 2016), which contains data about 544 individuals, including height (centimetres), weight (kilograms), age (years) and gender (0 indicating female and 1 indicating male).\n\nlibrary(rethinking)\nlibrary(tidyverse)\nlibrary(ggExtra)\n\ndata(Howell1)\nd &lt;- Howell1 %&gt;% filter(age &gt;= 18)\n\np &lt;- d %&gt;%\n  ggplot(aes(x = weight, y = height) ) +\n  geom_point(pch = 21, color = \"white\", fill = \"black\", size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", colour = \"black\") +\n  theme_bw(base_size = 12)\n\nggMarginal(p, type = \"histogram\")\n\n\n\n\n\n\n\n\nA quick visual exploration of the dataset reveals a positive relationship between height and weight. The above plotted regression line corresponds to the following model, where we assume a normal likelihood:\n\\[\n\\begin{aligned}\n\\text{height}_{i} &\\sim \\mathrm{Normal}(\\mu_{i}, \\sigma) \\\\\n\\mu_{i} &= \\alpha + \\beta \\cdot \\text{weight}_{i} \\\\\n\\end{aligned}\n\\]\nThis model can be fitted easily in R with the following syntax.\n\n(mod1 &lt;- lm(height ~ weight, data = d) )\n\n\nCall:\nlm(formula = height ~ weight, data = d)\n\nCoefficients:\n(Intercept)       weight  \n    113.879        0.905  \n\n\nThe intercept (113.879) represents the predicted height when weight is at 0 (which makes no much sense in our case), whereas the slope (0.905) represents the change in height when weight increases by one unit (i.e., one kilogram)."
  },
  {
    "objectID": "blog/2018-01-20-glm/index.html#statistical-models",
    "href": "blog/2018-01-20-glm/index.html#statistical-models",
    "title": "Using R to make sense of the generalised linear model",
    "section": "",
    "text": "Modern science rests on several foundational pillars. Among these is the ability to construct sufficiently solid theoretical abstractions that are able to explain concrete observable aspects of the world. Once the applicability range of a theoretical model has been defined, it is usually compared to another model that is similar in its goal (i.e., another model that aims to explain similar aspects of the world). However, we rarely directly compare theoretical models. Instead, we are brought to compare statistical models that aim to represent theories.\nAccording to Rouder, Morey, & Wagenmakers (2016), “Models are devices that connect theories to data. A model is an instantiation of a theory as a set of probabilistic statements”. One common and convenient example of such instantiation is the linear model, which –in its general form– allows to predict parameter(s) of a distribution, which is supposed to reflect the distribution from which the observed data is issued (the data generation process).\nBut what does it mean for a model to predict something? In the current post, I focus on four R functions (the predict, fitted, residuals and simulate functions), exploring the similarities and differences between these functions, to illustrate the mechanisms and assumptions of the generalised linear model.\nThe usual linear model is of the following form.\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{Normal}(\\mu_{i}, \\sigma) \\\\\n\\mu_{i} &= \\alpha + \\beta \\cdot \\text{x}_{i} \\\\\n\\end{aligned}\n\\]\nWhere, in Bayesian terms, the first line of the model corresponds to the likelihood of the model, which is the assumption made about the data generation process. We make the assumption that the outcomes \\(y_{i}\\) are normally distributed around a mean \\(\\mu_{i}\\) with some error \\(\\sigma\\). This is equivalent to say that the errors are normally distributed around \\(0\\).\nOf course, the distributional assumption is not restricted to be Gaussian, and can be adapted to whatever distribution that makes sense in consideration of the data at hand. The linear aspect of the linear model actually refers to the second line of the above description, in which one tries to predict parameters of the distribution (e.g., \\(\\mu_{i}\\) or \\(\\sigma\\)) by a linear combination of some predictor variable(s). Generalising to other distributions, the generalised linear model can be rewritten as:\n\\[\n\\begin{aligned}\ny_{i} &\\sim \\mathrm{D}(f(\\eta_{i}), \\theta) \\\\\n\\eta &= \\mathbf{X} \\beta \\\\\n\\end{aligned}\n\\]\nWhere the response \\(y_{i}\\) is predicted through the linear combination \\(\\eta\\) of predictors transformed by the inverse link function \\(f\\), assuming a certain distribution \\(D\\) for \\(y\\) (also called the family), and family-specific parameters \\(\\theta\\) (Bürkner, 2017).\nBelow, we illustrate a simple Gaussian linear model using the Howell1 dataset from the rethinking package (McElreath, 2016), which contains data about 544 individuals, including height (centimetres), weight (kilograms), age (years) and gender (0 indicating female and 1 indicating male).\n\nlibrary(rethinking)\nlibrary(tidyverse)\nlibrary(ggExtra)\n\ndata(Howell1)\nd &lt;- Howell1 %&gt;% filter(age &gt;= 18)\n\np &lt;- d %&gt;%\n  ggplot(aes(x = weight, y = height) ) +\n  geom_point(pch = 21, color = \"white\", fill = \"black\", size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", colour = \"black\") +\n  theme_bw(base_size = 12)\n\nggMarginal(p, type = \"histogram\")\n\n\n\n\n\n\n\n\nA quick visual exploration of the dataset reveals a positive relationship between height and weight. The above plotted regression line corresponds to the following model, where we assume a normal likelihood:\n\\[\n\\begin{aligned}\n\\text{height}_{i} &\\sim \\mathrm{Normal}(\\mu_{i}, \\sigma) \\\\\n\\mu_{i} &= \\alpha + \\beta \\cdot \\text{weight}_{i} \\\\\n\\end{aligned}\n\\]\nThis model can be fitted easily in R with the following syntax.\n\n(mod1 &lt;- lm(height ~ weight, data = d) )\n\n\nCall:\nlm(formula = height ~ weight, data = d)\n\nCoefficients:\n(Intercept)       weight  \n    113.879        0.905  \n\n\nThe intercept (113.879) represents the predicted height when weight is at 0 (which makes no much sense in our case), whereas the slope (0.905) represents the change in height when weight increases by one unit (i.e., one kilogram)."
  },
  {
    "objectID": "blog/2018-01-20-glm/index.html#prediction-is-key-predict-and-fitted",
    "href": "blog/2018-01-20-glm/index.html#prediction-is-key-predict-and-fitted",
    "title": "Using R to make sense of the generalised linear model",
    "section": "Prediction is key: predict and fitted",
    "text": "Prediction is key: predict and fitted\nThe main advantage of the previous model is that it allows to make predictions for any value of \\(\\text{weight}\\). In R, this is done using the aptly named predict function. For instance, we can ask our model what is the expected height for an individual of weight 43, which is equal to \\(\\alpha + \\beta \\cdot 43\\).\n\nwght &lt;- 43\n\nd %&gt;%\n  ggplot(aes(x = weight, y = height) ) +\n  geom_line(aes(y = predict(mod1) ), size = 1) +\n  geom_point(size = 2, alpha = 0.2) +\n  geom_segment(\n    x = wght, xend = wght,\n    y = 0, yend = predict(mod1, newdata = data.frame(weight = wght) ),\n    linetype = 2, lwd = 0.5\n    ) +\n  geom_segment(\n    x = 0, xend = wght,\n    y = predict(mod1, newdata = data.frame(weight = wght) ),\n    yend = predict(mod1, newdata = data.frame(weight = wght) ),\n    linetype = 2, lwd = 0.5\n    ) +\n  theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\nImplementing the function predict by hand is quite easy and will allow us to better understand how it works. This function is actually simply retrieving parameters of the fitted model (in our case, the intercept and the slope) to make predictions about the outcome variable, given some values of the predictor(s). In other words, it corresponds to the second line of our model.\n\nd &lt;- d %&gt;%\n  mutate(\n    pred_mod1 = predict(mod1),\n    pred_mod1_2 = coef(mod1)[1] + coef(mod1)[2] * weight\n    )\n\nhead(d)\n\n   height   weight age male pred_mod1 pred_mod1_2\n1 151.765 47.82561  63    1  157.1630    157.1630\n2 139.700 36.48581  63    0  146.9001    146.9001\n3 136.525 31.86484  65    0  142.7180    142.7180\n4 156.845 53.04191  41    1  161.8839    161.8839\n5 145.415 41.27687  51    0  151.2362    151.2362\n6 163.830 62.99259  35    1  170.8895    170.8895\n\n\nWe could also be interested in predicting the height of individuals with other weights that the weights we observed (e.g., weights between 80 and 100 kgs). Below we simulate new data from our model (i.e., we simulate heights) and predictions for this new set of data (i.e., the \\(\\mu_{i}\\)).\n\n# generating weights from 80 to 100 kgs\ndata.frame(weight = 80:100) %&gt;%\n  # retrieving mod1 predictions\n  mutate(pred = predict(mod1, newdata = .) ) %&gt;%\n  # simulating data from our model, taking into account sigma\n  mutate(sim =  pred + rnorm(length(weight), 0, sd(residuals(mod1) ) ) ) %&gt;%\n  # or using sigma(mod1) instead of sd(residuals(mod1) ), as suggested\n  # by Tristan Mahr (https://www.tjmahr.com)\n  mutate(sim =  pred + rnorm(length(weight), 0, sigma(mod1) ) ) %&gt;%\n  # plotting these predictions\n  ggplot(aes(x = weight, y = pred) ) +\n  geom_line(size = 1) +\n  geom_point(aes(x = weight, y = sim), size = 2) +\n  geom_segment(\n      aes(xend = weight, yend = sim),\n      size = 0.5, alpha = 0.5, lineend = \"round\"\n      ) +\n  theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\nWhere the vertical lines represent deviations from the predicted values. OK, so we’ve seen the the predict function simply uses the linear model to make predictions about the \\(\\mu_{i}\\).\nYou might know that there exists a similar function, the fitted function, which allows to extract fitted values of a model.\n\nd &lt;- d %&gt;% mutate(fitted_mod1 = fitted(mod1) )\nhead(d)\n\n   height   weight age male pred_mod1 pred_mod1_2 fitted_mod1\n1 151.765 47.82561  63    1  157.1630    157.1630    157.1630\n2 139.700 36.48581  63    0  146.9001    146.9001    146.9001\n3 136.525 31.86484  65    0  142.7180    142.7180    142.7180\n4 156.845 53.04191  41    1  161.8839    161.8839    161.8839\n5 145.415 41.27687  51    0  151.2362    151.2362    151.2362\n6 163.830 62.99259  35    1  170.8895    170.8895    170.8895\n\n\nSurprisingly, the predict and fitted functions seem to do the exact same thing (at least their results are the same)… but do they? To answer this question, let’s ask another one."
  },
  {
    "objectID": "blog/2018-01-20-glm/index.html#link-function-toward-glms",
    "href": "blog/2018-01-20-glm/index.html#link-function-toward-glms",
    "title": "Using R to make sense of the generalised linear model",
    "section": "Link function, toward GLMs",
    "text": "Link function, toward GLMs\nCan we predict gender by individual height? The usual way to answer this kind of question is through a logistic regression model (or logit model). Logistic regression is used to model binary outcome variables, using the linear regression framework. In the logit model, the log-odds of the outcome \\(p_{i}\\) are modelled as a linear combination of the predictor variables:\n\\[\nlogit(p_{i}) = log\\Big(\\frac{p_{i}}{1 - p_{i}}\\Big) = \\alpha + \\beta _{1} x_{1} + \\cdots + \\beta _{n} x_{n}\n\\]\nThus, although the observed outcome variable is a dichotomic variable, the logistic regression estimates the log-odds, as a continuous variable, that the outcome variable is in a certain state (in our case, that the individual is a man)1. This model can be written as follows, where \\(p_{i}\\) is the probability that an individual is a man.\n\\[\n\\begin{aligned}\n\\text{gender}_{i} &\\sim \\mathrm{Binomial}(1, p_{i}) \\\\\nlogit(p_{i}) &= \\alpha + \\beta \\cdot \\text{height}_{i} \\\\\n\\end{aligned}\n\\]\nThis model is implemented easily in R using the glm function, where the family argument is used to specify the likelihood of the model, and the link function.\n\n(mod2 &lt;- glm(male ~ height, data = d, family = binomial(link = \"logit\") ) )\n\n\nCall:  glm(formula = male ~ height, family = binomial(link = \"logit\"), \n    data = d)\n\nCoefficients:\n(Intercept)       height  \n   -53.3653       0.3438  \n\nDegrees of Freedom: 351 Total (i.e. Null);  350 Residual\nNull Deviance:      486.6 \nResidual Deviance: 258.7    AIC: 262.7\n\n\nBelow we print predictions of the model, using both the predict and fitted functions.\n\nd &lt;- d %&gt;%\n  mutate(\n    pred_mod2 = predict(mod2),\n    fitted_mod2 = fitted(mod2)\n    )\n\nd %&gt;%\n  select(height, weight, male, pred_mod2, fitted_mod2) %&gt;%\n  head\n\n   height   weight male  pred_mod2 fitted_mod2\n1 151.765 47.82561    1 -1.1910139 0.233077651\n2 139.700 36.48581    0 -5.3387584 0.004778882\n3 136.525 31.86484    0 -6.4302701 0.001609421\n4 156.845 53.04191    1  0.5554049 0.635388646\n5 145.415 41.27687    0 -3.3740373 0.033116790\n6 163.830 62.99259    1  2.9567306 0.950580634\n\n\nThis time the results of predict and fitted appear to be quite different… We can plot the predictions of mod2 following the same strategy as previously using fitted. The logit_dotplot function displays the prediction of the logit model along with the marginal distribution of height by gender (detailed code can be found here ).\n\nsource(\"logit_dotplot.R\")\nlogit_dotplot(d$height, d$male, xlab = \"height\", ylab = \"p(male)\")\n\n\n\n\n\n\n\n\nThe output of the predict and fitted functions are different when we use a GLM because the predict function returns predictions of the model on the scale of the linear predictor (here in the log-odds scale), whereas the fitted function returns predictions on the scale of the response. To obtain the fitted values, we thus have to apply the inverse of the link function to the predicted values obtained with predict. In our case, this translates to the logistic transformation:\n\\[p = \\exp(\\alpha) / (1 + \\exp(\\alpha) )\\]\nWhich, in R, gives:\n\nexp(coef(mod2) ) / (1 + exp(coef(mod2) ) )\n\n (Intercept)       height \n6.664323e-24 5.851092e-01 \n\n\nWhich is equivalent to using the plogis function:\n\nplogis(coef(mod2) )\n\n (Intercept)       height \n6.664323e-24 5.851092e-01 \n\n\nLet’s compare our previous calls to the predict and fitted functions…\n\nd &lt;- d %&gt;%\n  mutate(pred_mod2_2 = plogis(predict(mod2) %&gt;% as.numeric) )\n\nd %&gt;%\n  select(height, weight, male, pred_mod2, fitted_mod2, pred_mod2_2) %&gt;%\n  head\n\n   height   weight male  pred_mod2 fitted_mod2 pred_mod2_2\n1 151.765 47.82561    1 -1.1910139 0.233077651 0.233077651\n2 139.700 36.48581    0 -5.3387584 0.004778882 0.004778882\n3 136.525 31.86484    0 -6.4302701 0.001609421 0.001609421\n4 156.845 53.04191    1  0.5554049 0.635388646 0.635388646\n5 145.415 41.27687    0 -3.3740373 0.033116790 0.033116790\n6 163.830 62.99259    1  2.9567306 0.950580634 0.950580634\n\n\nTo sum up, the fitted function automatically applies the inverse transformation to provide prediction on the scales of the outcome. A similar behaviour can be obtained by using the predict function, and by specifying the scale in which we want to obtain predictions (e.g., in the scale of the response variable).\n\nd &lt;- d %&gt;%\n  mutate(pred_mod2_3 = predict(mod2, type = \"response\") )\n\nd %&gt;%\n  select(height, weight, male, pred_mod2, fitted_mod2, pred_mod2_2, pred_mod2_3) %&gt;%\n  head\n\n   height   weight male  pred_mod2 fitted_mod2 pred_mod2_2 pred_mod2_3\n1 151.765 47.82561    1 -1.1910139 0.233077651 0.233077651 0.233077651\n2 139.700 36.48581    0 -5.3387584 0.004778882 0.004778882 0.004778882\n3 136.525 31.86484    0 -6.4302701 0.001609421 0.001609421 0.001609421\n4 156.845 53.04191    1  0.5554049 0.635388646 0.635388646 0.635388646\n5 145.415 41.27687    0 -3.3740373 0.033116790 0.033116790 0.033116790\n6 163.830 62.99259    1  2.9567306 0.950580634 0.950580634 0.950580634\n\n\nTo relate this to our understanding of the linear model equation, prediction pertains to the \\(\\mu_{i}\\), meaning that we try to predict the mean observed outcome for a specific value of the predictor \\(x_{i}\\).\nWe should go further and implement uncertainty in these predictions, but we should first take a break to examine the concepts of errors and residuals, and their relationship."
  },
  {
    "objectID": "blog/2018-01-20-glm/index.html#errors-and-residuals-the-residuals-function",
    "href": "blog/2018-01-20-glm/index.html#errors-and-residuals-the-residuals-function",
    "title": "Using R to make sense of the generalised linear model",
    "section": "Errors and residuals: the residuals function",
    "text": "Errors and residuals: the residuals function\nA very common fallacy about the assumptions of the linear (Gaussian) model is that the outcome variable should be normally distributed. Instead, this assumption concerns the distribution of the outcome variable around its predicted value (i.e., the distribution of the errors).\nWhat we actually said above is that the errors \\(\\epsilon_{i}\\) should be normally distributed around the predicted value. But the errors are the the non-observed (and non-observable) differences between the theoretical predicted value \\(\\mu\\) and the observed outcomes. Consequently, we do not have access to it. Instead, what we can work with are the residuals \\(e_{i}\\), which can be seen as an estimate (from the sample) of the errors \\(\\epsilon_{i}\\), in a similar way as \\(b\\) is an estimate of \\(\\beta\\). To sum up, the residuals are the \\(e_{i} = y_{i} - \\bar{X}\\) whereas the errors are the \\(\\epsilon_{i} = y_{i} - \\mu_{i}\\).\nIn other words, errors pertain to the data generation process, whereas residuals are the difference between the model’s estimation and the observed outcomes. Basically, the residuals are the difference between the observed value and the predicted value. We can obtain them easily using the residuals function (which is useful for more complex models), or by subtracting to each observed outcome \\(y_{i}\\) the predicted \\(\\mu_{i}\\).\n\nd &lt;- d %&gt;%\n  mutate(\n    res1 = residuals(mod1),\n    res2 = height - pred_mod1\n    )\n\nd %&gt;%\n  select(height, weight, male, pred_mod1, res1, res2) %&gt;%\n  head\n\n   height   weight male pred_mod1      res1      res2\n1 151.765 47.82561    1  157.1630 -5.397960 -5.397960\n2 139.700 36.48581    0  146.9001 -7.200111 -7.200111\n3 136.525 31.86484    0  142.7180 -6.193000 -6.193000\n4 156.845 53.04191    1  161.8839 -5.038870 -5.038870\n5 145.415 41.27687    0  151.2362 -5.821164 -5.821164\n6 163.830 62.99259    1  170.8895 -7.059520 -7.059520\n\n\nBelow we plot these residuals, and make the alpha (i.e., the transparency) and the size of the points dependent on the distance to the predicted value (so that larger residuals appear as bigger and less transparent). This distance is also represented by the length of the vertical lines.\n\nd %&gt;%\n  sample_frac(0.5) %&gt;% # selecting a (50%) subsample of the data\n  ggplot(aes(x = weight, y = height) ) +\n  geom_line(aes(y = pred_mod1), size = 1) +\n  geom_point(aes(alpha = abs(res1), size = abs(res1) ) ) +\n  guides(alpha = FALSE, size = FALSE) +\n  geom_segment(aes(xend = weight, yend = pred_mod1, alpha = abs(res1) ) ) +\n  theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\nIf we take all the above verticals bars (i.e., the residuals) and plot their distribution, we can compare them to a normal distribution with mean 0, and standard deviation \\(\\sigma\\) equal to the standard deviation of the residuals, to check our assumption about the distribution of the residuals (here a normality assumption).\n\nd %&gt;%\n  ggplot(aes(x = res1) ) +\n  geom_histogram(aes(y = ..density..), bins = 20, alpha = 0.6) +\n  geom_line(aes(y = dnorm(res1, mean = 0, sd = sd(res1) ) ), size = 1) +\n  guides(fill = FALSE) +\n  theme_bw(base_size = 12) +\n  labs(x = \"Residuals\", y = \"Density\")\n\n\n\n\n\n\n\n\nTo sum up, the distributional assumption of the linear model (Gaussian or whatever) concerns the distribution of the errors, that we do not know. Instead, we can evaluate the distribution of the residuals, which are the observed differences between the outcome variable and the predictions of the model."
  },
  {
    "objectID": "blog/2018-01-20-glm/index.html#prediction-uncertainty-the-simulate-function",
    "href": "blog/2018-01-20-glm/index.html#prediction-uncertainty-the-simulate-function",
    "title": "Using R to make sense of the generalised linear model",
    "section": "Prediction uncertainty: the simulate function",
    "text": "Prediction uncertainty: the simulate function\nRecall that our model is:\n\\[\n\\begin{aligned}\n\\text{height}_{i} &\\sim \\mathrm{Normal}(\\mu_{i}, \\sigma) \\\\\n\\mu_{i} &= \\alpha + \\beta \\cdot \\text{weight}_{i} \\\\\n\\end{aligned}\n\\]\nSo far, we used the linear part of the linear model (i.e., the second line) to make predictions about the \\(\\mu_{i}\\), from \\(\\alpha\\) and \\(\\beta\\). In other words, we made predictions about mean values of the Gaussian distribution for a specific value of weight.\nNow, if we want to generate actual data (i.e., the \\(y_{i}\\)), from our model, we still have to include \\(\\sigma\\). This can be done “by-hand”, by adding error (i.e., some random variation) to our predictions.\n\nd &lt;- d %&gt;%\n  mutate(\n    sim1 = coef(mod1)[1] + coef(mod1)[2] * d$weight + rnorm(1, 0, sigma(mod1) )\n    )\n\nd %&gt;%\n  select(height, weight, male, pred_mod1, sim1) %&gt;%\n  head\n\n   height   weight male pred_mod1     sim1\n1 151.765 47.82561    1  157.1630 150.8900\n2 139.700 36.48581    0  146.9001 140.6271\n3 136.525 31.86484    0  142.7180 136.4450\n4 156.845 53.04191    1  161.8839 155.6109\n5 145.415 41.27687    0  151.2362 144.9632\n6 163.830 62.99259    1  170.8895 164.6165\n\n\nThe simulate function allows to automate the previous step and to run it nsims times.\n\nnsims &lt;- 1e4\nsims &lt;- simulate(mod1, nsim = nsims) %&gt;% data.frame\n\nsims[1:6, 1:6]\n\n     sim_1    sim_2    sim_3    sim_4    sim_5    sim_6\n1 153.4623 144.9603 156.3070 153.9208 155.2848 164.3996\n2 140.8794 137.1671 145.2417 153.6900 135.6464 143.5053\n3 136.5193 146.7893 137.5192 139.2745 147.7782 148.4528\n4 171.1628 162.4432 157.0834 166.2178 162.9167 167.9813\n5 159.6189 157.5319 156.3198 150.2240 144.3771 150.5606\n6 164.5527 172.0227 173.4904 181.0220 164.2144 168.1470\n\nlower_ci_sim &lt;- apply(sims, 1, function(x) quantile(x, probs = 0.025) )\nupper_ci_sim &lt;- apply(sims, 1, function(x) quantile(x, probs = 0.975) )\n\nsims_summary &lt;- data.frame(\n  lower = lower_ci_sim,\n  upper = upper_ci_sim\n  )\n\nd %&gt;%\n  ggplot(aes(x = weight, y = height) ) +\n  geom_smooth(method = \"lm\", color = \"black\") +\n  geom_point(size = 2, alpha = 0.3) +\n  geom_ribbon(\n    data = sims_summary, inherit.aes = FALSE,\n    aes(x = d$weight, ymin = lower, ymax = upper), alpha = 0.1\n    ) +\n  theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\nThe first shaded region around the regression line represents the 95% confidence interval around the regression line, whereas the second broader shaded area represents the 95% central quantiles of the Gaussian distribution at each value of weight. These are computed by simulating nsims height values at each weight value, and by taking the 95% mean values of this distribution."
  },
  {
    "objectID": "blog/2018-01-20-glm/index.html#conclusions",
    "href": "blog/2018-01-20-glm/index.html#conclusions",
    "title": "Using R to make sense of the generalised linear model",
    "section": "Conclusions",
    "text": "Conclusions\nIn this post, we tried to shed light on some concepts of the generalised linear model by examining the similarities and differences between four common R functions. It should be acknowledged that these functions (predict, fitted, residuals, and simulate) work with many different types of models, including models fitted with lme4 or brms. For instance, here is an overview of the available methods in my current session.\n\nmethods(predict)\n\n [1] predict.ar*                predict.Arima*            \n [3] predict.arima0*            predict.bam*              \n [5] predict.brmsfit*           predict.bs*               \n [7] predict.bSpline*           predict.emmGrid*          \n [9] predict.gam*               predict.glm               \n[11] predict.glmmPQL*           predict.gls*              \n[13] predict.gnls*              predict.HoltWinters*      \n[15] predict.jam*               predict.lda*              \n[17] predict.lm                 predict.lme*              \n[19] predict.lmList*            predict.lmList4*          \n[21] predict.loess*             predict.lqs*              \n[23] predict.mca*               predict.merMod*           \n[25] predict.mlm*               predict.nbSpline*         \n[27] predict.nlme*              predict.nls*              \n[29] predict.npolySpline*       predict.ns*               \n[31] predict.pbSpline*          predict.polr*             \n[33] predict.poly*              predict.polySpline*       \n[35] predict.ppolySpline*       predict.ppr*              \n[37] predict.prcomp*            predict.princomp*         \n[39] predict.qda*               predict.rlm*              \n[41] predict.smooth.spline*     predict.smooth.spline.fit*\n[43] predict.StructTS*         \nsee '?methods' for accessing help and source code"
  },
  {
    "objectID": "blog/2018-01-20-glm/index.html#references",
    "href": "blog/2018-01-20-glm/index.html#references",
    "title": "Using R to make sense of the generalised linear model",
    "section": "References",
    "text": "References\n\n\nClick to expand\n\n\nBürkner, P.-C. (2017). brms: An R package for bayesian multilevel models using Stan. Journal of Statistical Software, 80 (1), 1–28. doi:10.18637/jss.v080.i01\nMcElreath, R. (2016). Statistical Rethinking (p. 469). Chapman; Hall/CRC.\nRouder, J. N., Morey, R. D., & Wagenmakers, E.-J. (2016). The Interplay between Subjectivity, Statistical Practice, and Psychological Science. Collabra, 2(1), 1–12. doi:10.1525/collabra.28"
  },
  {
    "objectID": "blog/2018-01-20-glm/index.html#footnotes",
    "href": "blog/2018-01-20-glm/index.html#footnotes",
    "title": "Using R to make sense of the generalised linear model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more details on the logit model, please check this previous post.↩︎"
  },
  {
    "objectID": "team/index.html",
    "href": "team/index.html",
    "title": "Team",
    "section": "",
    "text": "Background: Cognitive Neuroscience, Experimental Psychology, Statistical modelling\nPosition: Researcher in computational cognitive neuroscience\nMail GitHub Google Scholar Bluesky"
  },
  {
    "objectID": "team/index.html#principal-investigator-pi",
    "href": "team/index.html#principal-investigator-pi",
    "title": "Team",
    "section": "",
    "text": "Background: Cognitive Neuroscience, Experimental Psychology, Statistical modelling\nPosition: Researcher in computational cognitive neuroscience\nMail GitHub Google Scholar Bluesky"
  },
  {
    "objectID": "team/index.html#postdoctoral-researchers",
    "href": "team/index.html#postdoctoral-researchers",
    "title": "Team",
    "section": "Postdoctoral researchers",
    "text": "Postdoctoral researchers\n\n\n \n\n\nYou?\n\n…"
  },
  {
    "objectID": "team/index.html#phd-students",
    "href": "team/index.html#phd-students",
    "title": "Team",
    "section": "PhD students",
    "text": "PhD students\n\n\n \n\n\nYou?\n\n…\n\n\n\n\n\n\n\n\n\nAlumni\n\n\n\n\n\n\nVincent Pauline (2023 – 2024): MSc student in my previous lab. Now a research assistant at Helmoltz AI (Germany).\nHermine de Torcy, (2023 – 2024): MSc student in my previous lab. Now a PhD student at ULB (Belgium).\nMaël Delem (2021 – 2022): MSc student in my previous lab. Now a PhD student at Lyon Univ. (France)\nNina Stauffert (2021 – 2022): MSc student in my previous lab. Now a PhD student at Lyon Univ. (France)\nClara Grégoire (2021 – 2022): MSc student in my previous lab. Now a PhD student at LPL - Aix-Marseille Univ. (France)\nVictor Serveau (2021 – 2022): MSc student in my previous lab. Now working in the private sector. (France)"
  }
]